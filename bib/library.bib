@inproceedings{Mallik2015,
abstract = {Ubiquitous intelligent devices have enabled pro- vision of smart services to people in seamless way. Context- awareness helps understand current state-of-affairs or the sit- uation in which presently the system is. This understanding helps the IoT application provide more relevant and smarter services based on situations that change over a period of time. In this paper, we propose a novel context-aware situation- tracking framework that makes use of an ontology. The ontology represents the conceptual model of a dynamic world, where situations evolve over time in changing contexts. The ontology provides the reasoning framework to infer about a situation based on the input context data as well as the past information of earlier situations. Future situations can be predicted with some belief based on current situation and incoming context data. The context data is acquired from sensor devices and external inputs. For every recognized situation, system recommends some actions to provide context-aware service. We use Multimedia Web Ontology Language (MOWL) to represents the ontology. MOWL proposes a probabilistic framework for reasoning with uncertainties linked with observation of context. It makes use of Dynamic Bayesian networks to predict and track the dynamically changing situations.We illustrate use of this framework for Smart Mirror use case.},
author = {Mallik, Anupama and Tripathi, Anurag and Kumar, Ravi and Chaudhury, Santanu and Sinha, Komal},
booktitle = {2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)},
doi = {10.1109/WF-IoT.2015.7389137},
isbn = {978-1-5090-0366-2},
keywords = {context,ontology,situaion recognition},
month = {12},
pages = {687--692},
publisher = {IEEE},
title = {{Ontology based context aware situation tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7389137},
year = {2015}
}
@article{Garcia,
abstract = {The paper describes a layered architecture to formalize contextual knowledge in fusion processes, with a design following the JDL model. The use of contextual knowledge has been pointed out as a key element to reason about the properties and relations of the entities of interest in several domains such as computer vision or anomaly detection. The present paper is a summary of contributions in this direction: we propose a framework aimed at the construction of a symbolic model of the scene by integrating tracking data and contextual information. The model is represented with formal ontologies supporting the execution of reasoning procedures in order to obtain a high-level interpretation of the scenario and provide feedback to the low-level fusion methods to improve its accuracy and performance. Deductive and abductive reasoning processes are performed within the model to accomplish scene interpretation and performance improvement. To show the advantages of the proposal, two examples are used to illustrate the framework in surveillance applications requiring the representation of the domain contextual knowledge.},
author = {Garcia, Jesus and Gomez-Romero, Juan},
keywords = {context,jdl (joint directors of laboratories) model,ontologies,surveillance},
number = {i},
pages = {1--22},
title = {{Ontologies for Representation and Exploitation of Contextual Information}}
}
@article{Domaszewicz2016,
abstract = {Considerable effort in the area of context-aware systems goes into applications that process sensor data to proactively drive actuators. However, there are concerns about such fully automated operation. Most notably, due to imperfect context inferences, actuating decisions can be contrary to the user's desires. In this article, the authors focus on what they refer to as soft actuation: issuing low-key, nonverbal hints prompting the user to optionally perform a simple manual operation on a nearby object. Soft actuation targets noncritical applications for the home or office. In the spirit of calm technology, special care is taken to respect the user's attention. The authors elaborate the interaction concept and present its experimental evaluation "in the wild." The results are encouraging, given that users liked the soft-actuating system and accepted quite a few hints.},
author = {Domaszewicz, Jaroslaw and Lalis, Spyros and Pruszkowski, Aleksander and Koutsoubelias, Manos and Tajmajer, Tomasz and Grigoropoulos, Nasos and Nati, Michele and Gluhak, Alexander},
doi = {10.1109/MPRV.2016.5},
issn = {1536-1268},
journal = {IEEE Pervasive Computing},
keywords = {HCI,Internet of Things,calm technology,context-aware applications,human-computer interaction,pervasive computing,proactive actuation,smart home,soft actuation,user control,user interaction},
month = {1},
number = {1},
pages = {48--56},
title = {{Soft Actuation: Smart Home and Office with Human-in-the-Loop}},
url = {http://ieeexplore.ieee.org/document/7389262/},
volume = {15},
year = {2016}
}
@article{Ye2011,
abstract = {Pervasive systems must offer an open, extensible, and evolving portfolio of services which integrate sensor data from a diverse range of sources. The core challenge is to provide appropriate and consistent adaptive behaviours for these services in the face of huge volumes of sensor data exhibiting varying degrees of precision, accuracy and dynamism. Situation identification is an enabling technology that resolves noisy sensor data and abstracts it into higher-level concepts that are interesting to applications. We provide a comprehensive analysis of the nature and characteristics of situations, discuss the complexities of situation identification, and review the techniques that are most popularly used in modelling and inferring situations from sensor data. We compare and contrast these techniques, and conclude by identifying some of the open research opportunities in the area.},
author = {Ye, Juan and Dobson, Simon and McKeever, Susan},
doi = {10.1016/j.pmcj.2011.01.004},
issn = {15741192},
journal = {Pervasive and Mobile Computing},
keywords = {Ontologies,bayes,bayes networks,cfg,crf,decision trees,dst,evidence theory,fuzzy logic,hmm,ontologies,pervasive computing,situation,survey},
month = {2},
number = {1},
pages = {36--66},
title = {{Situation identification techniques in pervasive computing: A review}},
url = {http://www.sciencedirect.com/science/article/B7MF1-522SHTF-1/2/5f9de7a6e6322a87e365acea94b371bb http://linkinghub.elsevier.com/retrieve/pii/S1574119211000253},
volume = {8},
year = {2012}
}
@book{Dupre2007,
author = {Dupr{\'{e}}, Lyn},
edition = {Rev ed., 1},
isbn = {978-0201379211},
publisher = {Addison$\backslash$hyp{\{}{\}}Wesley},
title = {{{\{}BUGS{\}} in writing}}
}
@inproceedings{Carvalho2016,
abstract = {Multi-Entity Bayesian Network (MEBN) is an expressive first-order probabilistic logic that represents the domain using parameterized fragments of Bayesian networks. Probabilistic-OWL (PR-OWL) uses MEBN to add uncer- tainty support to OWL, the main language of the Semantic Web. The reason- ing in MEBN is made by the construction of a Situation-Specific Bayesian Net- work (SSBN), a minimal Bayesian network sufficient to compute the response to queries. A Bottom-Up algorithm has been proposed for generating SSBNs in MEBN. However, this approach presents scalability problems since the algorithm starts from all the query and evidence nodes, which can be a very large set in real domains. To address this problem, we present a new scalable algorithm for generating SSBNs based on the Bayes-Ball method, a well-known and efficient algorithm for discovering d-separated nodes of target sets in Bayesian networks. The novel SSBN algorithm used together with Resource Description Framework (RDF) databases and PR-OWL 2 RL, an amenable version of PR-OWL, allows reasoning with probabilistic ontologies containing large assertive bases, offering a scalable approach for the treatment of uncertainty in the SemanticWeb.},
author = {Santos, La{\'{e}}cio L. and Carvalho, Rommel N. and Ladeira, Marcelo and Li, Weigang},
booktitle = {URSW@ISWC2016},
keywords = {bayesian,owl,situation},
title = {{A New Algorithm for Generating Situation-Specific Bayesian Networks Using Bayes-Ball Method}},
year = {2016}
}
@inproceedings{Fricker2011,
abstract = {In this paper we propose a sequential pattern mining method to analyze multimodal data streams using a quantitative temporal approach. While the existing algorithms can only find sequential orders of temporal events, this paper presents a new temporal data mining method focusing on extracting exact timings and durations of sequential patterns extracted from multiple temporal event streams. We present our method with its application to the detection and extraction of human sequential behavioral patterns over multiple multimodal data streams in human-robot interactions. Experimental results confirmed the feasibility and quality of our proposed pattern mining algorithm, and suggested a quantitative data-driven way to ground social interactions in a manner that has never been achieved before.},
author = {Fricker, Damian and {Hui Zhang} and {Chen Yu}},
booktitle = {2011 IEEE International Conference on Development and Learning (ICDL)},
doi = {10.1109/DEVLRN.2011.6037334},
isbn = {978-1-61284-989-8},
issn = {2161-9476},
keywords = {cognitive science,human robot interaction,sequential pattern mining},
month = {8},
pages = {1--6},
publisher = {IEEE},
title = {{Sequential pattern mining of multimodal data streams in dyadic interactions}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6037334 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6037334},
volume = {2},
year = {2011}
}
@article{Holthaus2011,
abstract = {Social interaction between humans takes place in the spatial environment on a daily basis. We occupy space for ourselves and respect the dynamics of spaces that are occupied by others. In human-robot interaction, spatial models are commonly used for structuring relatively far-away interactions or passing-by scenarios. This work instead, focuses on the transition between distant and close communication for an interaction opening. We applied a spatial model to a humanoid robot and implemented an attention system that is connected to it. The resulting behaviors have been verified in an online video study. The questionnaire revealed that these behaviors are applicable and result in a robot that has been perceived as more interested in the human and shows its attention and intentions earlier and to a higher degree than other strategies.},
author = {Holthaus, Patrick and Pitsch, Karola and Wachsmuth, Sven},
doi = {10.1007/s12369-011-0108-9},
isbn = {3-642-17247-4 978-3-642-17247-2},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Attention,Experimental evaluation,Human-robot interaction,Interaction opening,attention,hri,icub,spacial},
month = {11},
number = {4},
pages = {383--393},
title = {{How Can I Help?}},
url = {http://link.springer.com/10.1007/s12369-011-0108-9},
volume = {3},
year = {2011}
}
@article{Uckermann2012a,
abstract = {We present an algorithm to segment an unstruc- tured table top scene. Operating on the depth image of a Kinect camera, the algorithm robustly separates objects of previously unknown shape in cluttered scenes of stacked and partially occluded objects. The model-free algorithm finds smooth sur- face patches which are subsequently combined to form object hypotheses. We evaluate the algorithm regarding its robustness and real-time capabilities and discuss its advantages compared to existing approaches as well as its weak spots to be addressed in future work. We also report on an autonomous grasping experiment with the Shadow Robot Hand which employs the estimated shape and pose of segmented objects.},
author = {{\"{U}}ckermann, Andre and Elbrechter, Christof and Haschke, Robert and Ritter, Helge},
doi = {10.1109/IROS.2012.6385692},
isbn = {978-1-4673-1736-8},
journal = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
month = {10},
pages = {1734--1740},
publisher = {Ieee},
title = {{3D scene segmentation for autonomous robot grasping}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6385692},
year = {2012}
}
@inproceedings{Klingelschmitt2016,
abstract = {Situation recognition is a prerequisite for many advanced driver assistance systems as well as for partially and fully automated vehicles. Current situation recognition approaches focus mainly on estimating maneuvers of single scene entities. However, assessing multiple, possibly interacting, traffic participants simultaneously is crucial in complex traffic scenes and has hardly been investigated. Considering the variability and combinatorics of such scenarios, having specialized situation recognition systems covering each case directly is unrealistic. In this paper, we present a flexible framework for assessing generic traffic scenes with multiple interacting traffic participants. It is able to construct a fully interaction-respecting probabilistic situation assessment, while relying on reusable state-of-the-art single-entity-based maneuver predictions. The benefits and applicability are presented on a real-world data set. The evaluation indicates that the approach is not only able to reconstruct underlying interdependent probability distributions; it outperforms specially designed models, due to the reduced complexities of the single-entity-based recognition models.},
author = {Klingelschmitt, Stefan and Damerow, Florian and Willert, Volker and Eggert, Julian},
booktitle = {2016 IEEE Intelligent Vehicles Symposium (IV)},
doi = {10.1109/IVS.2016.7535533},
isbn = {978-1-5090-1821-5},
keywords = {situation,traffic},
month = {6},
pages = {1141--1148},
publisher = {IEEE},
title = {{Probabilistic situation assessment framework for multiple, interacting traffic participants in generic traffic scenes}},
url = {http://ieeexplore.ieee.org/document/7535533/},
volume = {2016-Augus},
year = {2016}
}
@techreport{Yumak2014,
abstract = {3D virtual humans and physical human-like robots can be used to interact with people in a remote location in order to increase the feeling of presence. In a tele-presence set-up their behaviors are driven by real participants. We envision that in the absence of the real users, when they have to leave or they do not want to do a repetitive task, the control of the robots can be handed to an artificial intelligence component to sustain the ongoing interaction. The point when human-mediated interaction is required again, the control can be given back to the real users. One of the main challenges in tele-presence research is the adaptation of 3D position and orientation of the remote participants to the actual physical environment to have appropriate eye-contact and gesture awareness in a group conversation. In case the human behind the robot and/or virtual human leaves, multi-party interaction should be handed to an artificial intelligence component. In this paper, we discuss the challenges in autonomous multi-party interaction between virtual characters, human-like robots and real participants and describe a prototype system to study these challenges.},
author = {Yumak, Zerrin and Ren, Jianfeng and Magnenat-Thalmann, Nadia and Yuan, Junsong},
keywords = {telepresence},
pages = {34},
publisher = {ACM Press},
title = {{Multi-party interaction with a virtual character and human-like robot: A case study}},
year = {2013}
}
@inproceedings{Jakobson2005,
abstract = {This paper is an introduction to the Workshop on Situation Management, SIMA 2005. We discuss the scope of the workshop, the big picture of situation management, and a summarization of the papers selected for inclusion in the workshop. Topics include Situation Knowledge Acquisi- tion, Learning {\&} Situation Recognition, Structural {\&} Be- havioral Modeling of Sensor Networks, Robotic Sensors {\&} Mobile Sensor Grids, Advanced Architectures for Situation Awareness, and Human-Centric Situation Management. We conclude with a discussion of hard, outstanding chal- lenges in Situation Management and future R{\&}D areas.},
author = {Jakobson, Gabriel and Lewis, L. and Matheus, C.J. and Kokar, M.M. and Buford, John},
booktitle = {MILCOM 2005 - 2005 IEEE Military Communications Conference},
doi = {10.1109/MILCOM.2005.1605908},
isbn = {0-7803-9393-7},
keywords = {situation},
pages = {1--7},
publisher = {IEEE},
title = {{Overview of Situation Management at SIMA 2005}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1605908},
year = {2005}
}
@article{Cristina2016,
abstract = {Nowadays, the Internet of Things gains more and more attention through cheap, highly interconnected hardware devices that are attached with sensors and actuators. This results in an instrumented environment that provides sufficient context information to drive what is called situation recognition. Situations are derived from large amounts of context data, which is difficult to handle. In this article, we present SitRS XT, an extension of our previously introduced situation recognition service SitRS, to enable situation recognition in near real time. SitRS XT provides easy to use situation recognition based on Complex Event Processing, which is highly efficient. The architecture and method of SitRS XT is described and evaluated through a prototypical implementation.},
author = {{Franco da Silva}, Ana Cristina and Hirmer, Pascal and Wieland, Matthias and Mitschang, Bernhard},
journal = {Journal of Information and Data Management},
keywords = {complex event processing,event processing,internet of things,situation,situation recognition,situation-awareness},
number = {1},
pages = {4--17},
title = {{SitRS XT – Towards Near Real Time Situation Recognition}},
volume = {7},
year = {2016}
}
@article{Jovanovic2004,
abstract = {The paper is about the issue of addressing in multi-party dialogues. Analysis of addressing behavior in face to face meetings results in the identification of several addressing mechanisms. From these we extract several utterance features and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed.},
annote = {Addressee identification

* verbal: 
** you (group vs single with context)
** names (single)
** we (group)
** open ended questions (group)
** open ended question cont. 'you' (single)
** dialogue acts: 
*** forward (flf) vs. backward looking (blf) utterances
*** (blf) responses of multiple actors to one utterance
are related to the same addressee

* nonverbal
** gaze
*** 'channel-control'
*** depends on meeting action/role
**** discussion,presentation,note-taking

* gesture: 
** may be used

context:
** interaction history
** meeting action history
** user context
** spatial context


* foa from head orientation reliability 88.7{\%}},
author = {Jovanovic, Natasa and op den Akker, Rieks},
journal = {Proceedings of the 5th SIGdial Workshop on Discourse and Dialogue},
keywords = {adressing,dialog,multiparty},
pages = {89--92},
title = {{Towards automatic addressee identification in multi-party dialogues}},
year = {2004}
}
@incollection{Bazzani12,
author = {Bazzani, L and Cristani, M and Paggetti, G and Tosato, D and Menegaz, G and Murino, V},
booktitle = {Video Analytics for Business Intelligence},
pages = {271--305},
title = {{Analyzing Groups: A Social Signaling Perspective}},
volume = {409},
year = {2012}
}
@inproceedings{Shriberg2005,
abstract = {Spontaneous conversation is optimized for human-human com- munication, but differs in some important ways from the types of speech for which human language technology is often devel- oped. This overview describes four fundamental properties of spontaneous speech that present challenges for spokenlanguage applications because they violate assumptions often applied in automatic processing technology.},
author = {Shriberg, Elizabeth},
booktitle = {European Conf. on Speech Communication and Technology (Eurospeech)},
keywords = {dialog,natural speech,spontanious speech},
pages = {1781--1784},
title = {{Spontaneous speech: How people really talk and why engineers should care}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.3765{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@book{Hall1969,
annote = {Only from other sourcecs. The correctness would need to be verified.

--- Rios-Martinez2015
It is possible to classify the space around a person with respect to social interaction in four specific zones whose distances from human body are listed below:
– the public zone {\textgreater} 3.6m 
– the social zone {\textgreater} 1.2m 
– the personal zone {\textgreater} 0.45m
– the intimate zone {\textless}= 0.45m

Distances vary with age, culture, relationship, context

--- Holthaus
- intimate distance (up to 0.46m)
- personal distance (ranging to 1.22m)
- social distance (at 3.70m)

difference between outer and inner phase of the respective class. 
inner phase of the public distance (7.60m) which itself is unlimited.},
author = {Hall, Edward T.},
isbn = {9780370013084},
publisher = {Bodley Head},
title = {{The Hidden Dimension: Man's Use of Space in Public and Private}},
url = {https://books.google.de/books?id=H6RgQgAACAAJ},
year = {1969}
}
@article{Neumann2017,
abstract = {Cooking is a complex activity of daily living that requires intuition, coordination, multitasking and time-critical planning abilities. We introduce KogniChef, a cognitive cooking assistive system that provides users with interactive, multi-modal and intuitive assistance while preparing a meal. Our system augments common kitchen appliances with a wide variety of sensors and user-interfaces, interconnected internally to infer the current state in the cooking process and to provide smart guidance. Our vision is to endow the system with the processing and the reasoning skills needed to guide a cook through recipes, similar to the assistance an expert chef would be able to provide on-site.},
author = {Neumann, Alexander and Elbrechter, Christof and Pfeiffer-Le{\ss}mann, Nadine and K{\~{o}}iva, Risto and Carlmeyer, Birte and R{\"{u}}ther, Stefan and Schade, Michael and {\"{U}}ckermann, Andr{\'{e}} and Wachsmuth, Sven and Ritter, Helge J.},
doi = {10.1007/s13218-017-0488-6},
issn = {0933-1875},
journal = {K{\"{u}}nstliche Intelligenz (KI)},
keywords = {Assist,Assisted living,Cooking,Human machine interaction,Smart home,assist,assisted,cooking,human machine interaction,living,smart home},
month = {8},
number = {3},
pages = {273--281},
title = {{KogniChef: A Cognitive Cooking Assistant}},
url = {http://link.springer.com/10.1007/s13218-017-0488-6},
volume = {31},
year = {2017}
}
@article{Mollahosseini2018,
abstract = {Both robotic and virtual agents could one day be equipped with social abilities necessary for effective and natural interaction with human beings. Although virtual agents are relatively inexpensive and flexible, they lack the physical embodiment present in robotic agents. Surprisingly, the role of embodiment and physical presence for enriching human-robot-interaction is still unclear. This paper explores how these unique features of robotic agents influence three major elements of human-robot face-to-face communication, namely the perception of visual speech, facial expression, and eye-gaze. We used a quantitative approach to disentangle the role of embodiment from the physical presence of a social robot, called Ryan, with three different agents (robot, telepresent robot, and virtual agent), as well as with an actual human. We used a robot with a retro-projected face for this study, since the same animation from a virtual agent could be projected to this robotic face, thus allowing comparison of the virtual agent's animation behaviors with both telepresent and the physically present robotic agents. The results of our studies indicate that the eye gaze and certain facial expressions are perceived more accurately when the embodied agent is physically present than when it is displayed on a 2D screen either as a telepresent or a virtual agent. Conversely, we find no evidence that either the embodiment or the presence of the robot improves the perception of visual speech, regardless of syntactic or semantic cues. Comparison of our findings with previous studies also indicates that the role of embodiment and presence should not be generalized without considering the limitations of the embodied agents.},
author = {Mollahosseini, Ali and Abdollahi, Hojjat and Sweeny, Timothy D. and Cole, Ron and Mahoor, Mohammad H.},
doi = {10.1016/j.ijhcs.2018.04.005},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {Embodiment,Physical presence,Retro-projected robots,Social robot,embodiment,physical presence},
month = {8},
number = {July 2017},
pages = {25--39},
publisher = {Elsevier Ltd},
title = {{Role of embodiment and presence in human perception of robots' facial cues}},
url = {https://doi.org/10.1016/j.ijhcs.2018.04.005 https://linkinghub.elsevier.com/retrieve/pii/S1071581918301745},
volume = {116},
year = {2018}
}
@article{Breiman2001,
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
issn = {1573-0565},
journal = {Machine Learning},
number = {1},
pages = {5--32},
title = {{Random Forests}},
url = {https://doi.org/10.1023/A:1010933404324},
volume = {45},
year = {2001}
}
@phdthesis{Stevenson2014,
abstract = {A key enabler of pervasive computing is the ability to drive service delivery through the analysis of situations: Semantically meaningful classifications of system state, identified through analysing the readings from sensors attached to the everyday objects that people interact with. Situation recognition is a mature area of research, with techniques primarily falling into two categories. Knowledge-based techniques use inference rules crafted by experts; however often they compensate poorly for sensing peculiarities. Learning-based ap- proaches excel at extracting patterns from noisy training data, however their lack of transparency can make it difficult to diagnose errors. In this thesis we propose a novel hybrid approach to situation recognition that combines both techniques. This offers improvements over each used individually, through not sacrificing the intelligibility of the decision processes that the use of machine learning alone often implies, and through providing better recognition accuracy through robust- ness to noise typically unattainable when developers use knowledge-based techniques in isolation. We present an ontology model and reasoning framework that supports the uniform modelling of pervasive environments, and infers additional knowledge from that which is specified, in a principled way. We use this as a basis from which to learn situation recognition models that exhibit comparable performance with more complex machine learning techniques, while retaining intelligibility. Finally, we extend the approach to construct ensemble classifiers with either improved recognition accuracy, intelligibility or both. To validate our approach, we apply the techniques to real-world data sets collected in smart-office and smart-home environments. We analyse the situation recognition performance and intelligibility of the decision processes, and compare the results to standard machine learning techniques and results published in the literature.},
author = {Stevenson, Graeme},
pages = {267},
school = {University of St Andrews},
title = {{An Approach to Situation Recognition Based on Learned Semantic Models}},
year = {2014}
}
@inproceedings{ros,
author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian P and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y},
booktitle = {ICRA Workshop on Open Source Software},
title = {{ROS: an open-source Robot Operating System}},
year = {2009}
}
@inproceedings{VanTurnhout2005,
abstract = {Against the background of developments in the area of speech-based and multimodal interfaces, we present research on determining the addressee of an utterance in the context of mixed human-human and multimodal human-computer interaction. Working with data that are taken from realistic scenarios, we explore several features with respect to their relevance to the question who is the addressee of an utterance: eye gaze both of speaker and listener, dialogue history and utterance length. With respect to eye gaze, we inspect the detailed timing of shifts in eye gaze between different communication partners (human or computer). We show that these features result in an improved classification of utterances in terms of addressee-hood relative to a simple classification algorithm that assumes that "the addressee is where the eye is", and compare our results to alternative approaches.},
address = {New York, New York, USA},
author = {van Turnhout, Koen and Terken, Jacques and Bakx, Ilse and Eggen, Berry},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/1088463.1088495},
isbn = {1595930280},
keywords = {eye gaze,multi party interaction,perceptive user interfaces},
number = {May 2014},
pages = {175},
publisher = {ACM Press},
title = {{Identifying the Intended Addressee in Mixed Human-Human and Human-Computer Interaction from Non-Verbal Features}},
url = {http://portal.acm.org/citation.cfm?doid=1088463.1088495},
year = {2005}
}
@incollection{Theelen2015,
abstract = {In our mission to advance innovation by industrial adoption of academic results, we perform many projects with high-tech industries. Favoring formal methods, we observe a gap between industrial needs in performance modeling and the analysis capabilities of formal methods for this goal. After clarifying this gap, we highlight some relevant deficiencies for state-of-the-art quantitative analysis techniques (focusing on model checking and simulation). As an ingredient to bridging the gap, we propose to unite domain-specific industrial contexts with academic performance approaches through Domain Specific Languages (DSLs). We illustrate our vision with examples from different high-tech industries and discuss lessons learned from the migration process of adopting it.},
address = {Cham},
author = {Carnevali, Laura and Nugent, Christopher and Patara, Fulvio and Vicario, Enrico},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-22264-6_3},
editor = {Campos, Javier and Haverkort, Boudewijn R.},
isbn = {978-3-319-22263-9},
issn = {16113349},
keywords = {Domain specific languages,Model checking,Performance analysis,Performance modelling,Quantitative analysis,Simulation},
pages = {38--53},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{A Continuous-Time Model-Based Approach to Activity Recognition for Ambient Assisted Living}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84944714858{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/978-3-319-22264-6 http://link.springer.com/10.1007/978-3-319-22264-6{\_}3},
volume = {9259},
year = {2015}
}
@article{Cassell1999,
abstract = {In this article we describe results froman experiment of user interaction with autonomous , human - like ( humanoid ) conversational agents . We hypothesize that for embodied conversational agents , nonverbal behaviors related to the process of conversation , what we call envelope feedback, is much more important than other feedback , such as emotional expression . We test this hypothesis by having subjects interact with three autonomous agents , all capable of full - duplex multimodal interaction: able to generate and recognize speech , intonation , facial displays , and gesture . Each agent , however , gave a different kind of feedback: ( 1 ) content - related only , ( 2 ) content + envelope feedback , and ( 3 ) content + emotional . Content-related feedback includes answering questions and executing commands; envelope feedback includes behaviors such as gaze , manual beat gesture , and head movements; emotional feedback includes smiles and looks of puzzlement . Subjects' evaluations of the systemwere collected with a questionnaire , and videotapes of their speech patterns and behaviors were scored according to how often the users repeated themselves , how often they hesitated , and how often they got frustrated . The results confirmour hypothesis that envelope feedback is more important in interaction than emotional feedback and that envelope feedback plays a crucial role in supporting the process of dialog . A secondary result fromthis study shows that users give our multimodal conversational humanoids very high ratings of lifelikeness and fluidity of interaction when the agents are capable of giving such feedback .},
author = {Cassell, Justine and Th{\'{o}}risson, Kristinn R.},
doi = {10.1080/088395199117360},
issn = {10876545},
journal = {Applied Artificial Intelligence},
number = {4-5},
pages = {519--538},
title = {{The Power of a Nod and a Glance: Envelope vs. Emotional Feedback in Animated Conversational Agents}},
volume = {13},
year = {1999}
}
@inproceedings{Schulz2019,
abstract = {While many social robots already include carefully designed robotic faces, functional robot eyes that meet human expectations are still an open challenge. As a consequence, many robots either have cameras separated from their robot eyes or active camera heads missing an anthropomorphic face. In this paper, we propose a new robot eye that is integrated in the anthropomorphic robot head Floka and fulfills a similar technical specification as a human eye including zero backlash, an increased range of motion, high velocities and accelerations, an integrated high resolution camera, and fast actuated eyelids. The robot eye is built using state-of-the-art off-the-shelf components and the CAD model of our prototype is available free of charge on request for non-commercial applications. We evaluate the technical properties of the robot eye and show that it meets and partially outperforms human eye movements and saccades.},
author = {Schulz, Simon and Borgsen, Sebastian Meyer Zu and Wachsmuth, Sven},
booktitle = {International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2019.8794319},
isbn = {978-1-5386-6027-0},
keywords = {Humanoid Robots,Mechanism Design,Social Human-Robot Interaction},
month = {5},
pages = {2524--2530},
publisher = {IEEE},
title = {{See and Be Seen – Rapid and Likeable High-Definition Camera-Eye for Anthropomorphic Robots}},
url = {https://ieeexplore.ieee.org/document/8794319/},
volume = {2019-May},
year = {2019}
}
@incollection{Kitazawa2010,
abstract = {This study investigates the Information Process Space (IPS) of pedestrians, which has been widely used in microscopic pedestrian movement simulation models. IPS is a conceptual framework to define the spatial extent within which all objects are considered as potential obstacles for each pedestrian when computing where to move next. The particular focus of our study was identifying the size and shape of IPS by examining observed gaze patterns of pedestrians. A series of experiments was conducted in a controlled laboratory environment, in which up to 4 participants walked on a platform at their natural speed. Their gaze patterns were recorded by a head-mounted eye tracker and walking paths by laser-range-scanner–based tracking systems at the frequency of 25Hz. Our findings are threefold: pedestrians pay much more attention to ground surfaces to detect immediate potential environmental hazards than fixating on obstacles; most of their fixations fall within a cone-shape area rather than a semicircle; and the attention paid to approaching pedestrians is not as high as that paid to static obstacles. These results led to an insight that the structure of IPS should be re-examined by researching directional characteristics of pedestrians' vision.},
address = {Berlin, Heidelberg},
author = {Kitazawa, Kay and Fujiyama, Taku},
booktitle = {Pedestrian and Evacuation Dynamics},
doi = {10.1007/978-3-642-04504-2_7},
editor = {Klingsch, Wolfram W. F. and Rogsch, Christian and Schadschneider, Andreas and Schreckenberg, Michael},
isbn = {978-3-642-04503-5},
keywords = {collision avoidance,information process,pedestrian vision},
pages = {95--108},
publisher = {Springer Berlin Heidelberg},
title = {{Pedestrian Vision and Collision Avoidance Behavior: Investigation of the Information Process Space of Pedestrians Using an Eye Tracker}},
url = {http://link.springer.com/10.1007/978-3-642-04504-2{\_}7},
year = {2010}
}
@inproceedings{DAniello2015,
author = {DAniello, Giuseppe and Gaeta, Matteo and Loia, Vincenzo and Orciuoli, Francesco and Sampson, Demetrios G.},
booktitle = {2015 International Conference on Intelligent Networking and Collaborative Systems},
doi = {10.1109/INCoS.2015.59},
isbn = {978-1-4673-7695-2},
keywords = {-seamless learning,as an active and,con-,dss,i,i ntroduction and motivation,learning,self-regulated learning is defined,situaion recognition,situation awareness,situation modeling,situation recognition,situation theory},
month = {9},
pages = {440--445},
publisher = {IEEE},
title = {{Situation Awareness Enabling Decision Support in Seamless Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7312113},
year = {2015}
}
@inbook{luckmann1995,
address = {Cambridge},
author = {Luckmann, Thomas},
booktitle = {Social intelligence and interaction},
doi = {10.1017/CBO9780511621710.011},
editor = {Goody, Esther N.Editor},
pages = {175--186},
publisher = {Cambridge University Press},
title = {{Interaction planning and intersubjective adjustment of perspectives by communicative genres}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511621710A019},
year = {1995}
}
@article{Remagnino2007,
abstract = {Ambient intelligence is a term coined in Europe at the turn of the century to identify the methodologies and technologies that enable an environment to better respond to a user's needs. The core concept is to endowan environment with the computational power sufficient to sense its inhabitants and to interpret their actions and interactions in order to anticipate their needs, supply them with necessary information, and/or to act on their behalves. Ambient intelligence covers a wide spectrum of applications, ranging from entertainment services to safety and security. This special issue introduces the concept of intelligent environments, and explores the algorithms required to build such systems. The articles address a range of applications but emphasize domestic contexts (i.e., smart homes),which have been a major motivator of ambi- ent intelligence research. Taken together, the articles provide a window into the technologies most relevant to this area of research, emphasizing agent-based methods for information fusion, situation recognition, planning, monitoring, and behavior modeling.},
author = {Remagnino, Paolo and Shapiro, Daniel},
doi = {10.1111/j.1467-8640.2007.00312.x},
issn = {08247935},
journal = {Computational Intelligence},
month = {12},
number = {4},
pages = {393--394},
title = {{ARTIFICIAL INTELLIGENCE METHODS FOR AMBIENT INTELLIGENCE}},
url = {http://doi.wiley.com/10.1111/j.1467-8640.2007.00312.x},
volume = {23},
year = {2007}
}
@article{Nakano2015,
abstract = {Gaze is an important nonverbal feedback signal in multiparty face-to-face conversations. It is well known that gaze behaviors differ depending on participation role: speaker, addressee, or side participant. In this study, we focus on dominance as another factor that affects gaze. First, we conducted an empirical study and analyzed its results that showed how gaze behaviors are affected by both dominance and participation roles. Then, using speech and gaze information that was statistically significant for distinguishing the more dominant and less dominant person in an empirical study, we established a regression-based model for estimating conversational dominance. On the basis of the model, we implemented a dominance estimation mechanism that processes online speech and head direction data. Then we applied our findings to human-robot interaction. To design robot gaze behaviors, we analyzed gaze transitions with respect to participation roles and dominance and implemented gaze-transition models as robot gaze behavior generation rules. Finally, we evaluated a humanoid robot that has dominance estimation functionality and determines its gaze based on the gaze models, and we found that dominant participants had a better impression of less dominant robot gaze behaviors. This suggests that a robot using our gaze models was preferred to a robot that was simply looking at the speaker. We have demonstrated the importance of considering dominance in human-robot multiparty interaction. ACM Reference Format: Yukiko I. Nakano, Takashi Yoshino, Misato Yatsushiro, and Yutaka Takase. 2015. Generating robot gaze on the basis of participation roles and dominance estimation in multiparty interaction. ACM Trans. Interact. Intell. Syst. 5, 4, Article 22 (December 2015), 23 pages.},
author = {Nakano, Yukiko I. and Yoshino, Takashi and Yatsushiro, Misato and Takase, Yutaka},
doi = {10.1145/2743028},
issn = {21606455},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {conversational roles,gaze,multi person,turn-taking},
month = {12},
number = {4},
pages = {1--23},
title = {{Generating Robot Gaze on the Basis of Participation Roles and Dominance Estimation in Multiparty Interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2866565.2743028},
volume = {5},
year = {2015}
}
@techreport{Heylighen1999,
abstract = {A new concept of formality of linguistic expressions is introduced and argued to be the most important dimension of variation between styles or registers. Formality is subdivided into "deep" formality and "surface" formality. Deep formality is defined as avoidance of ambiguity by minimizing the context-dependence and fuzziness of expressions. This is achieved by explicit and precise description of the elements of the context needed to disambiguate the expression. A formal style is characterized by detachment, accuracy, rigidity and heaviness; an informal style is more flexible, direct, implicit, and involved, but less informative. An empirical measure of formality, the F-score, is proposed, based on the frequencies of different word classes in the corpus. Nouns, adjectives, articles and prepositions are more frequent in formal styles; pronouns, adverbs, verbs and interjections are more frequent in informal styles. It is shown that this measure, though coarse-grained, adequately distinguishes more from less formal genres of language production, for some available corpora in Dutch, French, Italian, and English. A factor similar to the F-score automatically emerges as the most important one from factor analyses applied to extensive data in 7 different languages. Different situational and personality factors are examined which determine the degree of formality in linguistic expression. It is proposed that formality becomes larger when the distance in space, time or background between the interlocutors increases, and when the speaker is male, introverted or academically educated. Some empirical evidence and a preliminary theoretical explanation for these propositions is discussed.},
author = {Heylighen, Francis and Dewaele, Jean-marc},
keywords = {formality,language formality},
title = {{Formality of Language : definition , measurement and behavioral determinants}},
volume = {1999},
year = {1999}
}
@inproceedings{Zancanaro2006,
abstract = {In this paper, we discuss a machine learning approach to automatically$\backslash$ndetect functional roles played by participants in a face to face$\backslash$ninteraction. We shortly introduce the coding scheme we used to classify$\backslash$nthe roles of the group members and the corpus we collected to assess$\backslash$nthe coding scheme reliability as well as to train statistical systems$\backslash$nfor automatic recognition of roles. We then discuss a machine learning$\backslash$napproach based on multi-class SVM to automatically detect such roles$\backslash$nby employing simple features of the visual and acoustical scene.$\backslash$nThe effectiveness of the classification is better than the chosen$\backslash$nbaselines and although the results are not yet good enough for a$\backslash$nreal application, they demonstrate the feasibility of the task of$\backslash$ndetecting group functional roles in face to face interactions.},
address = {New York, New York, USA},
author = {Zancanaro, Massimo and Lepri, Bruno and Pianesi, Fabio},
booktitle = {Proceedings of the 8th international conference on Multimodal interfaces - ICMI '06},
doi = {10.1145/1180995.1181003},
isbn = {159593541X},
keywords = {dialog,group,group interaction,intelligent environments,roles,support vector machines},
pages = {28},
publisher = {ACM Press},
title = {{Automatic detection of group functional roles in face to face interactions}},
url = {http://doi.acm.org/10.1145/1180995.1181003 http://portal.acm.org/citation.cfm?doid=1180995.1181003},
year = {2006}
}
@article{Zotowski2015,
abstract = {Anthropomorphism is a phenomenon that describes the human tendency to see human-like shapes in the environment. It has considerable consequences for people's choices and beliefs. With the increased presence of robots, it is important to investigate the optimal design for this technology. In this paper we discuss the potential benefits and challenges of building anthropomorphic robots, from both a philosophical perspective and from the viewpoint of empirical research in the fields of human--robot interaction and social psychology. We believe that this broad investigation of anthropomorphism will not only help us to understand the phenomenon better, but can also indicate solutions for facilitating the integration of human-like machines in the real world.},
author = {Z{\l}otowski, Jakub and Proudfoot, Diane and Yogeeswaran, Kumar and Bartneck, Christoph},
doi = {10.1007/s12369-014-0267-6},
isbn = {1875-4791},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Anthropomorphism,Child-machines,Contact theory,Human–robot interaction,Turing,Uncanny valley},
month = {6},
number = {3},
pages = {347--360},
publisher = {Springer Netherlands},
title = {{Anthropomorphism: Opportunities and Challenges in Human–Robot Interaction}},
url = {http://dx.doi.org/10.1007/s12369-014-0267-6 http://link.springer.com/10.1007/s12369-014-0267-6},
volume = {7},
year = {2015}
}
@article{Holler2015a,
abstract = {One intriguing feature of the human communication system is the interactional infrastructure it builds on. In both dyadic and multi-person interactions, conversation is highly structured and organized according to set principles (Sacks et al., 1974). Human adult interaction is characterized by a mechanism of exchange based on alternating (and relatively short) bursts of information. In the majority of cases, only one person tends to speak at a time and each contribution usually receives a response. What is remarkable is the precise timing of these sequential contributions, resulting in gaps between speaking turns averaging around just 200 ms (Stivers et al., 2009). From psycholinguistic experiments, we know that the time it takes to produce even simple one-word-utterances (min. 600 ms, Indefrey and Levelt, 2004) by far exceeds this average gap duration, hinting at the complexity of the cognitive processes that must be involved (Levinson, 2013). While the behavioral principles governing turn-taking in interaction have been researched for some decades—primarily by scholars of conversation analysis—the cognitive underpinnings of the human turn-taking system have long remained elusive. Recently, psycholinguists have begun to explore the cognitive and neural processes that allow us to deal effectively with the immensely complex task of taking turns on time. Amongst other things, this has highlighted the anticipatory, predictive processes that must be at work, as well as the different layers of processing allowing production planning and comprehension to take place simultaneously (de Ruiter et al., 2006; Magyari and de Ruiter; B{\"{o}}gels et al., 2015). These insights mesh well with the conversation analytic literature that has illuminated the interactional environments in which individual turns are embedded: their sequential organization and the use of conventionalized linguistic constructions allow for the projection of upcoming talk, as well as for the recognition of points of possible completions in the turn which make transition to the next speaker relevant (Sacks et al., 1974; Ford and Thompson, 1996; Schegloff, 2007). The articles in this Research Topic bring together these as yet largely independent lines of research to elucidate our understanding of turn-taking from multiple perspectives and aim to foster future synergies. In addition to exploring the adult psycholinguistic machinery and its workings, researchers have begun to wonder how and when the required cognitive and social processes mature in children, as well as how they compare to those in other species. Levinson (2006) proposed that human beings are inherently social and interactive in orientation. He argues that an " interaction engine " may lie at the heart of children's early predisposition for turn-taking. Likewise, this particular human capacity might explain the strong cultural universals in the structure of human interaction as well as the striking commonalities and differences in communication systems brought about by the course of evolution. The present Research Topic provides a collection of experimental and observational empirical studies using qualitative and quantitative approaches, complemented by articles offering reviews, Holler et al. Editorial: Turn-Taking in Human Communicative Interaction opinions, and models. They aim to inform the reader about the most recent advances in our endeavor of unraveling the workings of the human turn-taking system in communicative interaction. The contributions are organized into six sections: (1) Foundations of turn-taking, (2) Signals and mechanisms for prediction and timing, (3) Planning next turns in conversation, (4) Effects of context and function on timing, (5) Turn-taking in signed languages, and (6) Development of turn-taking skills.},
author = {Holler, Judith and Kendrick, Kobin H. and Casillas, Marisa and Levinson, Stephen C.},
doi = {10.3389/fpsyg.2015.01919},
isbn = {9782889198252},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Conversation,Language processing,Prediction,Projection,Social interaction,Turn transitions,Turn-taking,Turn-timing},
month = {12},
number = {DEC},
pages = {1--4},
title = {{Editorial: Turn-Taking in Human Communicative Interaction}},
url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.01919/abstract},
volume = {6},
year = {2015}
}
@inproceedings{Li2012,
abstract = {For robots to perform interaction with multiple persons, they have to be able to identify the addressees to interact with. We classify the methods of addressee detection and selection into two categories, namely, passive and active approaches. For passive approaches, the robot is programmed to detect a predefined signal, e.g., a voice command or a specific gesture, from a person who is supposed to be the addressee. In contrast, for active approaches, the robot is able to select a person as an addressee based on subtle cues that are inferred from the human pose, gaze, and facial expression. We present two new approaches for attention-based addressee selec- tion, one is a passive method and the other is an active method. The passive method is designed for the robot to recognize com- mon hand-waving gesture, where a Bayessian ensemble approach is proposed to fuse hand detections from depth segmentation, palm shape, skin color, and body pose. The active method is developed for the robot to perform natural interaction with multiple persons. It employs a novel human attention estimation algorithm based on human de-tection, tracking, upper body pose recogni-tion, face de- tection, gaze detection, lip motion analysis, and facial expression recognition. Extensive experiments have been conducted and the effectiveness of the proposed approaches is reported.},
address = {New York, New York, USA},
author = {Li, Liyuan and Xu, Qianli and Tan, Yeow Kee},
booktitle = {Proceedings of the Workshop at SIGGRAPH Asia on - WASA '12},
doi = {10.1145/2425296.2425319},
isbn = {9781450318358},
keywords = {addressee,attention,attention estimation,human robot interaction,multi person,robotics,service robot,social robot,visual perception},
number = {212},
pages = {131},
publisher = {ACM Press},
title = {{Attention-based addressee selection for service and social robots to interact with multiple persons}},
url = {http://dl.acm.org/citation.cfm?id=2425296.2425319 http://dl.acm.org/citation.cfm?doid=2425296.2425319},
volume = {1},
year = {2012}
}
@inproceedings{Cheng2008,
abstract = {With more and more applications available on mobile devices, it has become increasingly difficult for users to find a desired application. Although research has been conducted for situation-aware recommendations on mobile devices, none addresses this problem; most research is for media content recommendations. Moreover, existing approaches assume predefined situations and/or user-specified profiles; some require users to intentionally train their devices before using them for recommendations. We believe that what defines a situation and what applications are preferred in the situation not only vary from user to user but also change over time, and therefore these assumptions and requirements are impractical for ordinary consumers. In this paper, we will describe our approach of using unsupervised learning, specifically co-clustering, to derive latent situation-based patterns from usage logs of user interactions with the device and environments and use the patterns for task and communication mode recommendations.},
author = {Cheng, D and Song, H and Cho, H and Jeong, S and Kalasapur, S and Messer, A},
booktitle = {2008 The Second International Conference on Next Generation Mobile Applications, Services, and Technologies},
doi = {10.1109/NGMAST.2008.104},
isbn = {978-0-7695-3333-9},
pages = {228--233},
publisher = {IEEE},
title = {{Mobile Situation-Aware Task Recommendation Application}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4756438},
year = {2008}
}
@article{Koller1997,
abstract = {Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of pro- gramming using logical circuits. In this paper, we de- scribe an object-oriented Bayesian network (OOBN) lan- guage, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian net- work fragment to describe the probabilistic relations be- tween the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to pro- vide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inher- itance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochas- tic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural infor- mation encoded by an OOBN—particularly the encap- sulation of variables within an object and the reuse of model fragments in different contexts—can also be used to speed up the inference process.},
archivePrefix = {arXiv},
author = {Hepler, Amanda B. and Weir, Bruce S.},
doi = {10.1016/j.fsigen.2007.12.003},
eprint = {1302.1554},
isbn = {1-55860-485-5},
issn = {18724973},
journal = {Forensic Science International: Genetics},
keywords = {object oriented bayesian networks},
month = {6},
number = {3},
pages = {166--175},
title = {{Object-oriented Bayesian networks for paternity cases with allelic dependencies}},
url = {http://dl.acm.org/citation.cfm?id=2074262 http://linkinghub.elsevier.com/retrieve/pii/S1872497307004036},
volume = {2},
year = {2008}
}
@article{Groh,
abstract = {We motivate why models of social networks on small time and space scales and especially social situations are valuable for new useful social networking services that are context aware and socially aware. A model for social situations and pragmatic approaches for instantiating such models with the help of mobile devices is introduced. Two example applications are discussed: Social Life-logging and a communication service with social situations as recipients. Social Life-logging aims at the recording and later retrieval of social situations, which represent valuable granularity elements that simplify retrieval and improve usefulness in comparison or in addition to known multimedia-based life-logging concepts. Social situations as recipients for communication acts support an interesting new mode of communication.},
annote = {Study: 5 persons, manual noting interactions (touple). Testign situaltion recall with help of 'life-log'


Def. Individual context: 
* related to users 
** current time
** current place
** features relevant for current interactions
** individual parameters
** characteristics of physical environment


(Sometimes) Situations, individual social context 
* closely related
-{\textgreater} may be deducted from each other


Social situation model: (T,S,P,C)
* Time span
* subset of 3D Space
* set of Participants
* Content-tags, semantic aspects (purpose, topic, ...)
Situation may overlap and entail each other},
author = {Groh, Georg and Lehmann, Alexander and Wang, Tianyu and Huber, Stefan and Hammerl, Felix},
keywords = {context aware computing,life logging,mobile computing,reality mining,social,social computing,social context,social networks,social signal processing,social signals,social situation,social situation model},
title = {{Applications for social situation models}},
year = {2010}
}
@inproceedings{Verweij2017,
address = {New York, New York, USA},
author = {Verweij, David and Esteves, Augusto and Khan, Vassilis-Javed and Bakker, Saskia},
booktitle = {International Conference on Interactive Surfaces and Spaces (ISS)},
doi = {10.1145/3132272.3132283},
isbn = {9781450346917},
pages = {466--468},
publisher = {ACM Press},
title = {{Smart Home Control using Motion Matching and Smart Watches}},
url = {https://doi.org/10.1145/3132272.3132283 http://dl.acm.org/citation.cfm?doid=3132272.3132283},
year = {2017}
}
@inproceedings{openposep,
author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
booktitle = {Computer Vision and Pattern Recognition (CVPR)},
title = {{Convolutional Pose Machines}},
year = {2016}
}
@inproceedings{Huang2016,
abstract = {Efficient collaboration requires collaborators to monitor the behaviors of their partners, make inferences about their task intent, and plan their own actions accordingly. To work seamlessly and efficiently with their human counterparts, robots must similarly rely on predictions of their users' intent in planning their actions. In this paper, we present an anticipatory control method that enables robots to proactively perform task actions based on anticipated actions of their human partners. We implemented this method into a robot system that monitored its user's gaze, predicted his or her task intent based on observed gaze patterns, and performed anticipatory task actions according to its predictions. Results from a humanrobot interaction experiment showed that anticipatory control enabled the robot to respond to user requests and complete the task faster—2.5 seconds on average and up to 3.4 seconds—compared to a robot using a reactive control method that did not anticipate user intent. Our findings highlight the promise of performing anticipatory actions for achieving efficient humanrobot teamwork.},
author = {Huang, Chien-Ming and Mutlu, Bilge},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1109/HRI.2016.7451737},
isbn = {978-1-4673-8370-7},
keywords = {Action observation,Anticipatory action,Gaze,Human-robot collaboration,Intent prediction},
month = {3},
number = {Section V},
pages = {83--90},
publisher = {ACM/IEEE},
title = {{Anticipatory Robot Control for Efficient Human-Robot Collaboration}},
url = {http://ieeexplore.ieee.org/document/7451737/},
volume = {2016-April},
year = {2016}
}
@inproceedings{Huettenrauch2006,
abstract = {Co-presence and embodied interaction are two fundamental characteristics of the command and control situation for service robots. This paper presents a study of spatial distances and orientation of a robot with respect to a human user in an experimental setting. Relevant concepts of spatiality from social interaction studies are introduced and related to human-robot interaction (HRI). A Wizard-of-Oz study quantifies the observed spatial distances and spatial formations encountered. However, it is claimed that a simplistic parameterization and measurement of spatial interaction misses the dynamic character and might be counter-productive in the design of socially appropriate robots},
author = {Huettenrauch, Helge and Eklundh, Kerstin and Green, Anders and Topp, Elin},
booktitle = {2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2006.282535},
isbn = {1-4244-0258-1},
issn = {2153-0858},
keywords = {Spatiality in human-robot interaction},
month = {10},
pages = {5052--5059},
publisher = {IEEE},
title = {{Investigating Spatial Relationships in Human-Robot Interaction}},
url = {http://ieeexplore.ieee.org/document/4059223/},
year = {2006}
}
@incollection{Kendon2010,
author = {Kendon, Adam},
doi = {10.1007/978-3-642-12397-9_1},
pages = {1--15},
title = {{Spacing and Orientation in Co-present Interaction}},
url = {http://link.springer.com/10.1007/978-3-642-12397-9{\_}1},
year = {2010}
}
@book{Clark1996,
address = {Cambridge},
author = {Clark, Herbert H.},
doi = {10.1017/CBO9780511620539},
isbn = {9780511620539},
publisher = {Cambridge University Press},
title = {{Using language}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511620539},
year = {1996}
}
@inproceedings{Mayer2014,
abstract = {For the Internet of Things to be adopted in people's homes and at their workplaces, it is important to provide mechanisms that support them when controlling and monitoring smart things in their surroundings. We present the concept of user interface beaming, where the capabilities of different personal wearable computers are combined to allow users to conveniently interact with smart things in their environment. Smartglasses are used to select a target smart thing by means of current object recognition technologies. Then, an appropriate user interface for the target is rendered on the user's smartwatch. This interface is continuously updated to reflect state changes of the target and can be used to interact with that smart thing using different interaction modalities.},
author = {Mayer, Simon and Soros, Gabor},
booktitle = {International Conference on Wearable and Implantable Body Sensor Networks Workshops},
doi = {10.1109/BSN.Workshops.2014.17},
isbn = {978-1-4799-6136-8},
keywords = {Interaction,Smart Environment,Smartglasses,Smartphone,Smartwatch,Wearable Computer},
month = {6},
pages = {46--49},
publisher = {IEEE},
title = {{User Interface Beaming -- Seamless Interaction with Smart Things Using Personal Wearable Computers}},
url = {http://ieeexplore.ieee.org/document/6970626/},
year = {2014}
}
@incollection{Rehm2005,
abstract = {Intelligent Technologies for Interactive Entertainment},
author = {Rehm, Matthias and Andr{\'{e}}, Elisabeth and Nischt, Michael},
booktitle = {Intelligent Technologies for Interactive Entertainment},
doi = {10.1007/11590323_47},
editor = {Maybury, Mark and Stock, Oliviero and Wahlster, Wolfgang},
isbn = {3540305092},
pages = {336--336},
title = {{Lets Come Together – Social Navigation Behaviors of Virtual and Real Humans}},
url = {http://link.springer.com/10.1007/11590323{\_}47},
volume = {3814 LNAI},
year = {2005}
}
@inproceedings{Mutlu2009,
abstract = {During conversations, speakers establish their and others' participant roles (who participates in the conversation and in what capacity)-or "footing" as termed by Goffman-using gaze cues. In this paper, we study how a robot can establish the participant roles of its conversational partners using these cues. We designed a set of gaze behaviors for Robovie to signal three kinds of participant roles: addressee, bystander, and overhearer. We evaluated our design in a controlled laboratory experiment with 72 subjects in 36 trials. In three conditions, the robot signaled to two subjects, only by means of gaze, the roles of (1) two addressees, (2) an addressee and a bystander, or (3) an addressee and an overhearer. Behavioral measures showed that subjects' participation behavior conformed to the roles that the robot communicated to them. In subjective evaluations, significant differences were observed in feelings of groupness between addressees and others and liking between overhearers and others. Participation in the conversation did not affect task performance-measured by recall of information presented by the robot-but affected subjects' ratings of how much they attended to the task.},
address = {New York, New York, USA},
author = {Mutlu, Bilge and Shiwa, Toshiyuki and Kanda, Takayuki and Ishiguro, Hiroshi and Hagita, Norihiro},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1145/1514095.1514109},
isbn = {9781605584041},
keywords = {11,addition a,addressee,conversational participation,dialog,footing,gaze,multiparty,non participant,participant roles,participation structure,robovie,scenario differs with},
number = {1},
pages = {61},
publisher = {ACM Press},
title = {{Footing in Human-Robot Conversations}},
url = {http://portal.acm.org/citation.cfm?id=1514109 http://portal.acm.org/citation.cfm?doid=1514095.1514109},
volume = {2},
year = {2009}
}
@article{Greenberg1991,
author = {Greenberg, Saul and Marquardt, Nicolai and Diaz-marino, Rob and Wang, Miaosen},
journal = {Interactions},
keywords = {proxemics,smart environments,ubicomp},
number = {1},
pages = {42--50},
title = {{Proxemic Interactions: The New Ubicomp?}},
volume = {18},
year = {2011}
}
@inproceedings{Vazquez2016,
abstract = {— We explore online reinforcement learning tech-niques to find good policies to control the orientation of a mobile robot during social group conversations. In this scenario, we assume that the correct behavior for the robot should convey attentiveness to the focus of attention of the conversation. Thus, the robot should turn towards the speaker. Our results from tests in a simulated environment show that a new state representation that we designed for this problem can be used to find good policies for the robot. These policies can generalize across interactions with different numbers of people and can handle various levels of sensing noise.},
author = {Vazquez, Marynel and Steinfeld, Aaron and Hudson, Scott E},
booktitle = {2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
doi = {10.1109/ROMAN.2016.7745088},
isbn = {978-1-5090-3929-6},
month = {8},
pages = {36--43},
publisher = {IEEE},
title = {{Maintaining awareness of the focus of attention of a conversation: A robot-centric reinforcement learning approach}},
url = {http://ieeexplore.ieee.org/document/7745088/},
year = {2016}
}
@inproceedings{Batz2009,
abstract = {We consider the recognition of dangerous situations in vehicle traffic. Unscented Kalman filters are used to predict vehicle trajectories within a short prediction horizon [to, to +{\~{}}t]. Based on this prediction, for each vehicle pair the mutual distance is computed for [to, to+{\~{}}t], whereby the distance accounts for the geometric distance, for the prediction uncertainties as well as for the spatial dimensions of the vehicles. If at least one of the mutual distances falls below a distance threshold E within [to, to + {\~{}}t], then a dangerous situation arises for the cooperative group and may lead to an autonomous cooperative driving manoeuvre. This approach allows the usage of the system in a mixed environment (only some vehicles are cooperative and cognitive). Obstacles can also be handled. The key issues in this ongoing research work are the recognition and classification of dangerous situations and the formation of a cooperative group constituting an operational unit. A common relevant picture within a group coordinator fuses the necessary information from all cooperative vehicles of the group and forms the basis for situation recognition and classification. This paper is a step to expand a Cooperative Collision Warning System (CCWS) to an integrated Cooperative Collision Avoidance and Cooperative Collision Mitigation System (CCAMS).},
author = {Batz, Thomas and Watson, Kym and Beyerer, Jurgen},
booktitle = {2009 IEEE Intelligent Vehicles Symposium},
doi = {10.1109/IVS.2009.5164400},
isbn = {978-1-4244-3503-6},
month = {6},
pages = {907--912},
publisher = {IEEE},
title = {{Recognition of dangerous situations within a cooperative group of vehicles}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5164400},
year = {2009}
}
@inproceedings{Yamazaki2006,
abstract = {Due to progress in wired and wireless home networking, sensor networks, networked appliances, mechanical and control engineering, and computers, we can build smart homes, and many smart home projects are currently proceeding throughout the world. However, we have to be careful not to repeat the same mistake that was made with home automation technologies that were booming in the 1970s. That is, [total?] automation should not be a goal of smart home technologies. I believe the following points are important in construction of smart homes from users¿ viewpoints: development of interface technologies between humans and systems for detection of human intensions, feelings, and situations; improvement of system knowledge; and extension of human activity support outside homes to the scopes of communities, towns, and cities.},
author = {Yamazaki, Tatsuya},
booktitle = {2006 International Conference on Hybrid Information Technology},
doi = {10.1109/ICHIT.2006.253633},
isbn = {0-7695-2674-8},
month = {11},
pages = {350--355},
publisher = {IEEE},
title = {{Beyond the Smart Home}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4021238},
volume = {2},
year = {2006}
}
@article{Alameda-Pineda2016,
abstract = {Studying free-standing conversational groups (FCGs) in unstructured social settings (e.g., cocktail party ) is gratifying due to the wealth of information available at the group (mining social networks) and individual (recognizing native behavioral and personality traits) levels. However, analyzing social scenes involving FCGs is also highly challenging due to the difficulty in extracting behavioral cues such as target locations, their speaking activity and head/body pose due to crowdedness and presence of extreme occlusions. To this end, we propose SALSA, a novel dataset facilitating multimodal and Synergetic sociAL Scene Analysis, and make two main contributions to research on automated social interaction analysis: (1) SALSA records social interactions among 18 participants in a natural, indoor environment for over 60 minutes, under the poster presentation and cocktail party contexts presenting difficulties in the form of low-resolution images, lighting variations, numerous occlusions, reverberations and interfering sound sources; (2) To alleviate these problems we facilitate multimodal analysis by recording the social interplay using four static surveillance cameras and sociometric badges worn by each participant, comprising the microphone, accelerometer, bluetooth and infrared sensors. In addition to raw data, we also provide annotations concerning individuals' personality as well as their position, head, body orientation and F-formation information over the entire event duration. Through extensive experiments with state-of-the-art approaches, we show (a) the limitations of current methods and (b) how the recorded multiple cues synergetically aid automatic analysis of social interactions. SALSA is available at http://tev.fbk.eu/salsa.},
archivePrefix = {arXiv},
author = {Alameda-Pineda, Xavier and Staiano, Jacopo and Subramanian, Ramanathan and Batrinca, Ligia and Ricci, Elisa and Lepri, Bruno and Lanz, Oswald and Sebe, Nicu},
doi = {10.1109/TPAMI.2015.2496269},
eprint = {1506.06882},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Dataset,F-formations,Multimodal group behavior analysis,f-formations,free-standing conversational groups,head and body pose,multimodal social data sets,personality,personality traits,speaker recognition,tracking},
month = {8},
number = {8},
pages = {1707--1720},
title = {{Salsa: A Novel Dataset for Multimodal Group Behavior Analysis}},
url = {http://ieeexplore.ieee.org/document/7313015/},
volume = {38},
year = {2016}
}
@article{Haussermann2010,
abstract = {Using spatial models as a shared common basis of information about the environment for different kinds of context- aware systems has been a heavily researched topic in the last years. Thereby the research focused on how to create, to update, and to merge spatial models so as to enable highly dynamic, consistent and coherent spatial models at large scale. In this paper however, we want to concentrate on how context-aware applications could use this information so as to adapt their behavior according to the situation they are in. The main idea is to provide the spatial model infrastructure with a situation recognition component based on generic situation templates. A situation template is – as part of a much larger situation template library – an abstract, machine- readable description of a certain basic situation type, which could be used by different applications to evaluate their situation. In this paper, different theoretical and practical issues – technical, ethical and philosophical ones – are discussed important for understanding and developing situation dependent systems based on situation templates. A basic system design is presented which allows for the reasoning with uncertain data using an improved version of a learning algorithm for the automatic adaption of situation templates. Finally, for supporting the development of adaptive applications, we present a new situation-aware adaptation concept based on workflows.},
author = {H{\"{a}}ussermann, Kai and Hubig, Christoph and Levi, Paul and Leymann, Frank and Siemoneit, Oliver and Wieland, Matthias and Zweigle, Oliver},
journal = {World Academy of Science, Engineering {\&} Technology},
keywords = {based on,context-awareness,ethics,facilitation of system use,ontology,s,situation recognition and learning,situation templates and situation,theory of situation-,through workflows},
number = {39},
pages = {972--981},
title = {{Understanding and Designing Situation-Aware Mobile and Ubiquitous Computing Systems}},
year = {2010}
}
@incollection{Sacks1978,
author = {Sacks, Harvey and Schegloff, Emanuel A. and Jefferson, Gail},
booktitle = {Studies in the Organization of Conversational Interaction},
doi = {10.1016/B978-0-12-623550-0.50008-2},
editor = {Schenkein, Jim},
isbn = {0160487749},
number = {3},
pages = {7--55},
publisher = {Elsevier},
title = {{A Simplest Systematics for the Organization of Turn Taking for Conversation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780126235500500082 https://linkinghub.elsevier.com/retrieve/pii/B9780126235500500082},
volume = {84},
year = {1978}
}
@article{Auer,
abstract = {Der Beitrag gibt einen {\"{U}}berblick {\"{u}}ber die Forschungsergebnisse der Gespr{\"{a}}chsanalyse und benachbarter Forschungstraditionen zu der Frage, wie fokussierte Interaktionen (besonders: " Gespr{\"{a}}che ") begonnen und aufgel{\"{o}}st werden.},
author = {Auer, Peter},
journal = {InLiSt - Interaction and Linguistic Structures},
keywords = {focused interaction,interaction,interaction end,interaction start},
number = {59},
title = {{Anfang und Ende Fokussierter Interaktion: Eine Einf{\"{u}}hrung}},
url = {http://www.inlist.uni-bayreuth.de/issues/59/index.htm},
year = {2017}
}
@misc{Grest2004,
abstract = {We present a system for robust realtime person tracking that integrates face detection, face color tracking and foot tracking in a uniform way by using a particle filter. The system is embedded in a complete immersive environment (3- sided CAVE with I-sided stereo back projection). The person controls the visual environment by walking around inside.},
author = {Grest, Daniel and Koch, Reinhard},
isbn = {0780385780},
keywords = {person tracking},
pages = {387--390},
title = {{Realtime Multi{\textperiodcentered}Camera Person Tracking for Immersive Environments}},
year = {2004}
}
@inproceedings{Holthaus2016a,
abstract = {In order to explore intuitive verbal and non-verbal interfaces in smart environments we recorded user interactions with an intelligent apartment. Besides offering various interactive capabilities itself, the apartment is also inhabited by a social robot that is available as a humanoid interface. This paper presents a multi-modal corpus that contains goal-directed actions of naive users in attempts to solve a number of predefined tasks. Alongside audio and video recordings, our data-set consists of large amount of temporally aligned sensory data and system behavior provided by the environment and its interactive components. Non-verbal system responses such as changes in light or display contents, as well as robot and apartment utterances and gestures serve as a rich basis for later in-depth analysis. Manual annotations provide further information about meta data like the current course of study and user behavior including the incorporated modality, all literal utterances, language features, emotional expressions, foci of attention, and addressees.},
address = {Portoro{\v{z}}, Slovenia},
author = {Holthaus, Patrick and Leichsenring, Christian and Bernotat, Jasmin and Richter, Viktor and Pohling, Marian and Carlmeyer, Birte and K{\"{o}}ster, Norman and {Meyer zu Borgsen}, Sebastian and Zorn, Rene and Schiffhauer, Birte and Engelmann, Kai Frederic and Lier, Florian and Schulz, Simon and Cimiano, Philipp and Eyssel, Friederike and Kummert, Franz and Herrmann, Thomas and Schlangen, David and R{\"{u}}ckert, Ulrich and Wachsmuth, Sven and Wrede, Britta and Wrede, Sebastian},
booktitle = {International Conference on Language Resources and Evaluation (LREC)},
keywords = {interaction corpus,smart home,social robot},
pages = {3440--3446},
publisher = {ELRA},
title = {{How to Address Smart Homes with a Social Robot? A Multi-modal Corpus of User Interactions with an Intelligent Environment}},
url = {http://www.lrec-conf.org/proceedings/lrec2016/summaries/1046.html},
year = {2016}
}
@inproceedings{Laskowski2010,
abstract = {Substantial research effort has been in- vested in recent decades into the com- putational study and automatic process- ing of multi-party conversation. While most aspects of conversational speech have benefited from a wide availabil- ity of analytic, computationally tractable techniques, only qualitative assessments are available for characterizing multi-party turn-taking. The current paper attempts to address this deficiency by first proposing a framework for computing turn-taking model perplexity, and then by evaluat- ing several multi-participant modeling ap- proaches. Experiments show that direct multi-participant models do not general- ize to held out data, and likely never will, for practical reasons. In contrast, the Extended-Degree-of-Overlap model rep- resents a suitable candidate for future work in this area, and is shown to success- fully predict the distribution of speech in time and across participants in previously unseen conversations.},
author = {Laskowski, Kornel},
booktitle = {Annual Meeting of the Association for Computational Linguistics},
keywords = {multiparty,turn-taking},
number = {July},
pages = {999--1008},
title = {{Modeling Norms of Turn-Taking in Multi-Party Conversation}},
year = {2010}
}
@article{Pejsa2015,
abstract = {To facilitate natural interactions between humans and embodied conversational agents (ECAs), we need to endow the latter with the same nonverbal cues that humans use to communicate. Gaze cues in particular are integral in mechanisms for communication and management of attention in social interactions, which can trigger important social and cognitive processes, such as establishment of affiliation between people or learning new information. The fundamental building blocks of gaze behaviors are gaze shifts: coordinated movements of the eyes, head, and body toward objects and information in the environment. In this article, we present a novel computational model for gaze shift synthesis for ECAs that supports parametric control over coordinated eye, head, and upper body movements. We employed the model in three studies with human participants. In the first study, we validated the model by showing that participants are able to interpret the agent�s gaze direction accurately. In the second and third studies, we showed that by adjusting the participa- tion of the head and upper body in gaze shifts, we can control the strength of the attention signals conveyed, thereby strengthening or weakening their social and cognitive effects. The second study shows that manip- ulation of eye�head coordination in gaze enables an agent to convey more information or establish stronger affiliation with participants in a teaching task, while the third study demonstrates how manipulation of upper body coordination enables the agent to communicate increased interest in objects in the environment.},
author = {Pejsa, Tomislav and Andrist, Sean and Gleicher, Michael and Mutlu, Bilge},
doi = {10.1145/2724731},
issn = {21606455},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {Affiliation,Attention,Body orientation,Embodied conversational agents,Gaze model,Learning},
month = {3},
number = {1},
pages = {1--34},
title = {{Gaze and Attention Management for Embodied Conversational Agents}},
url = {http://dl.acm.org/citation.cfm?doid=2744352.2724731},
volume = {5},
year = {2015}
}
@book{Turabian,
author = {Turabian, Kate L.},
edition = {7th Editio},
isbn = {9780226823386},
publisher = {The University of Chicago Press},
title = {{A Manual for Writers of Research Papers, Theses, and Dissertations}},
url = {http://jcs.edu.au/wp-content/uploads/2016/09/A-manual-for-writers-of-research-papers-theses-and-dissertations.pdf}
}
@phdthesis{Sheikhi2014,
author = {Sheikhi, Samira},
keywords = {addressee,humavips,vfoa},
pages = {132},
school = {{\'{E}}cole Polytechnique F{\'{e}}d{\'{e}}rale de Lausanne (EPFL)},
title = {{Inferring Visual Attention and Addressee in Human Robot Interaction}},
year = {2014}
}
@book{Hairer1996,
author = {Hairer, E and N{\o}rsett, S P and Wanner, G},
isbn = {9783540604525},
publisher = {Springer},
series = {Lecture Notes in Economic and Mathematical Systems},
title = {{Solving Ordinary Differential Equations II: Stiff and Differential-Algebraic Problems}},
url = {http://books.google.de/books?id=m7c8nNLPwaIC},
year = {1996}
}
@book{Odersky,
author = {Odersky, Martin and Spoon, Lex and Venners, Bill},
edition = {PrePrint},
isbn = {9780981531601},
pages = {1--547},
title = {{Scala artima}},
year = {2007}
}
@inproceedings{Wang2012,
abstract = {Effective street peddlers monitor passersby, where they tune their message to capture and keep the passerby's attention over the entire duration of the sales pitch. Similarly, advertising displays in today's public environments can be more effective if they were able to tune their content in response to how passersby were attending them vs. just showing fixed content in a loop. Previously, others have prototyped displays that monitor and react to the presence or absence of a person within a few proxemic (spatial) zones surrounding the screen, where these zones are used as an estimate of attention. However, the coarseness and discrete nature of these zones mean that they cannot respond to subtle changes in the user's attention towards the display. In this paper, we contribute an extension to existing proxemic models. Our Peddler Framework captures (1) fine-grained continuous proxemic measures by (2) monitoring the passerby's distance and orientation with respect to the display at all times. We use this information to infer (3) the passerby's interest or digression of attention at any given time, and (4) their attentional state with respect to their short-term interaction history over time. Depending on this attentional state, we tune content to lead the passerby into a more attentive stage, ultimately resulting in a purchase. We also contribute a prototype of a public advertising display -- called Proxemic Peddler -- that demonstrates these extensions as applied to content from the Amazon.com website.},
address = {New York, New York, USA},
author = {Wang, Miaosen and Boring, Sebastian and Greenberg, Saul},
booktitle = {International Symposium on Pervasive Displays (PerDis)},
doi = {10.1145/2307798.2307801},
isbn = {9781450314145},
keywords = {advertising,attention,pervasive displays,proxemic interactions},
number = {3},
pages = {1--6},
publisher = {ACM Press},
title = {{Proxemic Peddlerr: A Public Advertising Display that Captures and Preserves the Attention of a Passerby}},
url = {http://dl.acm.org/citation.cfm?id=2307801 http://dl.acm.org/citation.cfm?doid=2307798.2307801},
year = {2012}
}
@article{LeRoux2008,
abstract = {Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.},
author = {{Le Roux}, Nicolas and Bengio, Yoshua},
doi = {10.1162/neco.2008.04-07-510},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,Computer Simulation,Computer-Assisted,Humans,Learning,Learning: physiology,Models,Neural Networks (Computer),Signal Processing,Statistical,learning},
month = {6},
number = {6},
pages = {1631--49},
title = {{Representational power of restricted boltzmann machines and deep belief networks.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18254699},
volume = {20},
year = {2008}
}
@article{Kristoffersson2013,
abstract = {This article presents a method for measuring the quality of interaction in social mobile robotic telepresence. The methodology is in part based on Adam Kendon's theory of F-formations. The theory is based on observations of how bodies naturally orient themselves during interaction between people in real life settings. In addition, two presence questionnaires (Temple Presence Inventory and Networked Minds Social Presence Inventory), designed to measure the users' perceptions of others and the environment when experienced through a communication medium are used. The perceived presence and ease of use are correlated to the spatial formations between the robot and an actor. The proposed methodology is validated experimentally on a dataset consisting of interactions between an elder (actor) and 21 different users being trained in piloting a mobile robotic telepresence unit. The evaluation has shown that these tools are suitable for evaluating mobile robotic telepresence and also that correlations between the tools used exist. Further, these results give important guidelines on how to improve the interface in order to increase the quality of interaction.},
author = {Kristoffersson, Annica and {Severinson Eklundh}, Kerstin and Loutfi, Amy},
doi = {10.1007/s12369-012-0166-7},
isbn = {9789176689424},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {F-formations,Methodology,Mobile robotic telepresence,Presence,Quality of interaction,Telepresence},
month = {1},
number = {1},
pages = {89--101},
title = {{Measuring the Quality of Interaction in Mobile Robotic Telepresence: A Pilot's Perspective}},
url = {http://link.springer.com/10.1007/s12369-012-0166-7},
volume = {5},
year = {2013}
}
@article{Minotto2014,
abstract = {Humans can extract speech signals that they need to understand from a mixture of background noise, interfering sound sources, and reverberation for effective communication. Voice Activity Detection (VAD) and Sound Source Localization (SSL) are the key signal processing components that humans perform by processing sound signals received at both ears, sometimes with the help of visual cues by locating and observing the lip movements of the speaker. Both VAD and SSL serve as the crucial design elements for building applications involving human speech. For example, systems with microphone arrays can benefit from these for robust speech capture in video conferencing applications, or for speaker identification and speech recognition in Human Computer Interfaces (HCIs). The design and implementation of robust VAD and SSL algorithms in practical acoustic environments are still challenging problems, particularly when multiple simultaneous speakers exist in the same audiovisual scene. In this work we propose a multimodal approach that uses Support Vector Machines (SVMs) and Hidden Markov Models (HMMs) for assessing the video and audio modalities through an RGB camera and a microphone array. By analyzing the individual speakers' spatio-temporal activities and mouth movements, we propose a mid-fusion approach to perform both VAD and SSL for multiple active and inactive speakers. We tested the proposed algorithm in scenarios with up to three simultaneous speakers, showing an average VAD accuracy of 95.06{\%} with an average error of 10.9 cm when estimating the three-dimensional locations of the speakers.},
author = {Minotto, Vicente P. and Jung, Claudio R. and Lee, Bowon},
doi = {10.1109/TMM.2014.2305632},
isbn = {1520-9210 VO - 16},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Beamforming,SRP-PHAT,audio-visual vad,hidden Markov model,lip movements,microphone array,multimodal fusion,optical-flow,sound source localization,support vector machine,voice activity detection},
month = {6},
number = {4},
pages = {1032--1044},
title = {{Simultaneous-Speaker Voice Activity Detection and Localization Using Mid-Fusion of SVM and HMMs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6737222},
volume = {16},
year = {2014}
}
@inproceedings{Park2013,
abstract = {Predictive Situation Awareness (PSAW) is the ability to estimate and predict aspects of a temporally evolving situation. PSAW systems reason about complex and uncertain situations involving multiple targets observed by multiple sensors at different times. Multi-Entity Bayesian Networks (MEBN) are rich enough to represent and reason about uncertainty in complex, knowledge-rich domains, and have been applied to representation and reasoning for PSAW. To overcome a labor- intensive and insufficiently agile process for manual MEBN modeling by a domain expert, MEBN machine learning was developed. Although technologies for machine learning have improved dramatically, the necessary capabilities to build a MEBN model efficiently do not yet exist. The search space for components of an MEBN model is too large and complex to investigate all possible structures, variables, and parameters. For this reason, this paper proposes a method which relies partially on expert knowledge and insight to reduce the search space. The proposed method, a process for Human-aided MEBN learning in PSAW, is a framework to develop a MEBN model from the domain expert's knowledge combined with relational data. This paper presents the process for Human-aided MEBN learning in PSAW and a case study to evaluate the process on development of a defense system in PSAW.},
address = {Heidelberg},
author = {Park, Cheol Yound and Laskey, Kathryn Blackmond and Costa, Paulo and Matsumoto, Shou},
booktitle = {International Conference on Information Fusion},
isbn = {9780996452748},
keywords = {Algorithms,Bayes Theorem,Command and Control Systems,Data Fusion,Interactions,Learning Machines,Logic,Naval Operations,Network Centric Warfare,Ontology,Random Variables,Reasoning,Relational Data Bases,Situational Awareness,Symposia,Uncertainty,mebn,prediction,psaw,situation recognition},
number = {19},
title = {{Multi-Entity Bayesian Networks Learning In Predictive Situation Awareness}},
url = {http://www.dodccrp.org/events/18th{\_}iccrts{\_}2013/post{\_}conference/papers/043.pdf{\%}5Cnhttp://www.stormingmedia.us/08/0847/A084785.html},
year = {2016}
}
@inproceedings{Zhang2010,
abstract = {With the rapid development of computing and communication technology and the rising ratio of the aged, the research on elderly care by using smart home has widely aroused attention. Smart home is designated to provide adaptive and personalized service for the users in home environment. This paper aims to realize the personalized services in smart home environment for the elderly based on situation analysis technology. A new class of context, situation is defined, and its representation and modeling are presented. We then describe the context match method for situation recognition based on different relevant contexts. A personalized service is built on the basis of the result of situation analysis.},
annote = {"The situation is the set of contexts in the application over a period of time that affects future system behavior [7]."
OSGi [10] architecture

* physical layer
* context aquisition layer
** collection {\&} reognition of lower level subject information
** transforms information in uniform format
* situation recognition layer
** creates situation information from context
* personalized service layer
** analyzes situation information 
** provides corresponding personalized service},
author = {Zhang, Xing and Wang, Haipeng and Yu, Zhiwen},
booktitle = {2010 7th International Conference on Ubiquitous Intelligence {\&} Computing and 7th International Conference on Autonomic {\&} Trusted Computing},
doi = {10.1109/UIC-ATC.2010.65},
isbn = {978-1-4244-9043-1},
keywords = {context modeling,personalized services,situation analysis,situation recognition,smart home},
month = {10},
pages = {7--12},
publisher = {IEEE},
title = {{Toward a Smart Home Environment for Elder People Based on Situation Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5667099},
year = {2010}
}
@inproceedings{Dahlbom2009,
annote = {"the problem of situation recognition, by which we mean the task of tracking states and identifying situations that are of importance for decision makers"

"Barwise and Perry, who proposed that situations are the fundamental building blocks when assessing the world"

"According to Lambert6, situations are essentially collections of spatiotemporal facts that are related to each other, where facts consist of relations between objects that have properties."

"In the light of surveillance, physical processes are approximated by discrete processes in computers, and it is important to choose a suitable level of abstraction, depending on the purpose."

"continuous processes can be estimated by discrete processes which can be described in terms of states and events. An event describes a change in state and a series of these states are called
a history.16"

Def. State: A state s is a set of facts describing some discrete process P during an interval of time. Def. Event: An event e is a change in state s for some process P at a specific point in time.
Def. State sequence: A state sequence C is a vector of states C = {\textless}s{\_}1,...,s{\_}w{\textgreater} describing the evolution of some discrete process P.
Def. Situation: A situation S in a state sequence C of some process P is a vector of sets of facts S = {\textless} s'{\_}1,...,s'{\_}u{\textgreater}, where the following constraints are true: (1) each set of facts s'{\_}i in S is a subset of a state s{\_}k, which is part of the state sequence C: s'{\_}i ? s{\_}k, (2) for each pair of sets of facts s'{\_}i,s'{\_}j in a situation S, where s'{\_}i ? s{\_}k and s'{\_}j ? s{\_}l and where s{\_}k,s{\_}l is part of the state sequence C, i != j and necessarily i {\textless} j if k {\textless} l.
Def. Situation recognition: An approximate solution to a template-based situation recognition problem consists of a ranked list of situations S in a state sequence C for which the degree of similarity, d, between a template t and the situation S is determined to be larger than some cutoff value e, through some similarity function d = f{\_}a(t,S) elem [0,1]

Template based situation recognition
1. based on a priori knowledge
2. uses formalisms easily interpretable by humans
3. do not need to explicitly model everithing in the situation
4. do not need a perfect match},
author = {Dahlbom, Anders and Niklasson, Lars and Falkman, G{\"{o}}ran and Loutfi, Amy},
booktitle = {Intelligent Sensing, Situation Management, Impact Assessment, and Cyber-Sensing},
doi = {10.1117/12.818715},
editor = {Mott, Stephen and Buford, John F. and Jakobson, Gabe and Mendenhall, Michael J.},
keywords = {logic,situation analysis,situation assessment,situation awareness,situation recognition,surveillance,template based,template-based,time},
month = {5},
pages = {735205--735205--12},
title = {{Towards Template-based Situation Recognition}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1339070},
volume = {7352},
year = {2009}
}
@book{goffman1963,
address = {New York, NY, USA},
author = {Goffman, Erving},
edition = {First Free},
isbn = {0029119405},
keywords = {focused interaction,human-human,interaction,unfocused interaction},
pages = {248},
publisher = {Free Press},
title = {{Behavior in Public Places}},
url = {https://books.google.de/books?id=EM1NNzcR-V0C},
year = {1963}
}
@article{Riva2005,
abstract = {The chapter outlines a psychological framework for the concept of Ambient Intelligence (AmI), centered on three concepts: action, situation and presence. Using this framework it is provided a psychological definition of AmI, based on the experience of the user: AmI is the effective and transparent support to the activity of the subject/s through the use of information and communication technologies. The definition points at “activity support” as the core of the AmI experience. Further, it identifies “effectiveness” - the activity reaches its objective - and “transparency” - the activity is experienced without breakdowns - as the key characteristics of an effective AmI system. Using these features, the chapter highlights some general guidelines to support the work of AmI developers. Further, it identifies possible AmI applications related to the support and tracking of the subject's activity: AmI may offer a guidance/support to the activity of the subject and/or provide a feedback related to its status.},
author = {Riva, Giuseppe and Vatalaro, F. and Alcaniz, M.},
pages = {17--33},
title = {{The Psychology of Ambient Intelligence : Activity , Situation and Presence}},
year = {2005}
}
@book{Cook2005,
address = {Hoboken, NJ, USA},
author = {Cook, Diane and Das, Sajal},
doi = {10.1002/047168659X},
editor = {Cook, Diane J. and Das, Sajal K.},
isbn = {9780471686590},
month = {9},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Smart Environments - Technology, Protocols, and Applications}},
url = {http://doi.wiley.com/10.1002/047168659X},
year = {2004}
}
@incollection{Traum2003,
abstract = {This article examines some of the issues in representation of, processing, and automated agent participation in natural language dialogue, considering expansion from two-party dialogue to multi-party dialogue. These issues include Some regarding the roles agents play in dialogue, interactive factors, and content management factors.},
author = {Traum, David},
booktitle = {Workshop on Agent Communication Languages (ACL)},
doi = {10.1007/978-3-540-24608-4_12},
editor = {Dignum, Frank},
isbn = {978-3-540-20769-6},
keywords = {dialog,multiparty},
pages = {201--211},
title = {{Issues in Multiparty Dialogues}},
url = {http://link.springer.com/10.1007/978-3-540-24608-4{\_}12},
year = {2004}
}
@article{Mitra2004,
abstract = {We propose a framework for pairwise registration of shapes represented by point cloud data (PCD). We assume that the points are sampled from a surface and formulate the problem of aligning two PCDs as a minimization of the squared distance between the underlying surfaces. Local quadratic approximants of the squared distance function are used to develop a linear system whose solution gives the best aligning rigid transform for the given pair of point clouds. The rigid transform is applied and the linear system corresponding to the new orientation is build. This process is iterated until it converges. The point-to-point and the point-to-plane Iterated Closest Point (ICP) algorithms can be treated as special cases in this framework. Our algorithm can align PCDs even when they are placed far apart, and is experimentally found to be more stable than point-to-plane ICP. We analyze the convergence behavior of our algorithm and of point-to-point and point-to-plane ICP under our proposed framework, and derive bounds on their rate of convergence. We compare the stability and convergence properties of our algorithm with other registration algorithms on a variety of scanned data.},
address = {New York, New York, USA},
author = {Mitra, Niloy J. and Gelfand, Natasha and Pottmann, Helmut and Guibas, Leonidas},
doi = {10.1145/1057432.1057435},
isbn = {3905673134},
issn = {17278384},
journal = {Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing - SGP '04},
pages = {22--31},
publisher = {ACM Press},
title = {{Registration of point cloud data from a geometric optimization perspective}},
url = {http://graphics.stanford.edu/projects/lgl/papers/mgpg-rpcdgop-04/mgpg-rpcdgop-04.pdf},
year = {2004}
}
@inproceedings{Bruce2002,
abstract = {This paper presents the results of an experiment in human-robot social interaction. Its purpose was to measure the impact of certain features and behaviors on people's willingness to engage in a short interaction with a robot. The behaviors tested were the ability to convey expression with a humanoid face and the ability to indicate attention by turning towards the person that the robot is addressing. We hypothesized that these features were minimal requirements for effective social interaction between a human and a robot. We will discuss the results of the experiment and their implications for the design of socially interactive robots.},
annote = {Test: Bildschirm auf pan-tilt-unit (open field).

* mit animiertem Gesicht vs ohne (expressiveness)
* mit Bildschirm zu Personen drehen vs ohne (attention)

={\textgreater} 

* beides erh{\"{o}}gt Wahrscheinlichkeit, dass Personen stoppen},
author = {Bruce, Allison and Nourbakhsh, Illah and Simmons, Reid},
booktitle = {International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ROBOT.2002.1014396},
isbn = {0-7803-7272-7},
keywords = {attention,human-robot interaction},
pages = {4138--4142},
publisher = {IEEE},
title = {{The Role of Expressiveness and Attention in Human-Robot Interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1014396},
volume = {4},
year = {2002}
}
@article{Wan2014,
author = {Wan, Jie and O'Grady, Michael J. and O'Hare, Gregory M. P.},
doi = {10.1007/s00779-014-0824-x},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {activity recognition,activity recognition {\'{a}} sensor,event segmentation,segmentation {\'{a}},smart home},
month = {2},
number = {2},
pages = {287--301},
title = {{Dynamic sensor event segmentation for real-time activity recognition in a smart home context}},
url = {http://link.springer.com/10.1007/s00779-014-0824-x},
volume = {19},
year = {2015}
}
@phdthesis{Wienke2018a,
author = {Wienke, Johannes},
doi = {10.4119/unibi/2932136},
school = {Bielefeld University},
title = {{Framework-Level Resource Awareness In Robotics and Intelligent Systems}},
url = {urn:nbn:de:0070-pub-29321366},
year = {2018}
}
@article{Hegel2008,
abstract = {Theory of Mind (ToM) is not only a key capability for cognitive development but also for successful social interaction. In order for a robot to interact successfully with a human both interaction part- ners need to have an adequate representation of the other's actions. In this paper we address the question of how a robot's actions are perceived and represented in a human subject interacting with the robot and how this perception is influenced by the appearance of the robot. We present the preliminary results of an fMRI-study in which participants had to play a version of the classical Prisoners' Dilemma Game (PDG) against four opponents: a human partner (HP), an anthropomorphic robot (AR), a functional robot (FR), and a computer (CP). The PDG scenario enables to implicitly measure mentalizing or Theory of Mind (ToM) abilities, a technique com- monly applied in functional imaging. As the responses of each game partner were randomized unknowingly to the participants, the attribution of intention or will to an opponent (i.e. HP, AR, FR or CP) was based purely on differences in the perception of shape and embodiment. The present study is the first to apply functional neuroimaging methods to study human-robot interaction on a higher cognitive level such as ToM. We hypothesize that the degree of anthropomor- phism and embodiment of the game partner will modulate cortical activity in previously detected ToM networks as the medial pre- frontal lobe and anterior cingulate cortex. Categories},
author = {Hegel, Frank and Krach, S{\"{o}}ren and Kircher, Tilo},
doi = {10.1145/1349822.1349866},
isbn = {9781605580173},
issn = {2167-2121},
journal = {Human-Robot Interaction (HRI)},
pages = {335--342},
title = {{Theory of Mind (ToM) on robots: a functional neuroimaging study}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6249454},
year = {2008}
}
@article{Bazzani2013,
abstract = {In human behaviour analysis, the visual focus of attention (VFOA) of a person is a very important cue. VFOA detection is difficult, though, especially in a unconstrained and crowded environment, typical of video surveillance scenarios. In this paper, we estimate the VFOA by defining the Subjective View Frustum, which approximates the visual field of a person in a three-dimensional representation of the scene. This opens up to several intriguing behavioural investigations. In particular, we propose the Inter-Relation Pattern Matrix, which suggests possible social interactions between the people present in a scene. Theoretical justifications and experimental results substantiate the validity and the goodness of the analysis performed. [PUBLICATION ABSTRACT]},
author = {Loris, Bazzani and Marco, Cristani and Diego, Tosato and Michela, Farenzena and Giulia, Paggetti and Gloria, Menegaz and Vittorio, Murino},
doi = {10.1111/j.1468-0394.2012.00622.x},
issn = {02664720},
journal = {Expert Systems},
keywords = {head pose estimation,social interactions,social signal processing,tracking,visual focus of attention},
month = {5},
number = {2},
pages = {115--127},
title = {{Social Interactions by Visual Focus of Attention in a Three-Dimensional Environment}},
url = {http://doi.wiley.com/10.1111/j.1468-0394.2012.00622.x},
volume = {30},
year = {2013}
}
@inproceedings{Wodarczak2012,
abstract = {[We report on the functional and timing relations between head movements and the overlapping verbal-vocal feedback expressions. We investigate the effect of a distraction task on head gesture behaviour and the co-occurring verbal feedback. The results show that head movements overlapping with verbal expressions in a distraction task differ in terms of several features from a default, non-perturbed conversational situations, e.g.: frequency and type of movement and verbal to nonverbal display ratios.]},
author = {W{\l}odarczak, Marcin and Buschmeier, Hendrik and Malisz, Zofia and Kopp, Stefan and Wagner, Petra},
booktitle = {Interdisciplinary Workshop on Feedback Behaviors in Dialog, INTERSPEECH 2012 Satellite Workshop},
keywords = {BC,attentiveness,communicative feedback,dialogue,distraction task,head gestures,nods},
number = {August 2012},
pages = {93--96},
title = {{Listener head gestures and verbal feedback expressions in a distraction task}},
url = {http://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=2{\&}cad=rja{\&}ved=0CD8QFjAB{\&}url=http://pub.uni-bielefeld.de/download/2510942/2525693{\&}ei=hg6nUf0igsmEB5vLgfgO{\&}usg=AFQjCNEcCaNrrxg92wysoKDLdsOckJab4g{\&}sig2=uqBh-dlgPr965l19ggYvZw},
year = {2012}
}
@inproceedings{Richter2016,
address = {New York, New York, USA},
author = {Richter, Viktor and Kummert, Franz},
booktitle = {Workshop on Embodied Interaction with Smart Environments (EISE)},
doi = {10.1145/3008028.3008030},
isbn = {9781450345552},
keywords = {addressing,multi-modal,natural interaction,smart environments,social robot},
pages = {1--6},
publisher = {ACM Press},
title = {{Towards Addressee Recognition in Smart Robotic Environments}},
url = {http://doi.acm.org/10.1145/3008028.3008030 http://dl.acm.org/citation.cfm?doid=3008028.3008030},
year = {2016}
}
@article{Schegloff1968,
abstract = {An attempt is mude to ascertain rules for the sequencing of a limited part of natural con- versation and to determine some properties and empirical consequences of the operation of thoJe rules. Two formulations of conversational openings are suggested and the prop- erties “tionterr,iinality” and “conditional relevance” are developed to explicate the opera- tion of one of them and to suggest some of its interactional consequences. Some discussion is offered of the fit between the sequencing structure and the tasks of conversational openings.},
author = {Schegloff, Emanuel A.},
doi = {10.1525/aa.1968.70.6.02a00030},
issn = {0002-7294},
journal = {American Anthropologist},
month = {12},
number = {6},
pages = {1075--1095},
title = {{Sequencing in Conversational Openings}},
url = {http://doi.wiley.com/10.1525/aa.1968.70.6.02a00030},
volume = {70},
year = {1968}
}
@inproceedings{Makino2018,
author = {Makino, Takashi and Takegawa, Yoshinari and Hirata, Keiji},
booktitle = {International Conference on Intelligent Automation and Robotics (ICIAR)},
isbn = {978-988-14049-0-9},
title = {{Predicting Turn Taking from Gaze Transition Patterns Considering Participation Status in Multi-Party Conversation}},
volume = {II},
year = {2018}
}
@inproceedings{Munch2011a,
abstract = {In surveillance applications human operators are either con- fronted with a high cognitive load or monotonic time periods where the operator's attention rapidly decreases. Therefore, automatic high-level interpretation of image sequences gains increasing importance in assist- ing human operators.We present a generic hierarchical system that gen- erates high-level logic-based situation descriptions in various domains. The system consists of two components. First, a vision component pro- vides 3D spatial and temporal information about objects in scenes. Sec- ond, the situation recognition component uses knowledge encoded in Situation Graph Trees and a fuzzy graph traversal allowing exhaustive situation awareness. The system is tested with real video data compris- ing persons, their actions, and interactions. In order to show the domain independence we used recorded data from moving vehicles and static surveillance cameras. The results show that the system is usable with multi modal data and can easily be modified and extended.},
annote = {Recent surveys dealing with the interpretation of situations in image sequences are [17, 27].

In general, the likelihood between the learned situation and the image sequence is determined by the models' probability of labeled situations (see [1, 5, 22]).

Situation Graph Trees (SGT) to allow automatic high-level inference on the conceptual layer of situations, see [21]},
author = {M{\"{u}}nch, David and J{\"{u}}ngling, Kai and Arens, Michael},
booktitle = {International Workshop on Behaviour Analysis and Video Understanding (ICVS 2011)},
keywords = {activity recognition,fuzzy logic,fuzzy metric temporal,ism tracking,logic,mean shift,person tracking,sift,situation awareness,situation graph tree,situation recognition,surveillance,tracking,vision},
number = {Icvs},
title = {{Towards a Multi-purpose Monocular Vision-based High-Level Situation Awareness System}},
year = {2011}
}
@article{Shu2005,
abstract = {Team situation awareness (TSA) is a critical contributing factor in establishing collaborative rela- tions among team members involved in cooperative activity. Currently, however, there is still a lack of a clearly understandable and commonly agreeable model of TSA. To resolve misunderstanding or conflict among team members or between a team and machines, our research aim is to find out the underlying mechanism of TSA that reflects team cognitive process in a way con- sistent with team cooperative activity, and to focus on how to achieve mutual understanding, and how to effectively incorporate human teams into a socio-tech- nological system. In this paper, we argue that earlier models of TSA, where TSA was discussed as the inter- section of situation awareness (SA) owned by individual team members, are inadequate for study of a sophisti- cated team reciprocal process. We suggest that it is necessary for the definition of TSA to integrate the no- tion of individual SA (ISA) into cooperative team activity. In particular, understanding of mutual aware- ness is an essential element in cooperative activity. We propose a new notion of TSA, which is reducible to mutual beliefs as well as ISA at three levels. Further, we develop an operational TSA inference method and dis- cuss human competence and system-related factors that are required to build TSA. We then try to demonstrate how TSA is actively constructed via inferencing prac- tices. We also develop criteria to assess appropriateness of TSA from two aspects: soundness and completeness of mutual beliefs. Comparison of evaluation results indicates that the notion of TSA proposed in this work is more suitable to depict team cooperative activity than conventional},
author = {Shu, Yufei and Furuta, Kazuo},
doi = {10.1007/s10111-005-0012-x},
issn = {1435-5558},
journal = {Cognition, Technology {\&} Work},
keywords = {awareness {\ae} cooperative activity,mutual,situation awareness,team situation awareness {\ae},team-machine interaction,{\ae} tsa inference {\ae}},
month = {11},
number = {4},
pages = {272--287},
title = {{An inference method of team situation awareness based on mutual awareness}},
url = {http://link.springer.com/10.1007/s10111-005-0012-x},
volume = {7},
year = {2005}
}
@article{Sokol,
abstract = {Predicting human behaviour from smart-house data is an active and challenging area of research. Applications include improving quality of life and personal healthcare, e.g. smart thermostats and detecting falls. Smart-house tech- nology is becoming increasingly popular and in many cases making a house smarter is as simple as installing off-the-shelf devices. In this paper we focus on predicting the future location of a person in a simulated smart-house envi- ronment. We consider three different occupant types who exhibit various work- ing patterns. We develop a versatile model capable of adjusting to significant changes in a person's behaviour without the need of model retraining. To this end, we build a simple event calculus framework based on the Aleph Inductive Logic Programming system. Event calculus helps to handle time and persisting sensor states. Background knowledge encodes important information about the smart-house that is otherwise difficult to learn; it also facilitates transferability of the model to different house layouts. Moreover, rule models are white-box, hence human-readable. Finally, we show that a versatile model performs significantly better than other models that do not explicitly account for the context. Keywords:},
author = {Sokol, Kacper and Flach, Peter},
keywords = {aleph,event calculus,location prediction,smart-house,versatile model},
pages = {66--72},
title = {{Activity recognition in multiple contexts for smart-house data}}
}
@article{gco4,
author = {Delong, Andrew and Osokin, Anton and Isack, Hossam N. and Boykov, Yuri},
doi = {10.1007/s11263-011-0437-z},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {1},
number = {1},
pages = {1--27},
title = {{Fast Approximate Energy Minimization with Label Costs}},
url = {http://link.springer.com/10.1007/s11263-011-0437-z},
volume = {96},
year = {2012}
}
@article{Hasan2015,
abstract = {Most of the research on human activity recognition has focused on learning a static model considering that all the training instances are labeled and present in advance, while in streaming videos new instances continuously arrive and are not labeled. Moreover, these methods generally use application specific hand-engineered and static feature models which are not suitable for continuous learning. Some recent approaches on activity recognition use deep learning based hierarchical feature models, but the large size of these networks constrain them from being used in continuous learning scenarios. In this work, we propose a continuous activity learning framework for streaming videos by intricately tying together deep hybrid feature models and active learning. This allows us to automatically select the most suitable features and to take the advantage of incoming unlabeled instances to improve the existing model incrementally. Given the segmented activities from streaming videos, we learn features in an unsupervised manner using deep hybrid networks, which have the ability to take the advantage of both the local hand-engineered features and the deep model in an efficient way. Additionally, we use active learning to train the activity classifier using a reduced amount of manually labeled instances. Retraining the models with huge amount of accumulated exam- ples is computationally expensive and not suitable for continuous learning. Hence, we propose a method to select the best subset of these examples to update the models incrementally. We conduct rigorous experiments on four challenging human activity datasets to demonstrate the effectiveness of our framework.},
author = {Hasan, Mahmudul and Roy-Chowdhury, Amit K.},
doi = {10.1109/TMM.2015.2477242},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {activity recognition,continuous,deep learning},
month = {11},
number = {11},
pages = {1909--1922},
title = {{A Continuous Learning Framework for Activity Recognition Using Deep Hybrid Feature Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7244231},
volume = {17},
year = {2015}
}
@article{Bicocchi,
abstract = {Urban environments are increasingly pervaded by ICT de- vices. Soon, citizens and technologies could collaboratively constitute large-scale socio-technical organisms supporting both individual and col- lective awareness. This paper illustrates a modern awareness framework designed to deal with the complexity of this scenario. The framework is able to collect and classify data streams in a modular way by supporting service oriented, reconfigurable components. Furthermore, we evaluate an innovative meta-classifcation scheme based on state-automata for (i) improving energy efficiency, (ii) improving classification accuracy and (iii) improving software engineering of aware systems, without affecting the overall performance.},
author = {Bicocchi, Nicola and Fontana, Damiano and Zambonelli, Franco},
keywords = {awareness framework,meta-classication,self-reconfiguration},
title = {{A Self-Reconfigurable Framework for Context Awareness}},
year = {2015}
}
@article{Raux2012,
abstract = {Even as progress in speech technologies and task and dialog modeling has allowed the development of advanced spoken dialog systems, the low-level interaction behavior of those systems often remains rigid and inefficient. Based on an analysis of human-human and human-computer turn-taking in naturally occurring task-oriented dialogs, we define a set of features that can be automatically extracted and show that they can be used to inform efficient end-of-turn detection. We then frame turn-taking as decision making under uncertainty and describe the Finite-State Turn-Taking Machine (FSTTM), a decision-theoretic model that combines data-driven machine learning methods and a cost structure derived from Conversation Analysis to control the turn-taking behavior of dialog systems. Evaluation results on CMU Let's Go, a publicly deployed bus information system, confirm that the FSTTM significantly improves the responsiveness of the system compared to a standard threshold-based approach, as well as previous data-driven methods.},
author = {Raux, Antoine and Eskenazi, Maxine},
doi = {10.1145/2168748.2168749},
issn = {15504875},
journal = {ACM Transactions on Speech and Language Processing},
keywords = {dialog,turn-taking},
month = {5},
number = {1},
pages = {1--23},
title = {{Optimizing the turn-taking behavior of task-oriented spoken dialog systems}},
url = {http://dl.acm.org/citation.cfm?doid=2168748.2168749},
volume = {9},
year = {2012}
}
@article{Zywiczynski,
author = {{\.{Z}}ywiczy{\'{n}}ski, Przemys{\l}aw and Wacewicz, S{\l}awomir and Orzechowski, Sylwester},
doi = {10.1075/is.18.2.07zyw},
keywords = {activities,adaptors,conversation,displacement,interaction engine,language evolution,object manipulators,self touches,turn-taking},
number = {September 2017},
pages = {1--32},
title = {{Adaptors and the Turn-Taking Mechanism: The Distribution of Adaptors Relative to Turn Borders in Dyadic Conversation}}
}
@article{Ferrari1992,
abstract = {In this paper we will address the problem of plan- ning optimal grasps. Two general optimality criteria, that consider the total finger force and the maximum finger force will be introduced and discussed. More- over their formalization, using various metrics on a space of generalized forces, will be detailed. The ge- ometric interpretation of the two criteria will lead to an efficient planning algorithm. An example of its use in a robotic environment equipped with two-jaw and three-jaw grippers will also be shown.},
author = {Ferrari, C. and Canny, J.},
doi = {10.1109/ROBOT.1992.219918},
isbn = {0-8186-2720-4},
journal = {Proceedings 1992 IEEE International Conference on Robotics and Automation},
pages = {2290--2295},
publisher = {IEEE Comput. Soc. Press},
title = {{Planning optimal grasps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=219918},
year = {1992}
}
@article{Cooley1965,
author = {Cooley, By James W and Tukey, John W},
number = {90},
pages = {297--301},
title = {{An Algorithm for the Machine Calculation of Complex Fourier Series}},
volume = {19},
year = {1965}
}
@inproceedings{Gaschler2012,
abstract = {Robots that interact with humans in everyday situations, need to be able to interpret the nonverbal social cues of their human interaction partners. We show that humans use body posture and head pose as social signals to initiate and terminate interaction when ordering drinks at a bar. For that, we record and analyze 108 interactions of humans interacting with a human bartender. Based on these findings, we train a Hidden Markov Model (HMM) using automatic body posture and head pose estimation.With this model, the bartender robot of the project JAMES can recognize typical social behaviors of human customers. Evaluation shows a recognition rate of 82.9 {\%} for all implemented social behaviors and in particular a recognition rate of 91.2 {\%} for bartender attention requests, which will allow the robot to interact with multiple humans in a robust and socially appropriate way.},
author = {Gaschler, Andre and Giuliani, Manuel and Huth, Kerstin and Ruiter, Jan De and Knoll, Alois},
booktitle = {Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ},
isbn = {9781467317351},
keywords = {dialog,hmm,interaction,pose,social cues},
pages = {2128--2133},
title = {{Social Behavior Recognition Using Body Posture and Head Pose for Human-Robot Interaction}},
year = {2012}
}
@phdthesis{Arrival1997,
abstract = {Taken at face value, the world is complicated and confusing. When operating in such complexity, we are greatly advantaged by our ability to infer the underlying structure of the world – that is, the relationships between our observations and the underlying latent causes that generate them. At any given time, inferring the latent causes that are currently active – i.e., the current situation – allows us to execute the most appropriate actions and cognitive processes. In theories of episodic memory, this definition of the current situation is related to the cognitive constructs of “schemas” and “context”. In reinforcement learning and decision-making, representations of the current situation are called the “state”. In this work, I begin to uncover the computations and neural mechanisms that underlie our inference of the causal structure of the world, including inferences of the current situation, and also how the inferred situation affects decision-making and memory. Throughout this work, the overlapping brain areas of ventromedial prefrontal cortex (vmPFC) and orbitofrontal cortex (OFC) play a prominent role in the neural circuits that perform this inference. In the first experiment, I show that overall levels of activity in the OFC are related to learning about one type of causal structure – transitions between states of the world. In the second experiment, I present evidence that the OFC represents a belief distribution (a posterior probability distribution) over the underlying situation. In the third experiment, I present evidence that, in accordance with current theories of episodic memory and temporal context, memories seem to be organized according to information in the brain about the semantics of recent experience, which may serve as a heuristic proxy for the current situation.},
author = {Chan, Stephanie Caroline Yenne},
keywords = {neuronal,situation,theory,thesis},
school = {Princeton University},
title = {{What's going on? Inference and neural representations of the current situation and the underlying causal structure of the world}},
url = {http://arks.princeton.edu/ark:/88435/dsp011j92g988h},
year = {2016}
}
@article{Lockelt2008,
author = {L{\"{o}}ckelt, Markus},
pages = {285},
title = {{A Flexible and Reusable Framework for Dialogue and Action Management in Multi-Party Discourse}},
year = {2008}
}
@article{Huttenrauch2009,
abstract = {Special purpose service robots have already entered the market and their users' homes. Also the idea of the general purpose service robot or personal robot companion is increasingly discussed and investigated. To probe human–robot interaction with a mobile robot in arbitrary domestic settings, we conducted a study in eight different homes. Based on previous results from laboratory studies we identified particular interaction situations which should be studied thoroughly in real home settings. Based upon the collected sensory data from the robot we found that the different environments influenced the spatial management observable during our subjects' interaction with the robot. We also validated empirically that the concept of spatial prompting can aid spatial management and communication, and assume this concept to be helpful for Human–Robot Interaction (HRI) design. In this article we report on our exploratory field study and our findings regarding, in particular, the spatial management observed during show episodes and movement through narrow passages . Keywords: COGNIRON, Domestic Service Robotics, Robot Field Trial, Human Augmented Mapping (HAM), Human–Robot Interaction (HRI), Spatial Management, Spatial Prompting},
author = {H{\"{u}}ttenrauch, Helge and Topp, Elin A. and Severinson-Eklundh, Kerstin},
doi = {10.1075/is.10.3.02hut},
issn = {1572-0373},
journal = {Interaction Studies},
month = {12},
number = {3},
pages = {274--297},
title = {{The Art of Gate-Crashing: Bringing HRI Into Users' Homes}},
url = {http://ci.nii.ac.jp/ncid/BB00491071 https://benjamins.com/catalog/is.10.3.02hut},
volume = {10},
year = {2009}
}
@inproceedings{Johansson2015,
abstract = {In this paper we present a data-driven model for detecting opportunities and obligations for a robot to take turns in multi-party discus- sions about objects. The data used for the model was collected in a public setting, where the robot head Furhat played a collabo- rative card sorting game together with two users. The model makes a combined detec- tion of addressee and turn-yielding cues, us- ing multi-modal data from voice activity, syntax, prosody, head pose, movement of cards, and dialogue context. The best result for a binary decision is achieved when sever- al modalities are combined, giving a weighted F1 score of 0.876 on data from a previously unseen interaction, using only au- tomatically extractable features. 1},
author = {Johansson, Martin and Skantze, Gabriel},
booktitle = {Annual Meeting of the Special Interest Group on Discourse and Dialogue},
keywords = {addressee,turn-taking},
number = {September},
pages = {305--314},
title = {{Opportunities and Obligations to Take Turns in Collaborative Multi-Party Human-Robot Interaction}},
year = {2015}
}
@inproceedings{Burghart2005,
abstract = {Future life pictures humans having intelligent hu- manoid robotic systems taking part in their everyday life. Thus researchers strive to supply robots with an adequate artificial intelligence in order to achieve a natural and intuitive inter- action between human being and robotic system. Within the German Humanoid Project we focus on learning and cooperating multimodal robotic systems. In this paper we present a first cognitive architecture for our humanoid robot: The architecture is a mixture of a hierarchical three-layered form on the one hand and a composition of behaviour-specific modules on the other hand. Perception, learning, planning of actions, motor control, and human-like communication play an important role in the robotic system and are embedded step by step in our architecture. I.},
author = {Burghart, C. and Mikut, R. and Stiefelhagen, R. and Asfour, T. and Holzapfel, H. and Steinhaus, P. and Dillmann, R.},
booktitle = {International Conference on Humanoid Robots, 2005.},
doi = {10.1109/ICHR.2005.1573593},
isbn = {0-7803-9320-1},
keywords = {planning},
pages = {357--362},
publisher = {IEEE},
title = {{A cognitive architecture for a humanoid robot: a first approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1573593},
year = {2005}
}
@article{Skantze2014b,
abstract = {In this paper, we present a study where a robot instructs a human on how to draw a route on a map. The human and robot are seated face-to-face with the map placed on the table between them. The user's and the robot's gaze can thus serve several simultaneous functions: as cues to joint attention, turn-taking, level of understanding and task progression. We have compared this face-to-face setting with a setting where the robot employs a random gaze behaviour, as well as a voice-only setting where the robot is hidden behind a paper board. In addition to this, we have also manipulated turn-taking cues such as completeness and filled pauses in the robot's speech. By analysing the participants' subjective rating, task completion, verbal responses, gaze behaviour, and drawing activity, we show that the users indeed benefit from the robot's gaze when talking about landmarks, and that the robot's verbal and gaze behaviour has a strong effect on the users' turn-taking behaviour. We also present an analysis of the users' gaze and lexical and prosodic realisation of feedback after the robot instructions, and show that these cues reveal whether the user has yet executed the previous instruction, as well as the user's level of uncertainty. ?? 2014 Elsevier B.V. All rights reserved.},
author = {Skantze, Gabriel and Hjalmarsson, Anna and Oertel, Catharine},
doi = {10.1016/j.specom.2014.05.005},
isbn = {0167-6393, 0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Feedback,Gaze,Joint attention,Prosody,Turn-taking,Uncertainty,one on one interaction,turn-taking},
month = {11},
pages = {50--66},
publisher = {Elsevier B.V.},
title = {{Turn-taking, feedback and joint attention in situated human–robot interaction}},
url = {http://dx.doi.org/10.1016/j.specom.2014.05.005 http://linkinghub.elsevier.com/retrieve/pii/S016763931400051X},
volume = {65},
year = {2014}
}
@inproceedings{Chahuara2012,
annote = {* representation of concepts through ontologies and a set of logical rules
* situatoin recognition through SWRL (description logic reasoner)
* contextual information (location, activity,period{\_}of{\_}day) plays a major role to deliver appropriate support
* Situation: set of contraints where each constraint contraints the value of a source information. ex: Door = {\{}open{\}},location={\{}study,bedroom{\}}
* temporal situation: recognized when two instances of situations occur whithin a defined interval
* Coontext: complementary information to evaluate circumstance in certain quality Q in {\{}risk,comfort,safety,...{\}}
* MLN: structure {\&} weight learning},
author = {Chahuara, Pedro and Portet, Fran{\{}$\backslash$c{\{}c{\}}{\}}ois and Vacher, Michel},
booktitle = {The 4th International Workshop on Acquisition, Representation and Reasoning with Contextualized Knowledge (ARCOE-12)},
keywords = {decision making,influence diagrams,logic,mln,probabilistic,reasoning,situation,smart home},
pages = {52--64},
title = {{Context aware decision system in a smart home: knowledge representation and decision making using uncertain contextual information}},
year = {2012}
}
@article{Deng,
abstract = {Gaussian filtering has been intensively studied in image processing and computer vision. Using Gaussian filter for noise suppression, the noise is smoothed out, at the same time the signal is also distorted. The use of Gaussian filter as preprocessing for edge detection will also give rise to edge position displacement, edges vanishing, and phantom edges. In this paper, we first review various techniques for these problems. We then propose an adaptive Gaussian filtering algorithm in which the filter variance is adapted to both the noise characteristics and the local variance of the signal.},
author = {Deng, G. and Cahill, L.W.},
doi = {10.1109/NSSMIC.1993.373563},
isbn = {0-7803-1487-5},
journal = {Conference Record Nuclear Science Symposium and Medical Imaging Conference},
pages = {1615--1619},
publisher = {IEEE},
title = {{An adaptive Gaussian filter for noise reduction and edge detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=373563},
year = {1993}
}
@article{Heldner2010,
abstract = {This paper explores durational aspects of pauses, gaps and overlaps in three different conversational corpora with a view to challenge claims about precision timing in turn-taking. Distributions of pause, gap and overlap durations in conversations are presented, and methodological issues regarding the statistical treatment of such distributions are discussed. The results are related to published minimal response times for spoken utterances and thresholds for detection of acoustic silences in speech. It is shown that turn-taking is generally less precise than is often claimed by researchers in the field of conversation analysis or interactional linguistics. These results are discussed in the light of their implications for models of timing in turn-taking, and for interaction control models in speech technology. In particular, it is argued that the proportion of speaker changes that could potentially be triggered by information immediately preceding the speaker change is large enough for reactive interaction controls models to be viable in speech technology. {\textcopyright} 2010 Elsevier Ltd.},
archivePrefix = {arXiv},
author = {Heldner, Mattias and Edlund, Jens},
doi = {10.1016/j.wocn.2010.08.002},
eprint = {arXiv:1011.1669v3},
issn = {00954470},
journal = {Journal of Phonetics},
month = {10},
number = {4},
pages = {555--568},
title = {{Pauses, Gaps and Overlaps in Conversations}},
url = {http://dx.doi.org/10.1016/j.wocn.2010.08.002 https://linkinghub.elsevier.com/retrieve/pii/S0095447010000628},
volume = {38},
year = {2010}
}
@article{Castelli2000,
abstract = {We report a functional neuroimaging study with positron emission tomography (PET) in which six healthy adult volunteers were scanned while watching silent computer-presented animations. The characters in the animations were simple geometrical shapes whose movement patterns selectively evoked mental state attribution or simple action description. Results showed increased activation in association with mental state attribution in four main regions: medial prefrontal cortex, temporoparietal junction (superior temporal sulcus), basal temporal regions (fusiform gyrus and temporal poles adjacent to the amygdala), and extrastriate cortex (occipital gyrus). Previous imaging studies have implicated these regions in self-monitoring, in the perception of biological motion, and in the attribution of mental states using verbal stimuli or visual depictions of the human form. We suggest that these regions form a network for processing information about intentions, and speculate that the ability to make inferences about other people's mental states evolved from the ability to make inferences about other creatures' actions. {\textcopyright} 2000 Academic Press.},
author = {Castelli, Fulvia and Happ{\'{e}}, Francesca and Frith, Uta and Frith, Chris},
doi = {10.1006/nimg.2000.0612},
isbn = {9780203496190},
issn = {10538119},
journal = {NeuroImage},
keywords = {Autism,Biological motion,Brain imaging,Mentalizing,Theory of mind},
month = {9},
number = {3},
pages = {314--325},
publisher = {Psychology Press},
title = {{Movement and Mind: A Functional Imaging Study of Perception and Interpretation of Complex Intentional Movement Patterns}},
url = {https://www.taylorfrancis.com/books/9780203496190 http://linkinghub.elsevier.com/retrieve/pii/S1053811900906128},
volume = {12},
year = {2000}
}
@article{Solera2016,
abstract = {Modern crowd theories agree that collective behavior is the result of the underlying interactions among small groups of individuals. In this work, we propose a novel algorithm for detecting social groups in crowds by means of a Correlation Clustering procedure on people trajectories. The affinity between crowd members is learned through an online formulation of the Structural SVM framework and a set of specifically designed features characterizing both their physical and social identity, inspired by Proxemic theory, Granger causality, DTW and Heat-maps. To adhere to sociological observations, we introduce a loss function (G-MITRE) able to deal with the complexity of evaluating group detection performances. We show our algorithm achieves state-of-the-art results when relying on both ground truth trajectories and tracklets previously extracted by available detector/tracker systems.},
author = {Solera, Francesco and Calderara, Simone and Cucchiara, Rita},
doi = {10.1109/TPAMI.2015.2470658},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Correlation Clustering,Crowd analysis,Granger causality,Proxemic theory,Structural SVM,group detection},
month = {5},
number = {5},
pages = {995--1008},
title = {{Socially Constrained Structural Learning for Groups Detection in Crowd}},
url = {http://ieeexplore.ieee.org/document/7214317/},
volume = {38},
year = {2016}
}
@inproceedings{Sanchez-Riera2012,
abstract = {In this paper we address the problem of audio-visual speaker detection. We introduce an online system working on the humanoid robot NAO. The scene is perceived with two cameras and two microphones. A multimodal Gaussian mixture model (mGMM) fuses the information extracted from the auditory and visual sensors and detects the most probable audio-visual object, e.g., a person emitting a sound, in the 3D space. The system is implemented on top of a platform-independent middleware and it is able to process the information online (17Hz). A detailed description of the system and its implementation are provided, with special emphasis on the on-line processing issues and the proposed solutions. Experimental validation, performed with five different scenarios, show that that the proposed method opens the door to robust human-robot interaction scenarios.},
annote = {Speaker detection by correlating sound source localization with faced fund.},
author = {Sanchez-Riera, Jordi and Alameda-Pineda, Xavier and Wienke, Johannes and Deleforge, Antoine and Arias, Soraya and Cech, Jan and Wrede, Sebastian and Horaud, Radu},
booktitle = {2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012)},
doi = {10.1109/HUMANOIDS.2012.6651509},
isbn = {978-1-4673-1369-8},
issn = {21640572},
keywords = {humavips,nao,speaker detection},
month = {11},
pages = {126--133},
publisher = {IEEE},
title = {{Online multimodal speaker detection for humanoid robots}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6651509},
year = {2012}
}
@article{Zhao2016,
author = {Zhao, Yu and Zhang, Xihui and Crabtree, John},
journal = {Issues in Information Systems},
keywords = {human-computer interaction,smart home,technology,user experience},
number = {3},
pages = {11--19},
title = {{Human-Computer Interaction and User Experience in Smart Home Research: A Critical Analysis}},
volume = {17},
year = {2016}
}
@article{Chollet2017,
author = {Chollet, Mathieu and Ochs, Magalie and Pelachaud, Catherine},
doi = {10.1109/TAFFC.2017.2753777},
issn = {1949-3045},
journal = {IEEE Transactions on Affective Computing},
keywords = {generation,interaction,nonverbal signals,social attitude},
number = {c},
pages = {1--1},
title = {{A Methodology for the Automatic Extraction and Generation of Non-Verbal Signals Sequences Conveying Interpersonal Attitudes}},
url = {http://ieeexplore.ieee.org/document/8039497/},
volume = {XX},
year = {2017}
}
@article{Gilani,
abstract = {We present dialogue management routines for a system to engage in multiparty agent-infant interaction. The ultimate purpose of this research is to help infants learn a visual sign language by engaging them in naturalistic and socially contingent conversations during an early-life critical period for language development (ages 6 to 12 months) as initiated by an artificial agent. As a first step, we focus on creating and maintaining agent-infant engagement that elicits appropriate and socially contingent responses from the baby. Our system includes two agents, a physical robot and an animated virtual human. The system's multimodal perception includes an eye-tracker (measures attention) and a thermal infrared imaging camera (measures patterns of emotional arousal). A dialogue policy is presented that selects individual actions and planned multiparty sequences based on perceptual inputs about the baby's internal changing states of emotional engagement. The present version of the system was evaluated in interaction with 8 babies. All babies demonstrated spontaneous and sustained engagement with the agents for several minutes, with patterns of conversationally relevant and socially contingent behaviors. We further performed a detailed case-study analysis with annotation of all agent and baby behaviors. Results show that the baby's behaviors were generally relevant to agent conversations and contained direct evidence for socially contingent responses by the baby to specific linguistic samples produced by the avatar. This work demonstrates the potential for language learning from agents in very young babies and has especially broad implications regarding the use of artificial agents with babies who have minimal language exposure in early life.},
archivePrefix = {arXiv},
author = {Gilani, Setareh Nasihati and Traum, David and Merla, Arcangelo and Hee, Eugenia and Walker, Zoey and Manini, Barbara and Gallagher, Grady and Petitto, Laura-Ann},
eprint = {1809.01581},
journal = {Human-Computer Interaction},
month = {9},
title = {{Multimodal Dialogue Management for Multiparty Interaction with Infants}},
url = {http://arxiv.org/abs/1809.01581},
year = {2018}
}
@techreport{Stroustrup2015,
abstract = {You can write C++ programs that are statically type safe and have no resource leaks. You can do that without loss of performance and without limiting C++'s expressive power. This model for type- and resource-safe C++ has been implemented using a combination of ISO standard C++ language facilities, static analysis, and a tiny support library (written in ISO standard C++). This supports the general thesis that garbage collection is neither necessary nor sufficient for quality software. This paper describes the techniques used to eliminate dangling pointers and to ensure resource safety. Other aspects – also necessary for safe and effective use of C++ – have conventional solutions so they are mentioned only briefly here. The techniques and facilities presented are supported by the Core C++ Guidelines [Stroustrup,2015] and enforced by a static analysis tool for those [Sutter,2015].},
author = {Stroustrup, Bjarne and Sutter, Herb and {Dos Reis}, Gabriel},
keywords = {c++,programming},
number = {October},
pages = {1--19},
title = {{A brief introduction to C++'s model for type- and resource-safety}},
url = {https://github.com/isocpp/CppCoreGuidelines/blob/master/docs/Introduction to type and resource safety.pdf},
year = {2015}
}
@inproceedings{Aldoma2013,
abstract = {At the core of every object recognition system lies the development and integration of distinct feature descriptors to create object representations robust against varying perspec- tives or lightning conditions. Recent work has primarily focused on the development of distinct point features. While these features achieve impressive recognition results, point features fail to capture the shape and appearance of an object with less or even without texture. This paper proposes a novel method for the rapid and dense computation of 2D and 3D image cues from RGB-D data to target the recognition of objects without rich texture and a global histogram-based descriptor for the distinct description of object models.},
author = {Aldoma, A and Tombari, F and Prankl, J and Richtsfeld, A and Stefano, L Di and Vincze, M},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
isbn = {9781467356428},
pages = {2096--2103},
title = {{Multimodal Cue Integration through Hypotheses Verification for RGB-D Object Recognition and 6DOF Pose Estimation}},
year = {2013}
}
@article{Cardell-Oliver2010,
abstract = {A situation is an abstraction for a pattern of observations made by a distributed system such as a sensor network. Situations have previously been studied in different domains, as composite events in distributed event based systems, service composition in multi-agent systems, and macro- programming in sensor networks. However, existing languages do not address the specific challenges posed by sensor networks. This article presents a novel language for representing situa- tions in sensor networks that addresses these challenges. Three algorithms for recognizing sit- uations in relevant fields are reviewed and adapt- ed to sensor networks. In particular, distributed commitment machines are introduced and demonstrated to be the most suitable algorithm among the three for recognizing situations in sensor networks.},
author = {Cardell-Oliver, Rachel and Liu, Wei},
doi = {10.1109/MCOM.2010.5434382},
issn = {0163-6804},
journal = {IEEE Communications Magazine},
month = {3},
number = {3},
pages = {112--117},
title = {{Representation and recognition of situations in sensor networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5434382},
volume = {48},
year = {2010}
}
@article{Bose1990,
abstract = {This correspondence investigates the effect of the shape and size of a standard feature (fiducial) on the positional accuracy of the centroid in machine vision applications. It is shown that a circular fidu- cial provides superior subpixel registration accuracy among the fiducial shapes typically used today.},
author = {Bose, C.B. and Amir, I.},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {12},
pages = {1196--1200},
title = {{Design of Fiducials for Accurate Registration Using Machine Vision}},
volume = {12},
year = {1990}
}
@inproceedings{Meyer2017,
address = {Nagoya},
author = {Wachsmuth, Sven and Lier, Florian and {Meyer zu Borgsen}, Sebastian and Kummert, Johannes and Lach, Luca and Sixt, Dominik},
booktitle = {RoboCup 2017},
title = {{ToBI - Team of Bielefeld The Human-Robot Interaction System for RoboCup@Home 2017}},
url = {https://pub.uni-bielefeld.de/record/2910592},
year = {2017}
}
@inproceedings{Wright2002,
abstract = {Reasoning about military situations requires a scientifically sound and computationally robust uncertainty calculus, a supporting inference engine that procedurally encodes the axioms of the calculus, the capability to fuse information at multiple levels of abstraction, and the ability to respond to dynamic situations. The inference engine also needs to be able to encapsulate expert knowledge, including deep human doctrinal and domain knowledge. At Information Extraction {\&} Transport, Inc. (IET), we have developed techniques to encode domain and doctrinal expertise in reusable knowledge chunks, based on the technology of Bayesian network fragments, and the capability to automatically construct situation specific Bayesian Networks based on a combination of top down control and bottom up evidence-driven processes. These techniques have been used to prototype fusion systems capable of reasoning about uncertain numbers of uncertain hierarchically organized entities based on incomplete observations. These systems have demonstrated success in generating force level situation hypotheses from vehicle tracks and other evidence generated by level 1 fusion systems. This paper presents an overview of our technical approach with applications from recent projects.},
annote = {"template model is appropriate for problem domains in which the relevant variables, their state spaces, and their probabilistic relationships do not vary from problem instance to problem instance"

"Even when a domain can be represented by a template model, its size and complexity may make it necessary to represent it implicitly as a collection of modular subunits [3]"

Network Fragments
* represent shared elements of a probabilistic knowledge base
* small set of random variables
* may represent only portion of cpd for variable given its prents
* Entyty structure/behaviour/relationships encoded as fragments
-{\textgreater} Knowledge base of Fragments provides building blocks for assembling situation specific BN

MEBN extends BN
* allow replication and combination
-{\textgreater} reason with variable numbers of entities
* implicit joint probability over object-level domain entities
* ontology for higher order probability
* object language and meta-language

Hierarchical Type Model
* most specific evident type
* can be refined using observations,situation,interest

Hierarchical Activities
* individual activity related to platoon activity related to campany activity

Clusters of observations trigger Suggestor
-{\textgreater} SSN situation specific network is constructed
-{\textgreater} inference about target hypothesis
-{\textgreater} decision nodes are evaluated

Suggestors
* Construction suggestors add hypothesis to model
* Revision suggestors change model
* reason about 
** entity esistence
** relationships among entities
** entity subtypes
** variable resolution
** dependency relation (modification,adaptation,replacement)},
author = {Wright, E. and Mahoney, S. and Laskey, K. and Takikawa, M. and Levitt, T.},
booktitle = {Proceedings of the 5th International Conference on Information Fusion, FUSION 2002},
doi = {10.1109/ICIF.2002.1020889},
isbn = {0-9721844-1-4},
keywords = {Bayesian Network Fragments,Bayesian Networks,bayesian networks,fusion,hypothesis management,military,multi-entity Bayesian Networks,situation assessment},
pages = {804--811},
title = {{Multi-entity Bayesian networks for situation assessment}},
volume = {2},
year = {2002}
}
@book{TheEconomist2005,
address = {London},
author = {{The Economist}},
edition = {9},
isbn = {978 1 86197 916 2},
publisher = {Profile Books},
title = {{Style Guide}},
url = {http://www.loc.gov/catdir/enhancements/fy0716/2006491681-d.html},
year = {2005}
}
@article{Epley2007,
abstract = {Anthropomorphism describes the tendency to imbue the real or imagined behavior of nonhuman agents with humanlike characteristics, motivations, intentions, or emotions. Although surprisingly common, anthropomorphism is not invariant. This article describes a theory to explain when people are likely to anthropomorphize and when they are not, focused on three psychological determinants—the accessibility and applicability of anthropocentric knowledge (elicited agent knowledge), the motivation to explain and understand the behavior of other agents (effectance motivation), and the desire for social contact and affiliation (sociality motivation). This theory predicts that people are more likely to anthropomorphize when anthropocentric knowledge is accessible and applicable, when motivated to be effective social agents, and when lacking a sense of social connection to other humans. These factors help to explain why anthropomorphism is so variable; organize diverse research; and offer testable predictions about dispo- sitional, situational, developmental, and cultural influences on anthropomorphism. Discussion addresses extensions of this theory into the specific psychological processes underlying anthropomorphism, applications of this theory into robotics and human–computer interaction, and the insights offered by this theory into the inverse process of dehumanization.},
archivePrefix = {arXiv},
author = {Epley, Nicholas and Waytz, Adam and Cacioppo, John T.},
doi = {10.1037/0033-295X.114.4.864},
eprint = {epley2007},
isbn = {0033-295X (Print)$\backslash$r0033-295X (Linking)},
issn = {1939-1471},
journal = {Psychological Review},
keywords = {agency,animal cognition,anthropomorphism,mind perception,social cognition},
number = {4},
pages = {864--886},
title = {{On seeing human: A three-factor theory of anthropomorphism.}},
url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.114.4.864},
volume = {114},
year = {2007}
}
@inproceedings{Repiso2018,
abstract = {This paper presents a new model to make robots capable of approaching and engaging people with a human-like behavior, while they are walking in a side-by-side formation with a person. This method extends our previous work [1], which allows the robot to adapt its navigation behaviour according to the person being accompanied and the dynamic environment. In the current work, the robot is able to predict the best encounter point between the human-robot group and the approached person. Then, in the encounter point the robot modifies its position to achieve an engagement with both people. The encounter point is computed using a gradient descent method that takes into account all people predictions. Moreover, we make use of the Extended Social Force Model (ESFM), and it is modified to include the dynamic goal. The method has been validated over several situations and in real-life experiments, in addition, a user study has been realized to reveal the social acceptability of the robot in this task.},
author = {Repiso, Ely and Garrell, Anais and Sanfeliu, Alberto},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2018.8594149},
isbn = {978-1-5386-8094-0},
month = {10},
pages = {8200--8205},
publisher = {IEEE/RSJ},
title = {{Robot Approaching and Engaging People in a Human-Robot Companion Framework}},
url = {https://ieeexplore.ieee.org/document/8594149/},
year = {2018}
}
@article{Islam2016,
abstract = {The Internet of Things (IoT) is a network of recognizable physical objects (or things) [1], [2]. It includes anyone, anything, any service, and any network connected to the Internet used to retrieve information at any time and from anywhere. The IoT is smart enough to offer us solutions for a wide range of applications, including intelligent transportation systems, personal communications, consumer electronics, health care, organizational services, securi- ties and monitoring, and industrial controls.},
author = {Islam, S.M. Riazul and Uddin, M Nazim and Kwak, Kyung Sup},
doi = {10.1109/MCE.2016.2516079},
issn = {2162-2248},
journal = {IEEE Consumer Electronics Magazine},
keywords = {iot,situation recognition},
month = {4},
number = {2},
pages = {49--57},
title = {{The IoT: Exciting Possibilities for Bettering Lives: Special application scenarios.}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7450744},
volume = {5},
year = {2016}
}
@article{Homke2017,
abstract = {Does blinking function as a type of feedback in conversation? To address this question, we built a corpus of Dutch conversations, identified short and long addressee blinks during extended turns, and measured their occur- rence relative to the end of turn constructional units (TCUs), the location where feedback typically occurs. Addressee blinks were indeed timed to the end of TCUs. Also, long blinks were more likely than short blinks to occur during mutual gaze, with nods or continuers, and their occurrence was restricted to sequential contexts in which signaling understanding was particularly relevant, suggesting a special signaling capacity of long blinks.},
annote = {Das ist super speziel. 

Es geht vor allem um die kommunikativen Eigenschaften von kurzem und langem zwinkern.},
author = {H{\"{o}}mke, Paul and Holler, Judith and Levinson, Stephen C.},
doi = {10.1080/08351813.2017.1262143},
issn = {08351813},
journal = {Research on Language and Social Interaction},
keywords = {addressee,communication,multi-modal,turn-taking},
number = {1},
pages = {54--70},
publisher = {Routledge},
title = {{Eye Blinking as Addressee Feedback in Face-To-Face Conversation}},
url = {http://dx.doi.org/10.1080/08351813.2017.1262143 https://doi.org/10.1080/08351813.2017.1262143},
volume = {50},
year = {2017}
}
@article{Koskela2004,
abstract = {Smart home environments have evolved to the point where everyday objects and devices at home can be networked to give the inhabitants new means to control them. Familiar information appliances can be used as user interfaces (UIs) to home functions to achieve a more convenient user experience. This paper reports an ethnographic study of smart home usability and living experience. The purpose of the research was to evaluate three UIs—a PC, a media terminal, and a mobile phone—for smart home environments. The results show two main types of activity patterns, pattern control and instant control, which require different UI solutions. The results suggest that a PC can act as a central unit to control functions for activity patterns that can be planned and determined in advance. The mobile phone, on the other hand, is well suited for instant control. The mobile phone turned out to be the primary and most frequently used UI during the 6-month trial period in the smart apartment.},
author = {Koskela, Tiiu and V{\"{a}}{\"{a}}n{\"{a}}nen-Vainio-mattila, Kaisa},
doi = {10.1007/s00779-004-0283-x},
isbn = {1617-4909},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Ethnographic study,Information appliances,Smart home,Usability,User interface},
number = {3-4},
pages = {234--240},
title = {{Evolution towards smart home environments: Empirical evaluation of three user interfaces}},
volume = {8},
year = {2004}
}
@article{Airenti2015,
abstract = {Humans may react very differently with respect to mechanical devices, including robots. They can interact with them with delight or retreat in aversion or fear. According to the famous model of the uncanny valley these opposite reactions depend on the degree of familiarity that different artifacts engender in humans. The aim of my work is trying to find out the cognitive bases of familiarity, analyzing the origin of anthropomorphic projection, namely human disposition to attribute anthropomorphic features - like intentions or feelings—to artifacts. I shall discuss two concepts: relatedness and empathy, and argue that relatedness is the precondition for empathy. The fact that it is possible to attribute anthropomorphic features virtually to any object shows that resemblance is not the point. Anthropomorphism is a kind of relation that humans establish with an artifact, and in order to comprehend this phenomenon we have to focus on the relational aspect. I shall argue that what we call anthropomorphism is an extension to nonhumans of forms of interactions typical of human communication, i.e. the attribution to an artifact of the position of interlocutor in a possible dialogue. It can be shown that attributing to an artifact the position of interlocutor in a dialogue implies dealing with it as if it were endowed of the features characterizing human mind, i.e. mental states and emotions. {\textcopyright} 2015, Springer Science+Business Media Dordrecht.},
author = {Airenti, Gabriella},
doi = {10.1007/s12369-014-0263-x},
isbn = {1875-4791,18754791},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Anthropomorphism,Communication,Empathy,Relatedness,Theory of mind},
month = {2},
number = {1},
pages = {117--127},
title = {{The Cognitive Bases of Anthropomorphism: From Relatedness to Empathy}},
url = {http://link.springer.com/10.1007/s12369-014-0263-x},
volume = {7},
year = {2015}
}
@article{Chang2008,
abstract = {In this paper, a new image-denoising filter that is based on the standard median (SM) filter is proposed. In our method, a threshold and the standard median is used to detect noise and change the original pixel value to a newer that is closer to or the same as the standard median. We also incorporate the center weighted median (CWM) filter in our method. With our experimental results, we have made a comparison among our method, the standard median (SM) filter, the center weighted median (CWM) filter, and the Tri-State Median (TSM) filter, in which our method proves to be superior},
author = {Chang, Chin-Chen and Hsiao, Ju-Yuan and Hsieh, Chih-Ping},
doi = {10.1109/IITA.2008.259},
isbn = {978-0-7695-3497-8},
journal = {2008 Second International Symposium on Intelligent Information Technology Application},
month = {12},
pages = {346--350},
publisher = {Ieee},
title = {{An Adaptive Median Filter for Image Denoising}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4739784},
year = {2008}
}
@inproceedings{Uckermann2012,
abstract = {We present a real-time algorithm that segments unstructured and highly cluttered scenes. The algorithm robustly separates objects of unknown shape in congested scenes of stacked and partially occluded objects. The model-free approach finds smooth surface patches, using a depth image from a Kinect camera, which are subsequently combined to form highly probable object hypotheses. The real-time capabilities and the quality of the algorithm are evaluated on a benchmark database. Advantages compared to existing approaches as well as weaknesses are discussed. We also report on an autonomous grasping experiment with the Shadow Robot Hand which employs the estimated shape and pose of objects given by our algorithm in a task in which it cleans a table},
author = {{\"{U}}ckermann, Andre and Haschke, Robert and Ritter, Helge},
booktitle = {Humanoids},
keywords = {3D,Computer Vision,Robot Grasping,Segmentation},
title = {{Real-Time 3D Segmentation of Cluttered Scenes for Robot Grasping}},
url = {http://pub.uni-bielefeld.de/luur/download?func=downloadFile{\&}recordOId=2530701{\&}fileOId=2530720},
year = {2012}
}
@article{Jakobson2010,
author = {Jakobson, Gabriel and Buford, John and Lewis, Lundy},
doi = {10.1109/MCOM.2010.5434381},
issn = {0163-6804},
journal = {IEEE Communications Magazine},
month = {3},
number = {3},
pages = {110--111},
title = {{Situation management}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5434381},
volume = {48},
year = {2010}
}
@article{Damerow2016,
abstract = {Most current approaches to scene understanding lack the capability to adapt object and situation models to behavioral needs not anticipated by the human system designer. Here, we give a detailed description of a system architecture for self-referential autonomous learning which enables the refinement of object and situation models during operation in order to optimize behavior. This includes structural learning of hierarchical models for sit- uations and behaviors that is triggered by a mismatch between expected and actual action outcome. Besides proposing architectural concepts, we also describe a first implementation of our system within a simulated traffic scenario to demonstrate the feasibility of our approach.},
author = {Damerow, Florian and Knoblauch, Andreas and K{\"{o}}rner, Ursula and Eggert, Julian and K{\"{o}}rner, Edgar},
doi = {10.1007/s12559-016-9407-7},
issn = {1866-9956},
journal = {Cognitive Computation},
keywords = {autonomous learning {\'{a}} hierarchical,behavioral,model,self-referential control {\'{a}} scene,situation,situation model,understanding {\'{a}}},
month = {4},
title = {{Toward Self-Referential Autonomous Learning of Object and Situation Models}},
url = {http://link.springer.com/10.1007/s12559-016-9407-7},
year = {2016}
}
@article{Guo2010,
abstract = {This paper presents an application of SIFT (Scale Invariant Feature Transform) in 2.5D facial feature extraction. As range images have more rich in geometric features than that in 2D Images, we intend to improve the SIFT algorithm to extract facial features in 2.5D images. According to face topol- ogy and differential geometric properties of surfaces, the ex- tracted keypoints from 2.5D range images are divided into 9 different surface types for further match. The significance of work presented here is that 2.5D SIFT algorithm has more rotation invariance and robust in facial feature extraction.},
author = {Guo, He and Zhang, Kai and Jia, Qi},
doi = {10.1109/IIHMSP.2010.183},
isbn = {978-1-4244-8378-5},
journal = {2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing},
keywords = {2,5d,facial feature,shape index,sift},
month = {10},
pages = {723--726},
publisher = {Ieee},
title = {{2.5D SIFT Descriptor for Facial Feature Extraction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5636142},
year = {2010}
}
@article{Park2007,
abstract = {This paper introduces a new robotic smart house, Intelligent Sweet Home , developed at KAIST in Korea, which is based on several robotic agents and aims at testing advanced concepts for independent living of the elderly and people with disabilities. The work focuses on technical solutions for human-friendly assistance in motion/mobility and advanced human-machine interfaces that provide simple control of all assistive robotic systems and home-installed appliances. The smart house concept includes an intelligent bed, intelligent wheelchair, and robotic hoist for effortless transfer of the user between bed and wheelchair. The design solutions comply with most of the users' requirements and suggestions collected by a special questionnaire survey of people with disabilities. The smart house responds to the user's commands as well as to the recognized intentions of the user. Various interfaces, based on hand gestures, voice, body movement, and posture, have been studied and tested. The paper describes the overall system structure and explains the design and functionality of some main system components.},
author = {Park, Kwang-Hyun and Bien, Zeungnam and Lee, Ju-Jang and Kim, Byung Kook and Lim, Jong-Tae and Kim, Jin-Oh and Lee, Heyoung and Stefanov, Dimitar H. and Kim, Dae-Jin and Jung, Jin-Woo and Do, Jun-Hyeong and Seo, Kap-Ho and Kim, Chong Hui and Song, Won-Gyu and Lee, Woo-Jun},
doi = {10.1007/s10514-006-9012-9},
issn = {0929-5593},
journal = {Autonomous Robots},
keywords = {Assistive system,Human-friendly interface,Intelligent bed,Intelligent wheelchair,Movement assistance,Robotic hoist,Smart house,hri,smart home,user centered design},
month = {1},
number = {2},
pages = {183--198},
title = {{Robotic Smart House to Assist People with Movement Disabilities}},
url = {http://link.springer.com/10.1007/s10514-006-9012-9},
volume = {22},
year = {2007}
}
@article{Ma2007,
abstract = {The objective of this study was to identify task and vehicle factors that may affect driver situation awareness (SA) and its relationship to performance, particularly in strategic (navigation) tasks. An experiment was conducted to assess the effects of in-vehicle navigation aids and reliability on driver SA and performance in a simulated navigation task. A total of 20 participants drove a virtual car and navigated a large virtual suburb. They were required to follow traffic signs and navigation directions from either a human aid via a mobile phone or an automated aid presented on a laptop. The navigation aids operated under three different levels of information reliability (100{\%}, 80{\%} and 60{\%}). A control condition was used in which each aid presented a telemarketing survey and participants navigated using a map. Results revealed perfect navigation information generally improved driver SA and performance compared to unreliable navigation information and the control condition (task-irrelevant information). In-vehicle automation appears to mediate the relationship of driver SA to performance in terms of operational and strategic (navigation) behaviours. The findings of this work support consideration of driver SA in the design of future vehicle automation for navigation tasks.},
author = {Ma, R and Kaber, D B},
doi = {10.1080/00140130701318913},
issn = {0014-0139},
journal = {Ergonomics},
keywords = {Adult,Attention,Automation,Automation: instrumentation,Automobile Driving,Automobile Driving: psychology,Awareness,Computer Simulation,Data Collection,Female,Health Behavior,Humans,Male,Pilot Projects,Psychomotor Performance,Task Performance and Analysis},
month = {8},
number = {8},
pages = {1351--64},
title = {{Situation awareness and driving performance in a simulated navigation task.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17558674},
volume = {50},
year = {2007}
}
@article{Hershey1999,
abstract = {Psychophysical and physiological evidence shows that sound localization of acoustic signals is strongly inuenced by their synchrony with visual signals. This effect, known as ventriloquism, is at work when sound coming from the side of a TV set feels as if it were coming from the mouth of the actors. The ventriloquism effect suggests that there is important information about sound location encoded in the synchrony between the audio and video signals. In spite of this evidence, audiovisual synchrony is rarely used as a source of information in computer vision tasks. In this paper we explore the use of audio visual synchrony to locate sound sources. We developed a system that searches for regions of the visual landscape that correlate highly with the acoustic signals and tags them as likely to contain an acoustic source. We discuss our experience implementing the system, present results on a speaker localization task and discuss potential applications of the approach. Intro...},
author = {Hershey, J and Movellan, J},
journal = {Neural Information Processing Systems (NIPS'99)},
keywords = {audio visual,speaker localisation,ventriloquism},
pages = {813--819},
title = {{Audio-vision: Using audio-visual synchrony to locate sounds}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.6500},
year = {1999}
}
@article{Sutterer2008,
abstract = {Context-aware adaptive systems aim at automatically personalizing the user's environment depending on the user's situation, and hence, minimizing user interaction with the system. We present a novel user profile ontology that is dedicated to describe situation-dependent sub-profiles. This ontology can be used by context-aware adaptive service platforms for mobile communication and information services to automatically trigger the situation-dependent personalization of services. The design of this novel ontology also takes into consideration recommendations from the human factors research area. In particular, the ontology enables the easy specification of situational conditions and situation-dependent user sub-profiles.},
author = {Sutterer, Michael and Droegehorn, Olaf and David, Klaus},
doi = {10.1109/ACHI.2008.23},
isbn = {978-0-7695-3086-4},
journal = {First International Conference on Advances in Computer-Human Interaction},
pages = {230--235},
publisher = {Ieee},
title = {{UPOS: User Profile Ontology with Situation-Dependent Preferences Support}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4455987},
year = {2008}
}
@inproceedings{Brdiczka2007,
abstract = {In order to provide information and communication services without disrupting human activity, information services must implicitly conform to the current context of human activity. However, the variability of human environments and human preferences make it impossible to preprogram the appropriate behaviors for a context aware service. One approach to overcoming this obstacle is to have services adapt behavior to individual preferences though feedback from users. This article describes a method for learning situation models to drive context-aware services. With this approach an initial simplified situation model is adapted to accommodate user preferences by a supervised learning algorithm using feedback from users. To bootstrap this process, the initial situation model is acquired by applying an automatic segmentation process to sample observa- tion of human activities. This model is subsequently adapted to different operat- ing environments and human preferences through interaction with users, using a supervised learning algorithm.},
author = {Brdiczka, O and Crowley, J L and Reignier, P},
booktitle = {International Conference on Universal Access in Human-Computer Interaction, UAHCI 2007},
pages = {23--32},
title = {{Learning Situation Models for Providing Context- Aware Services}},
year = {2007}
}
@article{Holler2015,
abstract = {One of the most intriguing aspects of human communication is its turn-taking system. It requires the ability to process on-going turns at talk while planning the next, and to launch this next turn without considerable overlap or delay. Recent research has investigated the eye movements of observers of dialogs to gain insight into how we process turns at talk. More specifically, this research has focused on the extent to which we are able to anticipate the end of current and the beginning of next turns. At the same time, there has been a call for shifting experimental paradigms exploring social-cognitive processes away from passive observation toward on-line processing. Here, we present research that responds to this call by situating state-of-the-art technology for tracking interlocutors' eye movements within spontaneous, face-to-face conversation. Each conversation involved three native speakers of English. The analysis focused on question-response sequences involving just two of those participants, thus rendering the third momentarily unaddressed. Temporal analyses of the unaddressed participants' gaze shifts from current to next speaker revealed that unaddressed participants are able to anticipate next turns, and moreover, that they often shift their gaze toward the next speaker before the current turn ends. However, an analysis of the complex structure of turns at talk revealed that the planning of these gaze shifts virtually coincides with the points at which the turns first become recognizable as possibly complete. We argue that the timing of these eye movements is governed by an organizational principle whereby unaddressed participants shift their gaze at a point that appears interactionally most optimal: It provides unaddressed participants with access to much of the visual, bodily behavior that accompanies both the current speaker's and the next speaker's turn, and it allows them to display recipiency with regard to both speakers' turns.},
author = {Holler, Judith and Kendrick, Kobin H.},
doi = {10.3389/fpsyg.2015.00098},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Eye gaze,Eye-tracking,Turn projection,Turn-taking,Unaddressed participants},
month = {2},
number = {FEB},
pages = {1--14},
title = {{Unaddressed Participants' Gaze in Multi-Person Interaction: Optimizing Recipiency}},
url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.00098/abstract},
volume = {6},
year = {2015}
}
@inproceedings{Friedman1997,
abstract = {There is an obvious need for improving the per- formance and accuracy of a Bayesian network as newdata is observed. Because of errors in model construction and changes in the dynamics of the domains, we cannot afford to ignore the infor- mation in new data. While sequential update of parameters for a fixed structure can be accom- plished using standard techniques, sequential up- date of network structure is still an open problem. In this paper, we investigate sequential update of Bayesian networks were both parameters and structure are expected to change. We introduce a new approach that allows for the flexible ma- nipulation of the tradeoff between the quality of the learned networks and the amount of informa- tion that is maintained about past observations. We formally describe our approach including the necessary modifications to the scoring functions for learning Bayesiannetworks, evaluate its effec- tiveness through and empirical study, and extend it to the case of missing data. 1},
author = {Friedman, Nir and Goldszmidt, Moises},
booktitle = {UAI'97 Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence},
isbn = {1-55860-485-5},
keywords = {Bayesian Network,incremental,learning,missing data,structure learning},
number = {1},
pages = {165--174},
title = {{Sequential update of Bayesian network structure}},
url = {http://dl.acm.org/citation.cfm?id=2074246},
year = {1997}
}
@inproceedings{Vrzakova2016,
abstract = {When modeling natural conversational behavior of an agent, a head direction becomes an intuitive proxy to visual attention. We examine this assumption and carefully investigate the relationship between head directions and gaze dynamics through the use of eye-movement tracking. In a group conversation settings, we analyze relationships of the two nonverbal social signals -- head directions and gaze dynamics -- linked to influential and non-influential statements. We develop a clustering method to estimate the number of gaze targets. We employ this method to show that head and gaze dynamic behaviors are not correlated, and thus head cannot be used as a direct proxy to a person's gaze in the context of conversations. We also describe in detail how influential statements affect head and gaze behaviors. The findings have implications on methodology, modeling and design of natural conversational agents and present a supportive evidence for employing gaze-tracking into the future conversational technologies.},
address = {New York, New York, USA},
annote = {These findings have direct implications on the research that relies only on head directions and shifts as a direct proxy to gaze},
author = {Vrzakova, Hana and Bednarik, Roman and Nakano, Yukiko I. and Nihei, Fumio},
booktitle = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research {\&} Applications - ETRA '16},
doi = {10.1145/2857491.2857522},
isbn = {9781450341257},
keywords = {clustering,concepts,e,eye-tracking,g,gaze,hci,head,information interfaces and presentation,social interaction},
pages = {77--84},
publisher = {ACM Press},
title = {{Speakers' head and gaze dynamics weakly correlate in group conversation}},
url = {http://dl.acm.org/citation.cfm?doid=2857491.2857522},
year = {2016}
}
@inproceedings{Hung2014,
abstract = {In this paper we propose the novel task of detecting groups of con-versing people using only a single body-worn accelerometer per person. Our approach estimates each individual's social actions and uses the co-ordination of these social actions between pairs to identify group membership. The aim of such an approach is to be deployed in dense crowded environments. Our work differs sig-nificantly from previous approaches, which have tended to rely on audio and/or proximity sensing, often in much less crowded scenar-ios, for estimating whether people are talking together or who is speaking. Ultimately, we are interested in detecting who is speak-ing, who is conversing with whom, and from that, to infer socially relevant information about the interaction such as whether people are enjoying themselves, or the quality of their relationship in these extremely dense crowded scenarios. Striving towards this long-term goal, this paper presents a systematic study to understand how to detect groups of people who are conversing together in this set-ting, where we achieve a 64{\%} classification accuracy using a fully automated system.},
address = {New York, New York, USA},
author = {Hung, Hayley and Englebienne, Gwenn and {Cabrera Quiros}, Laura},
booktitle = {International Conference on Multimodal Interaction (ICMI)},
doi = {10.1145/2663204.2663228},
isbn = {9781450328852},
keywords = {accelerometer,algorithms,conversational groups,data,f-formations,human behavior,human factors,wearable sensors},
pages = {84--91},
publisher = {ACM Press},
title = {{Detecting Conversing Groups With a Single Worn Accelerometer}},
url = {http://dl.acm.org/citation.cfm?doid=2663204.2663228},
year = {2014}
}
@incollection{Devlin2006,
author = {Devlin, Keith},
booktitle = {Handbook of the History of Logic},
doi = {10.1016/S1874-5857(06)80034-8},
isbn = {9780444516220},
issn = {18745857},
keywords = {situation,situation semantics,theory},
pages = {601--664},
title = {{Situation theory and situation semantics}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1874585706800348},
volume = {7},
year = {2006}
}
@incollection{Al-Raziqi2017,
abstract = {Detecting groups plays an important role for group activity detection. In this paper, we propose an automatic group activity detection by segmenting the video sequences automatically into dynamic clips. As the first step, groups are detected by adopting a bottom-up hierarchical clustering, where the num- ber of groups is not provided beforehand. Then, groups are tracked over time to generate consistent trajectories. Furthermore, the Granger causality is used to compute the mutual effect between objects based on motion and appearances features. Finally, the Hierarchical Dirichlet Process is used to cluster the groups. Our approach not only detects the activity among the objects of a particular group (intra-group) but also extracts the activities among multiple groups (inter-group). The experiments on public datasets demonstrate the effectiveness of the proposed method. Although our approach is completely unsupervised, we achieved results with a clustering accuracy of up to 79.35{\%} and up to 81.94{\%} on the Behave and the NUS-HGA datasets.},
author = {Al-Raziqi, Ali and Denzler, Joachim},
doi = {10.1007/978-3-319-59876-5_44},
keywords = {group detection,tracking,yolo},
pages = {399--407},
title = {{Unsupervised Group Activity Detection by Hierarchical Dirichlet Processes}},
url = {http://link.springer.com/10.1007/978-3-319-59876-5{\_}44},
year = {2017}
}
@article{Ruhland2015,
abstract = {A person's emotions and state of mind are apparent in their face and eyes. As a Latin proverb states: ‘The face is the portrait of the mind; the eyes, its informers'. This presents a significant challenge for Computer Graphics researchers who generate artificial entities that aim to replicate the movement and appearance of the human eye, which is so important in human–human interactions. This review article provides an overview of the efforts made on tackling this demanding task. As with many topics in computer graphics, a cross-disciplinary approach is required to fully understand the workings of the eye in the transmission of information to the user. We begin with a discussion of the movement of the eyeballs, eyelids and the head from a physiological perspective and how these movements can be modelled, rendered and animated in computer graphics applications. Furthermore, we present recent research from psychology and sociology that seeks to understand higher level behaviours, such as attention and eye gaze, during the expression of emotion or during conversation. We discuss how these findings are synthesized in computer graphics and can be utilized in the domains of Human–Robot Interaction and Human–Computer Interaction for allowing humans to interact with virtual agents and other artificial entities. We conclude with a summary of guidelines for animating the eye and head from the perspective of a character animator.},
author = {Ruhland, Kerstin and Peters, Christopher E. and Andrist, Sean and Badler, Jeremy B. and Badler, Norman Ira and Gleicher, Michael L. and Mutlu, Bilge and McDonnell, Rachel},
doi = {10.1111/cgf.12603},
issn = {01677055},
journal = {Computer Graphics Forum},
keywords = {I.3.7 [Computer Graphics]: Three-Dimensional Graph,facial animation},
month = {9},
number = {6},
pages = {299--326},
title = {{A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI: Behaviour Generation, User Interaction and Perception}},
url = {http://doi.wiley.com/10.1111/cgf.12603},
volume = {34},
year = {2015}
}
@inproceedings{Katzenmaier2004,
abstract = {In this work we investigate the power of acoustic and visual cues, and their combination, to identify the addressee in a human-human-robot interaction. Based on eighteen audio-visual recordings of two human beings and a (simulated) robot we discriminate the interaction of the two humans from the interaction of one human with the robot. The paper compares the result of three approaches. The first approach uses purely acoustic cues to find the addressees. Low level, feature based cues as well as higher-level cues are examined. In the second approach we test whether the human's head pose is a suitable cue. Our results show that visually estimated head pose is a more reliable cue for the identification of the addressee in the human-human-robot interaction. In the third approach we combine the acoustic and visual cues which results in significant improvements.},
address = {New York, New York, USA},
author = {Katzenmaier, Michael and Stiefelhagen, Rainer and Schultz, Tanja},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/1027933.1027959},
isbn = {1581139950},
keywords = {attentive interfaces,focus of atten-,head pose estimation,human-robot,multimodal interfaces,speech recognition,tion},
pages = {144},
publisher = {ACM Press},
title = {{Identifying the Addressee in Human-Human-Robot Interactions Based on Head Pose and Speech}},
url = {http://portal.acm.org/citation.cfm?doid=1027933.1027959},
year = {2004}
}
@article{Davison2017,
abstract = {Fluent, multi-party, human-robot interaction calls for the mixing of deliberate conversational behaviour and re- active, semi-autonomous behaviour. In this project, we worked on a novel, state-of-the-art setup for realising such interactions. We approach this challenge from two sides. On the one hand, a dialogue manager requests deliberative behaviour and setting parameters on ongoing (semi)autonomous behaviour. On the other hand, robot control software needs to translate and mix these deliberative and bottom-up behaviours into consistent and coherent motion. The two need to collaborate to create behaviour that is fluent, naturally varied, and well-integrated. The resulting challenge is that, at the same time, this behaviour needs to conform to both high level requirements and to content and timing that are set by the dialogue manager. We tackled this challenge by designing a framework which can mix these two types of behaviour, using AsapRealizer, a Behaviour Markup Language realiser. We call this Heterogeneous Multilevel Mul- timodal Mixing (HMMM). Our framework is showcased in a scenario which revolves around a robot receptionist which is able to interact with multiple users.},
author = {Davison, Daniel and Gorer, Binnur and Kolkmeier, Jan and Linssen, Jeroen and Schadenberg, Bob R. and van de Vijver, Bob and Campbell, Nick and Dertien, Edwin and Reidsma, Dennis},
journal = {Proceedings of the 12th Summer Workshop on Multimodal Interfaces (eNTERFACE '16)},
keywords = {dialog,interaction,multi person},
pages = {6--20},
title = {{Things that Make Robots Go HMMM : Heterogeneous Multilevel Multimodal Mixing to Realise Fluent, Multiparty, Human-Robot Interaction}},
url = {http://wwwhome.ewi.utwente.nl/{~}truongkp/pubs/2016{\_}truong{\_}proceedings{\_}enterface16.pdf},
year = {2017}
}
@article{Admoni2017,
abstract = {This article reviews the state of the art in social eye gaze for human-robot interaction. It estab- lishes three categories of gaze research in HRI, defined by differences in goals and methods: a human-centered approach, which focuses on people's responses to gaze; a design-centered approach, which addresses the features of robot gaze behavior and appearance that improve interaction; and a technology-centered approach, which concentrates on the computational tools for implementing social eye gaze in robots. This paper begins with background information about gaze research in HRI, and ends with a set of open questions.},
author = {Admoni, Henny and Scassellati, Brian},
journal = {Journal of Human-Robot Interaction},
keywords = {collaboration,conver-,deixis,eye gaze,gaze,hri,mental models,nonverbal behavior,review,sation,social robotics,teamwork},
title = {{Social Eye Gaze in Human-Robot Interaction: A Review}},
year = {2017}
}
@techreport{Cohen2001,
abstract = {The paper introduces Voting EM, an online learning algorithm of Bayesian network parameters that builds on the EM(n) algorithm suggested by (Bauer et al., 1997). We prove convergence properties of the algorithm in the mean and variance, and demonstrate the algorithm's behavior on synthetic data. We show the relationship between Maximum-Likelihood (ML) counting and Voting EM. We demonstrate that Voting EM is able to adapt to changes in the modelled environment and to escape local maxima of the likelihood function. Voting EM also handles both the complete and missing data cases. We use the convergence properties to further improve Voting EM by automatically adapting the learning rate n. The resultant enhanced Voting EM algorithm converges more quickly and more closely to the true CPT parameters; further, it adapts more rapidly to changes in the modelled environment.},
author = {Cohen, Ira and Bronstein, Alexandre and Cozman, Fabio G and Urbana, N Mathews Ave},
title = {{Online Learning of Bayesian Network Parameters}},
volume = {55},
year = {2001}
}
@article{Chen2009a,
abstract = {This paper aims to serve two main purposes. In the first instance it aims to it provide an overview addressing the state-of-the-art in the area of activity recognition, in particular, in the area of object-based activity recognition. This will provide the necessary material to inform relevant research communities of the latest developments in this area in addition to providing a reference for researchers and system developers who ware working towards the design and development of activity-based context aware applications. In the second instance this paper introduces a novel approach to activity recognition based on the use of ontological modeling, representation and reasoning, aiming to consolidate and improve existing approaches in terms of scalability, applicability and easy-of-use.},
author = {Chen, Liming and Nugent, Chris},
doi = {10.1108/17440080911006199},
isbn = {1744-0084},
issn = {1744-0084},
journal = {International Journal of Web Information Systems},
keywords = {context,ontology,reasoning,situation recognition,survey},
month = {11},
number = {4},
pages = {410--430},
title = {{Ontology‐based activity recognition in intelligent pervasive environments}},
url = {http://www.emeraldinsight.com/doi/abs/10.1108/17440080911006199},
volume = {5},
year = {2009}
}
@article{Dautenhahn1999,
abstract = {This paper discusses the role of predictability and control in robot-human interaction. This involves the central question whether humans are good models for synthetic (social) agents. Design issues based on cognitive accounts towards robot-human interaction are discussed with respect to the author's recent work on building interactive robotic systems as remedial tools (teaching devices) for children with autism, a project which crucially requires a careful analysis of human-robot relationships, and which cannot rely on the ‘natural' tendency of humans to be interested in agents that appear life-like and social, a factor which is currently exploited extensively in the believable agents research community. The paper will argue that using humans as models for creating believable technology (e.g. making the robot life-like) is not a universal solution for designing robotic social actors.},
author = {Dautenhahn, Kerstin},
doi = {10.1.1.190.1767},
issn = {0289-2405},
journal = {Proceedings Third Cognitive Technology Conference CT'99},
number = {3},
pages = {359--374},
title = {{Robots as social actors: Aurora and the case of autism}},
volume = {359},
year = {1999}
}
@article{Rios-Martinez2015,
abstract = {In the context of a growing interest in modelling human behavior to increase the robots' social abilities, this article presents a survey related to socially-aware robot navigation. It presents a review from sociological concepts to social robotics and human-aware navigation. Social cues, signals and proxemics are discussed. Socially aware behavior in terms of navigation is tackled also. Finally, recent robotic experiments focusing on the way social conventions and robotics must be linked is presented.},
author = {Rios-Martinez, Jorge and Spalanzani, Anne and Laugier, Christian},
doi = {10.1007/s12369-014-0251-1},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Human-aware navigation,Proxemics,Socially-aware navigation,f-formations,proxemics,review,space},
month = {4},
number = {2},
pages = {137--153},
title = {{From Proxemics Theory to Socially-Aware Navigation: A Survey}},
url = {http://link.springer.com/10.1007/s12369-014-0251-1},
volume = {7},
year = {2015}
}
@inproceedings{Munch2011,
abstract = {Although computer vision and other machine perception have made great progress in recent years, corresponding high-level components have not progressed that fast. We present a general purpose framework for high-level situa- tion recognition that is suited for arbitrary application do- mains and sensor setups. Our approach is hierarchical as opposed to monolithic and we focus on modeling expert knowledge with Fuzzy Metric Temporal Logic and Situa- tion Graph Trees rather than learning from training data. To demonstrate the power and flexibility of our approach, we present case studies in two different settings: guiding the operator's attention in video surveillance and automatic report generation in smart environments. Our results show that this approach can yield a conceptually exhaustive situ- ation recognition for diverse input modalities and applica- tion domains.},
annote = {- surveys dealing with the high-level recognition of situations in image sequences are [3, 17, 25]

Metric Temporal Logic (FMTL) and Sit- uation Graph Trees (SGTs)
* no learning from data

Primitive knowledge is represented as a set of FMTL
* contains rules about spatiotemporal relations of the world
* vagueness: number intervals cannot be directly mapped to concepts
* uncertainty: uncertainty of low-level sensors
-{\textgreater} combined to 'truth-values'
* Inference on FMTL creates primitives which map to SGT-Elements

High-level knowledge is represented as SGT's (domain dependent)
* high-level situations
* finds one situation for each object and agend and timepoint
* fuzzy version finds multiple parallel situations for every agent

Experiment on synthetic data:
* detect
** Teamwork (3+ persons in a one zone, 1+ interacts=speaks or points)},
author = {Munch, David and IJsselmuiden, Joris and Arens, Michael and Stiefelhagen, Rainer},
booktitle = {2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)},
doi = {10.1109/ICCVW.2011.6130345},
isbn = {978-1-4673-0063-6},
keywords = {fuzzy logic,perfect data,prediction,situation graph trees,situation recognition},
month = {11},
pages = {882--889},
publisher = {IEEE},
title = {{High-level situation recognition using Fuzzy Metric Temporal Logic, case studies in surveillance and smart environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6130345},
year = {2011}
}
@article{Deng2015,
abstract = {Rich semantic relations are important in a variety of visual recognition problems. As a concrete example, group activity recognition involves the interactions and relative spatial relations of a set of people in a scene. State of the art recognition methods center on deep learning approaches for training highly effective, complex classifiers for interpreting images. However, bridging the relatively low-level concepts output by these methods to interpret higher-level compositional scenes remains a challenge. Graphical models are a standard tool for this task. In this paper, we propose a method to integrate graphical models and deep neural networks into a joint framework. Instead of using a traditional inference method, we instead use a sequential prediction approximation, modeled by a recurrent neural network. Beyond this, the appropriate structure for inference can be learned by imposing gates on edges between connections of nodes. Empirical results on group activity recognition demonstrate the potential of this model to handle highly structured learning tasks.},
archivePrefix = {arXiv},
author = {Deng, Zhiwei and Vahdat, Arash and Hu, Hexiang and Mori, Greg},
eprint = {1511.04196},
keywords = {activity recognition,group,recurrent networks},
month = {11},
title = {{Structure Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition}},
url = {http://arxiv.org/abs/1511.04196},
year = {2015}
}
@article{Buford2006,
abstract = {Natural and human-made disasters create unparalleled challenges to Disaster Situation Management. We describe two important information technology solutions meeting these challenges: distributed situation-driven disaster relief operations management and a multi-agent architecture scalable to large numbers of interacting agent platforms. To address limitations of event-driven cognitive processing, we extend the Belief-Desire-Intention multi- agent systems model with the capability of situation awareness. We describe how the key functions of event collection, situation identification, and situation assessment are implemented in a multi- agent systems architecture suitable to the characteristics of large- scale disaster recovery. To meet the requirements of large-scale agent systems, we propose an integrated agent platform with a peer- to-peer overlay which combines the semantic discovery mechanisms of agent systems with scalability to thousands of nodes of peer-to-peer overlays.},
author = {Buford, John F and Jakobson, Gabriel and Lewis, Lundy},
journal = {International Journal of Intelligent Control and Systems},
keywords = {situation},
number = {4},
pages = {284--295},
title = {{Multi-Agent Situation Management for Supporting Large-Scale Disaster Relief Operations}},
volume = {11},
year = {2006}
}
@article{Brdiczka2009b,
abstract = {Abstract $\backslash$nThis article addresses the problem of detecting configurations and activities of small groups of people in an augmented environment.$\backslash$nThe proposed approach takes a continuous stream of observations coming from different sensors in the environment as input.$\backslash$nThe goal is to separate distinct distributions of these observations corresponding to distinct group configurations and activities.$\backslash$nThis article describes an unsupervised method based on the calculation of the Jeffrey divergence between histograms over observations.$\backslash$nThese histograms are generated from adjacent windows of variable size slid from the beginning to the end of a meeting recording.$\backslash$nThe peaks of the resulting Jeffrey divergence curves are detected using successive robust mean estimation. After a merging$\backslash$nand filtering process, the retained peaks are used to select the best model, i.e. the best allocation of observation distributions$\backslash$nfor a meeting recording. These distinct distributions can be interpreted as distinct segments of group configuration and activity.$\backslash$nTo evaluate this approach, 5 small group meetings, one seminar and one cocktail party meeting have been recorded. The observations$\backslash$nof the small groups meetings and the seminar were generated by a speech activity detector, while the observations of the cocktail$\backslash$nparty meeting were generated by both the speech activity detector and a visual tracking system. The authors measured the correspondence$\backslash$nbetween detected segments and labeled group configurations and activities. The obtained results are promising, in particular$\backslash$nas the method is completely unsupervised.},
author = {Brdiczka, Oliver and Maisonnasse, J{\'{e}}r{\^{o}}me and Reignier, Patrick and Crowley, James L.},
doi = {10.1007/s10489-007-0074-y},
issn = {0924-669X},
journal = {Applied Intelligence},
keywords = {group detection,segmentation,situation},
month = {2},
number = {1},
pages = {47--57},
title = {{Detecting small group activities from multimodal observations}},
url = {http://link.springer.com/10.1007/s10489-007-0074-y},
volume = {30},
year = {2009}
}
@article{Dragone2015,
abstract = {Enabling robots to seamlessly operate as part of smart spaces is an important and extended challenge for robotics R{\&}D and a key enabler for a range of advanced robotic applications, such as AmbientAssisted Living (AAL) and home automation. The integration of these technologies is currently being pursued from two largely distinct view-points: On the one hand, people-centred initiatives focus on improving the user's acceptance by tackling human-robot interaction (HRI) issues, often adopting a social robotic approach, and by giving to the designer and - in a limited degree – to the final user(s), control on personalization and product customisation features. On the other hand, technologically-driven initiatives are building impersonal but intelligent systems that are able to pro-actively and autonomously adapt their operations to fit changing requirements and evolving users' needs, but which largely ignore and do not leverage human-robot interaction and may thus lead to poor user experience and user acceptance. In order to inform the development of a new generation of smart robotic spaces, this paper analyses and compares different research strands with a view to proposing possible integrated solutions with both advanced HRI and online adaptation capabilities.},
author = {Dragone, Mauro and Saunders, Joe and Dautenhahn, Kerstin},
doi = {10.1515/pjbr-2015-0009},
issn = {2081-4836},
journal = {Paladyn, Journal of Behavioral Robotics},
keywords = {am-,at university college dublin,bient assisted living,corresponding author,human robot interaction,mauro dragone,robotic ecology,smart homes,the corresponding author was,this work done while,ucd},
month = {1},
number = {1},
pages = {165--179},
title = {{On the Integration of Adaptive and Interactive Robotic Smart Spaces}},
url = {http://www.degruyter.com/view/j/pjbr.2015.6.issue-1/pjbr-2015-0009/pjbr-2015-0009.xml},
volume = {6},
year = {2015}
}
@article{Visser2010,
author = {Visser, Ingmar and Speekenbrink, Maarten},
doi = {10.18637/jss.v036.i07},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {7},
title = {{depmixS4 : An R Package for Hidden Markov Models}},
url = {http://www.jstatsoft.org/v36/i07/},
volume = {36},
year = {2010}
}
@inproceedings{Groh2011,
abstract = {Social Situations are social context models indicating n-ary social interaction on small spatio-temporal scales detected by means of Social Signal Processing. We discuss the problem of how to exchange and combine evidence from several sensor- sources for the benefit of algorithmic assessment of Social Situ- ations in a distributed agent-based Social Networking scenario. We propose a solution based on Subjective Logic that mediates between exchanging{\&}processing of raw low level sensor data, of intermediate results of 'sub-symbolic' probabilistic models typi- cally used for Social Signal Processing and of the final 'symbolic' Social Situation models.We evaluate key aspects of the approach on the basis of a social experiment, combining audio-based and geometry-of-interaction-based methods for Social Situation detection.},
author = {Groh, Georg and Fuchs, Christoph and Lehmann, Alexander},
booktitle = {International Conference on Social Computing},
doi = {10.1109/PASSAT/SocialCom.2011.18},
isbn = {978-1-4577-1931-8},
keywords = {5,a connected proper subset,a proper definition of,awareness,full mutual,general spatio-temporal,is a prerequisite for,k describing the semantics,of r 4,of the situation,projections of a more,reference x,s,social robotics,t and x are},
month = {10},
pages = {742--747},
title = {{Combining Evidence for Social Situation Detection}},
year = {2011}
}
@article{gco2,
author = {Kolmogorov, V. and Zabih, R.},
doi = {10.1109/TPAMI.2004.1262177},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {2},
number = {2},
pages = {147--159},
title = {{What Energy Functions Can be Minimized via Graph Cuts?}},
url = {http://ieeexplore.ieee.org/document/1262177/},
volume = {26},
year = {2004}
}
@inproceedings{Vazquez2017,
abstract = {We conducted a 2×2 between-subjects experiment to exam-ine the effects of two orientation and two gaze behaviors dur-ing group conversations for a mobile, low degree-of-freedom robot. For this experiment, we designed a novel protocol to induce changes in the robot's group and study different social contexts. In addition, we implemented a perception system to track participants and control the robot's orien-tation and gaze with little human intervention. The results showed that the gaze behaviors under consideration affected the participants' perception of the robot's motion and that its motion affected human perception of its gaze. This mu-tual dependency implies that robot gaze and body motion must be designed and controlled jointly, rather than inde-pendently of each other. Moreover, the orientation behaviors that we studied led to similar feelings of inclusion and sense of belonging to the robot's group, suggesting that both can be primitives for more complex orientation behaviors.},
address = {New York, New York, USA},
author = {V{\'{a}}zquez, Marynel and Carter, Elizabeth J. and McDorman, Braden and Forlizzi, Jodi and Steinfeld, Aaron and Hudson, Scott E.},
booktitle = {Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction - HRI '17},
doi = {10.1145/2909824.3020207},
isbn = {9781450343367},
issn = {21672148},
keywords = {conversational,gaze,group,interaction,multiparty,robot motion,social human-robot interaction},
pages = {42--52},
publisher = {ACM Press},
title = {{Towards Robot Autonomy in Group Conversations}},
url = {http://dl.acm.org/citation.cfm?doid=2909824.3020207},
year = {2017}
}
@inproceedings{Hammerla2015,
abstract = {The ability to generalise towards either new users or unfore- seen behaviours is a key requirement for activity recognition systems in ubiquitous computing. Differences in recognition performance for the two application cases can be significant, and user-dependent performance is typically assumed to be an upper bound on performance. We demonstrate that this as- sumption does not hold for the widely used cross-validation evaluation scheme that is typically employed both during sys- tem bootstrapping and for reporting results. We describe how the characteristics of segmented time-series data render ran- dom cross-validation a poor fit, as adjacent segments are not statistically independent. We develop an alternative approach – meta-segmented cross validation – that explicitly circum- vents this issue and evaluate it on two data-sets. Results in- dicate a significant drop in performance across a variety of feature extraction and classification methods if this bias is re- moved, and that prolonged, repetitive activities are particu- larly affected.},
author = {Hammerla, Nils Y and Pl, Thomas},
booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
isbn = {9781450335744},
keywords = {Activity Recognition,Cross validation,Evaluation,Model selection,corss validation,time series,validation methods},
pages = {1041--1051},
title = {{Let ' s ( not ) Stick Together : Pairwise Similarity Biases Cross-Validation in Activity Recognition}},
year = {2015}
}
@inproceedings{Tachos,
abstract = {Human activity recognition has gained a lot of attention in the computer vision so- ciety, due to its usefulness in numerous contexts. This work focuses on the recognition of Activities of Daily Living (ADL), which involves recordings constrained to specific daily activities that are of interest in assisted living or smart home environments. We present a novel technique for spatial activity localisation and recognition from colour- depth sequences, tailored to Activities of Daily Living (ADLs), which usually take place in relatively constrained environments. The proposed method significantly reduces the computational cost of activity recognition, while at the same time achieving a competi- tive accuracy rate, comparable to the State of the Art (SoA). This is achieved by the in- troduction of appearance and depth based spatiotemporal volumes, the Spatio-Temporal Activity Cells (STACs), extracted using appearance and depth information from succes- sive video frames. A novel adaptive background modelling method follows, to char- acterize the STACs as “active” or “inactive” and accumulate them into foreground or background history volumes respectively. After activity detection using the STACs, ac- tivity recognition takes place using a novel, depth-based descriptor, the Histogram of Surface Normals Projections (HONSP), in combination with well known appearance de- scriptors (Histograms of Oriented Gradients, HOGs). Fisher encoding aggregates them into a fixed size vector to train a multiclass SVM model, which is then used for activity recognition. Experiments on different ADL datasets recorded with elderly people verify that the suggested algorithm is very appropriate for real life scenarios. 1},
author = {Tachos, Stavros and Avgerinakis, Konstantinos and Briasouli, Alexia and Kompatsiaris, Ioannis},
booktitle = {British Machine Vision Conference},
keywords = {activities of daily living,depth,hog,temporal},
title = {{Appearance and Depth for Rapid Human Activity Recognition in Real Applications}},
year = {2015}
}
@article{Kenwright2012,
abstract = {This paper presents an overview of the analytical advantages of dual-quaternions and their potential in the areas of robotics, graphics, and animation. While quaternions have proven themselves as providing an unambiguous, un-cumbersome, computationally efficient method of representing rotational information, we hope after reading this paper the reader will take a parallel view on dual-quaternions. Despite the fact that the most popular method of describing rigid transforms is with homogeneous transformation matrices they can suffer from several downsides in comparison to dual-quaternions. For example, dual-quaternions offer increased computational efficiency, reduced overhead, and coordinate invariance. We also demonstrate and explain how, dual-quaternions can be used to generate constant smooth interpolation between transforms. Hence, this paper aims to provide a comprehensive step-by-step explanation of dual-quaternions, and it comprising parts (i.e., quaternions and dual-numbers) in a straightforward approach using practical real-world examples and uncomplicated implementation information. While there is a large amount of literature on the theoretical aspects of dual-quaternions there is little on the practical details. So, while giving a clear no-nonsense introduction to the theory, this paper also explains and demonstrates numerous workable aspect using real-world examples with statistical results that illustrate the power and potential of dual-quaternions.},
author = {Kenwright, Ben},
keywords = {blending,dual-number,dual-quaternion,interpolation,introduction,quaternion,transformation,why should we use},
number = {October},
pages = {1--11},
title = {{Dual-Quaternions From Classical Mechanics to Computer Graphics and Beyond}},
url = {www.xbdev.net},
year = {2012}
}
@article{Yang2017,
author = {Yang, K and Cho, Sung-Bae},
doi = {10.1177/1550147717708986},
issn = {1550-1477},
journal = {International Journal of Distributed Sensor Networks},
keywords = {19 april 2017,6 june 2016,academic editor,accepted,context-aware services,date received,davide brunelli,internet of things,modular bayesian networks,smart tv},
month = {5},
number = {5},
pages = {155014771770898},
title = {{A context-aware system in Internet of Things using modular Bayesian networks}},
url = {http://journals.sagepub.com/doi/10.1177/1550147717708986},
volume = {13},
year = {2017}
}
@book{Meyer2018,
address = {Stuttgart},
author = {Meyer, Christian},
doi = {10.1007/978-3-476-04606-2},
isbn = {978-3-476-04605-5},
publisher = {J.B. Metzler},
title = {{Culture, Practice, and the Body}},
url = {http://link.springer.com/10.1007/978-3-476-04606-2},
year = {2018}
}
@article{Meyer-Delius2009,
abstract = {To act intelligently in dynamic environments, a system must understand the current situation it is involved in at any given time. This requires dealing with temporal context, handling multiple and ambiguous interpretations, and accounting for various sources of uncertainty. In this paper we propose a probabilistic approach to modeling and recognizing situations.We define a situation as a distribution over sequences of states that have some meaningful interpretation. Each situ- ation is characterized by an individual hidden Markov model that describes the corresponding distribution. In particular, we consider typical traffic scenarios and describe how our frame- work can be used to model and track different situations while they are evolving. The approach was evaluated experimentally in vehicular traffic scenarios using real and simulated data. The results show that our system is able to recognize and track multiple situation instances in parallel and make sensible decisions between competing hypotheses. Additionally, we show that our models can be used for predicting the position of the tracked vehicles.},
author = {Meyer-Delius, D. and Plagemann, C. and Burgard, W.},
doi = {10.1109/ROBOT.2009.5152838},
isbn = {978-1-4244-2788-8},
journal = {2009 IEEE International Conference on Robotics and Automation},
month = {5},
pages = {459--464},
publisher = {Ieee},
title = {{Probabilistic situation recognition for vehicular traffic scenarios}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152838},
year = {2009}
}
@article{Pentland2007,
author = {Pentland, Alex Sandy},
journal = {Signal Processing Magazine, IEEE},
keywords = {social robotics},
number = {4},
pages = {108--111},
title = {{Social Signal Processing}},
volume = {24},
year = {2007}
}
@article{Olaru2014,
abstract = {In the field of ambient assisted living, the best results are achieved with systems that are less intrusive and more intelligent, that can easily integrate both formal and informal caregivers and that can easily adapt to the changes in the situation of the elderly or disabled person. This paper presents a graph-based representation for context information and a simple and intuitive method for situation recognition. Both the input and the results are easy to visualize, understand and use. Experiments have been performed on several AAL-specific scenarios.},
author = {Olaru, Andrei and Florea, Adina},
doi = {10.3390/s140611110},
issn = {1424-8220},
journal = {Sensors},
keywords = {Ambient assisted living,Ambient intelligence,Context awareness,Context graphs,Context patterns,Graph matching,Situation recognition,graph matching,situation,time},
month = {6},
number = {6},
pages = {11110--11134},
title = {{Context Graphs as an Efficient and User-Friendly Method of Describing and Recognizing a Situation in AAL}},
url = {http://www.mdpi.com/1424-8220/14/6/11110/},
volume = {14},
year = {2014}
}
@inproceedings{Budde2013,
address = {New York, NY, USA},
author = {Budde, Matthias and Berning, Matthias and Baumg{\"{a}}rtner, Christopher and Kinn, Florian and Kopf, Timo and Ochs, Sven and Reiche, Frederik and Riedel, Till and Beigl, Michael},
booktitle = {Conference on Pervasive and Ubiquitous Computing Adjunct Publication (UbiComp Adjunct)},
doi = {10.1145/2494091.2494184},
isbn = {9781450322157},
keywords = {12,13,2013,control,demo,interaction in,point,poster,september 8,sion,switzerland,ubicomp,video presentations,zurich},
pages = {303--306},
publisher = {ACM Press},
title = {{Point {\&} Control---Interaction in Smart Environments}},
url = {http://dl.acm.org/citation.cfm?doid=2494091.2494184},
year = {2013}
}
@inproceedings{Setti2013,
author = {Setti, Francesco and Lanz, Oswald and Ferrario, Roberta and Murino, Vittorio and Cristani, Marco},
booktitle = {International Conference on Image Processing},
doi = {10.1109/ICIP.2013.6738732},
isbn = {978-1-4799-2341-0},
month = {9},
pages = {3547--3551},
publisher = {IEEE},
title = {{Multi-Scale F-Formation Discovery for Group Detection}},
url = {http://ieeexplore.ieee.org/document/6738732/},
year = {2013}
}
@article{glas2003,
abstract = {Diagnostic testing can be used to discriminate subjects with a target disorder from subjects without it. Several indicators of diagnostic performance have been proposed, such as sensitivity and specificity. Using paired indicators can be a disadvantage in comparing the performance of competing tests, especially if one test does not outperform the other on both indicators. Here we propose the use of the odds ratio as a single indicator of diagnostic performance. The diagnostic odds ratio is closely linked to existing indicators, it facilitates formal meta-analysis of studies on diagnostic test performance, and it is derived from logistic models, which allow for the inclusion of additional variables to correct for heterogeneity. A disadvantage is the impossibility of weighing the true positive and false positive rate separately. In this article the application of the diagnostic odds ratio in test evaluation is illustrated. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
author = {Glas, Afina S. and Lijmer, Jeroen G. and Prins, Martin H. and Bonsel, Gouke J. and Bossuyt, Patrick M.M.},
doi = {10.1016/S0895-4356(03)00177-X},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Diagnostic odds ratio,Diagnostic test,Logistic regression,Meta-analysis,Sensitivity and specificity,Tutorial,medicine,statictics,statistical tests},
month = {11},
number = {11},
pages = {1129--1135},
title = {{The Diagnostic Odds Ratio: a Single Indicator of Test Performance}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S089543560300177X https://linkinghub.elsevier.com/retrieve/pii/S089543560300177X},
volume = {56},
year = {2003}
}
@inproceedings{Kopp2018,
abstract = {Conversational agents can provide valuable cognitive and/or emotional assistance to elderly users or people with cognitive impairments who often have difficulties in organizing and following a structured day schedule. Previous research showed that a virtual assistant that can interact in spoken language would be a desirable help for those users. However, these user groups pose specific requirements for spoken dialogue interaction that existing systems hardly meet. This paper presents work on a virtual conversational assistant that was designed for, and together with, elderly as well as cognitively handicapped users. It has been specifically developed to enable ‘socially cooperative dialogue' – adaptive and aware conversational interaction in which mutual understanding is co-constructed and ensured collaboratively. The technical approach is described and results of evaluation studies are reported.},
address = {Stockholm, Sweden},
author = {Kopp, Stefan and Cyra, Katharina and Kummert, Franz and Schillingmann, Lars and Brandt, Mara and Freigang, Farina and Opfermann, Christiane and Stra{\ss}mann, Carolin and Yaghoubzadeh, Ramin and Buschmeier, Hendrik and Kr{\"{a}}mer, Nicole and Pitsch, Karola and Wall, Eduard},
booktitle = {AAMAS Workshop on Intelligent Conversation Agents in Home and Geriatric Care Applications co-located with the Federated AI Meeting},
keywords = {Conversational assistants,Cooperative dialogue,Elderly users},
pages = {10--17},
title = {{Conversational Assistants for Elderly Users – The Importance of Socially Cooperative Dialogue}},
url = {https://pub.uni-bielefeld.de/record/2920166},
volume = {2338},
year = {2018}
}
@inproceedings{Nenci2014,
abstract = {Most robots need the ability to communicate with a base station or with an operator during their mission. Teleoperated and semi-autonomous robots typically communicate continuously through a network connection with an operator. Transmitting raw sensor data over a low bandwidth network such as wireless or HSDPA, however, is problematic as the stream of sensor data is often large. In this paper, we present a method that exploits H.264 compression to reduce the size of range data streams from sensors such as the Kinect camera or the Velodyne 3D laser scanner. We developed a practical and effective solution that exploits the state of the art in video compression to produce high-quality results. Our method is easy to implement and can have practical impact for researchers building robots for the real world. We implemented and thoroughly tested our approach using a large number of range data streams. Furthermore, we analyzed the impact of data compression on the accuracy and size of the transmitted data. We show that even a highly compressed stream of depth images can be used with dense mapping techniques such as KinFu for building environment models.},
annote = {software: http://www.ipb.uni-bonn.de/data-software/depth-streaming-using-h-264/

stackoverflow: https://stackoverflow.com/questions/34286771/compression-of-rgb-d-video-from-a-kinect-camera},
author = {Nenci, Fabrizio and Spinello, Luciano and Stachniss, Cyrill},
booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6943095},
isbn = {978-1-4799-6934-0},
issn = {21530866},
month = {9},
pages = {3794--3799},
publisher = {IEEE},
title = {{Effective compression of range data streams for remote robot operations using H.264}},
url = {http://ieeexplore.ieee.org/document/6943095/},
year = {2014}
}
@article{Newman2006,
abstract = {Many networks of interest in the sciences, including social networks, computer networks, and metabolic and regulatory networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure is one of the outstanding issues in the study of networked systems. One highly effective approach is the optimization of the quality function known as "modularity" over the possible divisions of a network. Here I show that the modularity can be expressed in terms of the eigenvectors of a characteristic matrix for the network, which I call the modularity matrix, and that this expression leads to a spectral algorithm for community detection that returns results of demonstrably higher quality than competing methods in shorter running times. I illustrate the method with applications to several published network data sets.},
author = {Newman, M E J},
doi = {10.1073/pnas.0601602103},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
month = {6},
number = {23},
pages = {8577--82},
title = {{Modularity and community structure in networks.}},
url = {http://www.pnas.org/cgi/content/short/103/23/8577},
volume = {103},
year = {2006}
}
@book{Werbos1975,
author = {Werbos, P J},
publisher = {Harvard University},
title = {{Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences}},
url = {https://books.google.de/books?id=z81XmgEACAAJ},
year = {1975}
}
@techreport{RFC3550,
annote = {$\backslash$url{\{}http://www.rfc-editor.org/rfc/rfc3550.txt{\}}},
author = {Schulzrinne, H. and Casner, S. and Frederick, R. and Jacobson, V.},
institution = {RFC Editor},
number = {64},
title = {{RTP: A Transport Protocol for Real-Time Applications}},
type = {STD},
url = {http://www.rfc-editor.org/rfc/rfc3550.txt},
year = {2003}
}
@phdthesis{Christiansen2015,
abstract = {n this essay I discuss situation awareness and sensemaking in organizations with respect to mind. Various models of mind are discussed and much effort is made to disprove Cartesian dualism as a viable theory. Gilbert Ryle's model of mind as a disposition to heed as used by Karl Weick is used as the construct of collective mind. The research question is to prove situation awareness as essentially mind and thus the disposition to heed. Three small cases are investigated to show the interdependence between situation awareness and sensemaking. The dependencies between these elements are discussed regarding known epistemology and on the fundamental level. Using current ontological standings in organizational theory and ergonomics this proved to be difficult, because it gave no common consensus regarding the primitives of the mind construct. To resolve this I have made a suggestion to ontology based on a restricted view of Heidegger's notion of dasein and existential time and existential space as n-dimensional Euclidean spaces as sole primitives. Both are properties of dasein and in physical world but restricted to human existence. Ontology should prove useful to other research areas than organizational theory as a model of human mind. Keywords:},
author = {Christiansen, Atle M.},
title = {{Stop Making Sense: Close Scrutiny of Situation Awareness in Organizations}},
year = {2015}
}
@book{Koller2009,
annote = {Sufficient Statistic: "a function of the data that summarizes the relevant information for computing the likelihood"},
author = {Koller, Daphne and Friedman, Nir},
keywords = {learning,person tracking},
pages = {1--1265},
title = {{Probabilistic Graphical Models: Principles and Techniques}},
year = {2009}
}
@article{Staudte2014,
abstract = {Previous research has shown that listeners follow speaker gaze to mentioned objects in a shared environment to ground referring expressions, both for human and robot speakers. What is less clear is whether the benefit of speaker gaze is due to the inference of referential intentions (Staudte and Crocker, 2011) or simply the (reflexive) shifts in visual attention. That is, is gaze special in how it affects simultaneous utterance comprehension? In four eye-tracking studies we directly contrast speech-aligned speaker gaze of a virtual agent with a non-gaze visual cue (arrow). Our findings show that both cues similarly direct listeners' attention and that listeners can benefit in utterance comprehension from both cues. Only when they are similarly precise, however, does this equality extend to incongruent cueing sequences: that is, even when the cue sequence does not match the concurrent sequence of spoken referents can listeners benefit from gaze as well as arrows. The results suggest that listeners are able to learn a counter-predictive mapping of both cues to the sequence of referents. Thus, gaze and arrows can in principle be applied with equal flexibility and efficiency during language comprehension. {\textcopyright} 2014 Elsevier B.V.},
author = {Staudte, Maria and Crocker, Matthew W. and Heloir, Alexis and Kipp, Michael},
doi = {10.1016/j.cognition.2014.06.003},
isbn = {1873-7838 (Electronic)$\backslash$r0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
keywords = {Arrows,Gaze,Joint attention,Language comprehension,Referential intention,Visual attention shifts,gaze,interaction,itention},
month = {10},
number = {1},
pages = {317--328},
publisher = {Elsevier B.V.},
title = {{The influence of speaker gaze on listener comprehension: Contrasting visual versus intentional accounts}},
url = {http://dx.doi.org/10.1016/j.cognition.2014.06.003 http://linkinghub.elsevier.com/retrieve/pii/S0010027714001139},
volume = {133},
year = {2014}
}
@article{Lee,
abstract = {Existing telehealth systems do not perform as effectively as would be expected due to their asymmetric focus on sensing and monitoring with little support or assurance to affect or alter behaviors. Many patients, including older adults, may be resistant to change, which can diminish the efficacy of telehealth systems. In response, we developed the Action-based Behavior Model (ABM) that enables persuasive telehealth. However, as will be shown, ABM requires an ongoing assessment of user behavior response and compliance to cyber influence. There are many challenging problems that must be overcome to enable such assessment. In this paper, we present Situation-based Assess Tree (SAT) as a methodology for domain-specific behavior assessment under ABM. We present the SAT methodology along with a proof-of-concept validation study based on a trace-driven simulation approach. The preliminary validation results demonstrate that SAT is sentient to both compliant and non-compliant user responses. It also demonstrates SAT's ability to learn different user personas through the assessment process.},
author = {Lee, Duckki and Helal, Sumi and Sung, Yunsick and Anton, Stephen and Model, A Action-based Behavior},
title = {{Situation-base d Assess Tree for User Behavior Assessment in Persuasive Telehealth}},
year = {2014}
}
@inproceedings{Fischer2011,
abstract = {Humans make decisions on the basis of their situation awareness and it is well-known that insufficient situation awareness leads to incorrect decisions. The challenge of an advanced surveillance system for supporting situation awareness of a human decision maker is therefore to detect and assess complex situations that evolve over time. In this article, we present a conceptual framework for automatic situation assessment that consists of four parts, namely the situation characterization, the situation abstraction, the situation recognition and the situation projection. The situation itself can be described at several different levels of abstraction. The proposed framework can be used as a guideline when designing automatic situation assessment processes.},
annote = {"As a high level of situation awareness provides the complete knowledge which is necessary for effective decision making, the decision process itself and the performance of actions are separate stages of the dynamic decision making process"

Roy proposed in [16] the concept of situation analysis as a process to provide and maintain a state of situation awareness. He also proposed definitions of situational elements like entities, events and activities.

Hidden Markov models can be used for situation recognition [11]

Def. Situation: "A situation at time t is defined as a world state, which is characterized by the collection of relevant activities up to the time t and their interpretation with respect to the context knowledge."

Situation Characterization: Creating situation templates (typically through experts).
Situation Abstraction: Creates Situation Representation from template and observations.
Situation Recognition: Calculate belief for every template according to representation.
Situation Projection: Spacial and temporal projection.},
author = {Fischer, Y. and Bauer, A. and Beyerer, J.},
booktitle = {2011 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support (CogSIMA)},
doi = {10.1109/COGSIMA.2011.5753451},
isbn = {978-1-61284-785-6},
keywords = {OOWM,conceptual,framework,high-level data fusion,situation,situation assessment,situation awareness,situation recognition,situational modeling,surveillance,system},
month = {2},
pages = {234--239},
publisher = {IEEE},
title = {{A conceptual framework for automatic situation assessment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5753451},
year = {2011}
}
@inproceedings{Bernotat2016,
abstract = {Abstract. The purpose of this Wizard-of-Oz study was to explore the intuitive verbal and non-verbal goal-directed behavior of na{\"{i}}ve participants in an intelli- gent robotics apartment. In a smart home context, participants had to complete seven mundane tasks, for instance, they were asked to turn the light on. Partici- pants were explicitly instructed to consider nonstandard ways of completing the respective tasks. A multi-method approach revealed that most participants fa- vored speech and interfaces like switches and screens to communicate with the intelligent robotics apartment. However, they required instructions to use the in- terfaces in order to perceive them as competent targets for human-machine inter- action. Hence, first important steps were taken to investigate how to design an intelligent robotics apartment in a user-centered and user-friendly manner.},
author = {Bernotat, Jasmin and Schiffhauer, Birte and Eyssel, Friederike and Holthaus, Patrick and Leichsenring, Christian and Richter, Viktor and Pohling, Marian and Carlmeyer, Birte and K{\"{o}}ster, Norman and {Meyer zu Borgsen}, Sebastian and Zorn, Ren{\'{e}} and Engelmann, Kai Frederic and Lier, Florian and Schulz, Simon and Br{\"{o}}hl, Rebecca and Seibel, Elena and Hellwig, Paul and Cimiano, Philipp and Kummert, Franz and Schlangen, David and Wagner, Petra and Hermann, Thomas and Wachsmuth, Sven and Wrede, Britta and Wrede, Sebastian},
booktitle = {International Conference on Social Robotics (ICSR)},
doi = {10.1007/978-3-319-47437-3_96},
keywords = {human-robot-interaction,intuitive design,smart home,social robot,usability,use-case scenario,user-centered design},
pages = {982--992},
publisher = {Springer International Publishing},
title = {{Welcome to the Future – How Na{\"{i}}ve Users Intuitively Address an Intelligent Robotics Apartment}},
url = {http://link.springer.com/10.1007/978-3-319-47437-3{\_}96},
year = {2016}
}
@article{Ishak2011,
abstract = {Probabilistic Graphical Models (PGMs) are powerful tools for representing and reasoning under uncertainty. Although useful in sev- eral domains, PGMs suffer from their build- ing phase known to be mostly an NP-hard problem which can limit in some extent their application, especially in real world applica- tions. Ontologies, from their side, provide a body of structured knowledge characterized by its semantic richness. This paper proposes to harness ontologies representation capabil- ities in order to enrich the process of PGMs building. We are in particular interested in object oriented Bayesian networks (OOBNs) which are an extension of standard Bayesian networks (BNs) using the object paradigm. We show how the semantical richness of on- tologies might be a potential solution to ad- dress the challenging field of structural learn- ing of OOBNs while minimizing experts in- volvement which is not always obvious to ob- tain. More precisely, we propose to set up a set of mapping rules allowing us to gener- ate a prior OOBN structure by morphing an ontology related to the problem under study to be used as a starting point to the global OOBN building algorithm.},
author = {Ishak, Mouna Ben and Leray, Philippe and Amor, Nahla Ben},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {Object oriented bayesian networks,generate,object oriented bayesian networks,ontology,pgm,structure learning},
pages = {9--17},
title = {{Ontology-based generation of object oriented bayesian networks}},
volume = {818},
year = {2011}
}
@inproceedings{Malik2019,
address = {New York, New York, USA},
author = {Malik, Usman and Barange, Mukesh and Ghannad, Naser and Saunier, Julien and Pauchet, Alexandre},
booktitle = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents - IVA '19},
doi = {10.1145/3308532.3329462},
isbn = {9781450366724},
keywords = {Human-Agent Interaction,Machine Learning,Mixed Communities,Multimodal Interaction,Multiparty Interaction,acm reference format,action,human-agent interaction,machine learning,mixed communities,multimodal interaction,multiparty inter-},
pages = {119--126},
publisher = {ACM Press},
title = {{A Generic Machine Learning Based Approach for Addressee Detection In Multiparty Interaction}},
url = {http://dl.acm.org/citation.cfm?doid=3308532.3329462},
year = {2019}
}
@inproceedings{Skantze2014,
abstract = {In this paper, we present a dialog system that was exhibited at the Swedish National Museum of Science and Technology. Two visitors at a time could play a collaborative card sorting game together with the robot head Furhat, where the three players dis- cuss the solution together. The cards are shown on a touch table between the players, thus constituting a target for joint attention. We describe how the system was implemented in order to manage turn-taking and attention to users and objects in the shared physi- cal space. We also discuss how multi-modal redundancy (from speech, card movements and head pose) is exploited to maintain meaningful discussions, given that the system has to process con- versational speech from both children and adults in a noisy envi- ronment. Finally, we present an analysis of 373 interactions, where we investigate the robustness of the system, to what extent the system's attention can shape the users' turn-taking behaviour, and how the system can produce multi-modal turn-taking signals (filled pauses, facial gestures, breath and gaze) to deal with pro- cessing delays in the system.},
address = {New York, New York, USA},
annote = {Interaktionsstudie

* Robustheit
* Formen des turn-taking durch Roboter
* turn-taking Signale um ({\"{a}}hm's) um 'Rederecht' zu behalten

* Addressaterkennung: Reagiere wenn angeschaut w{\"{a}}hrend sprache produziert wurde (1 Miktophon/Speechrec pp)

={\textgreater} 

* turn-taking Signale erm{\"{o}}glichen erlangen und steuerung von 'Rederecht'},
author = {Skantze, Gabriel and Johansson, Martin and Beskow, Jonas},
booktitle = {International Conference on Multimodal Interaction (ICMI)},
doi = {10.1145/2818346.2820749},
isbn = {9781450339124},
keywords = {attention,gaze,human-robot interaction,multi-party turn-taking},
pages = {67--74},
publisher = {ACM Press},
title = {{Exploring Turn-taking Cues in Multi-party Human-Robot Discussions about Objects}},
url = {http://dl.acm.org/citation.cfm?doid=2818346.2820749},
year = {2015}
}
@article{breazeal2003,
author = {Breazeal, Cynthia},
doi = {10.1016/S0921-8890(02)00373-1},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
month = {3},
number = {3-4},
pages = {167--175},
publisher = {Elsevier},
title = {{Toward sociable robots}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889002003731},
volume = {42},
year = {2003}
}
@inproceedings{Hegel2008a,
abstract = {Anthropomorphism is one of the keys to understand the expectations people have about social robots. In this paper we address the question of how a robotpsilas actions are perceived and represented in a human subject interacting with the robot and how this perception is influenced only by the appearance of the robot. We present results of an interaction-study in which participants had to play a version of the classical Prisonerspsila Dilemma Game (PDG) against four opponents: a human partner (HP), an anthropomorphic robot (AR), a functional robot (FR), and a computer (CP). As the responses of each game partner were randomized unknowingly to the participants, the attribution of intention or will to an opponent (i.e. HP, AR, FR or CP) was based purely on differences in the perception of shape and embodiment. We hypothesize that the degree of human-likeness of the game partner will modulate what the people attribute to the opponents - the more human like the robot looks the more people attribute human-like qualities to the robot.},
author = {Hegel, Frank and Krach, Soren and Kircher, Tilo and Wrede, Britta and Sagerer, Gerhard},
booktitle = {International Symposium on Robot and Human Interactive Communication (RO-MAN)},
doi = {10.1109/ROMAN.2008.4600728},
isbn = {978-1-4244-2212-8},
keywords = {Anthropomorphism,Cognitive robotics,Employment,Fixtures,Hospitals,Human robot interaction,Humanoid robots,Magnetic resonance imaging,Particle measurements,Shape,anthropomorphic robot,anthropomorphism,classical prisoners dilemma game,computer,functional robot,game theory,human partner,interaction study,man-machine systems,robots,social aspects of automation,social robots},
month = {8},
pages = {574--579},
publisher = {IEEE},
title = {{Understanding Social Robots: A User Study on Anthropomorphism}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4600728 http://ieeexplore.ieee.org/document/4600728/},
year = {2008}
}
@inproceedings{Li2018,
abstract = {Mixed reality offers new potentials for social interaction experiences with virtual agents. In addition, it can be used to experiment with the design of physical robots. However, while previous studies have investigated comfortable social distances between humans and artificial agents in real and virtual environments, there is little data with regards to mixed reality environments. In this paper, we conducted an experiment in which participants were asked to walk up to an agent to ask a question, in order to investigate the social distances maintained, as well as the subject's experience of the interaction. We manipulated both the embodiment of the agent (robot vs. human and virtual vs. physical) as well as closed vs. open posture of the agent. The virtual agent was displayed using a mixed reality headset. Our experiment involved 35 participants in a within-subject design. We show that, in the context of social interactions, mixed reality fares well against physical environments, and robots fare well against humans, barring a few technical challenges. {\textcopyright} 2018 ACM.},
address = {New York, New York, USA},
author = {Li, Chengjie and Androulakaki, Theofronia and Gao, Alex Yuan and Yang, Fangkai and Saikia, Himangshu and Peters, Christopher and Skantze, Gabriel},
booktitle = {Proceedings of the 18th International Conference on Intelligent Virtual Agents - IVA '18},
doi = {10.1145/3267851.3267870},
isbn = {9781450360135},
keywords = {Interaction,Mixed reality,Posture,Proxemics,Virtual agents},
pages = {191--196},
publisher = {ACM Press},
title = {{Effects of Posture and Embodiment on Social Distance in Human-Agent Interaction in Mixed Reality}},
url = {http://dl.acm.org/citation.cfm?doid=3267851.3267870},
year = {2018}
}
@inproceedings{Hulnhagen2010,
abstract = {This paper presents a general approach for recog- nition of driving maneuvers in advanced driver assistance systems (ADAS). Such systems often rely on the identification of driving maneuvers (overtaking, left turn at intersections, etc.) to improve the prediction of potential collisions or to trigger appropriate support for the driver. The proposed maneuver recognition approach combines a fuzzy rule base to model basic maneuver elements and probabilistic finite-state machines to capture all possible sequences of basic elements that constitute a driving maneuver. The proposed method is specifically tailored to ADAS requirements because of its low computational complexity, its flexibility and its straight-forward design based on easily comprehensible logical rules. In addition, we propose a suitable training method to optimize the fuzzy rule base. Our approach is evaluated on the recognition of turn maneu- vers. Experiments on real data from a test vehicle demonstrate the feasibility of the proposed method.},
author = {Hulnhagen, Till and Dengler, Ingo and Tamke, Andreas and Dang, Thao and Breuel, Gabi},
booktitle = {2010 IEEE Intelligent Vehicles Symposium},
doi = {10.1109/IVS.2010.5548066},
isbn = {978-1-4244-7866-8},
month = {6},
pages = {65--70},
publisher = {IEEE},
title = {{Maneuver recognition using probabilistic finite-state machines and fuzzy logic}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5548066},
year = {2010}
}
@incollection{Verdu2010,
abstract = {The success of new learning systems depends highly on their ability to adapt to the characteristics and needs of each student. QUESTOURnament is a competitive e-learning tool, which is being re-designed in order to turn it into an adaptive e-learning system, managing different contests adapted to the progress of the students. In this adaptation process, the first step is to design a mechanism that objectively estimates the difficulty level of the challenges proposed in this environment. The present paper describes the designed method, which uses a genetic algorithm in order to discover the characteristics of the answers to the questions corresponding to the different difficulty levels. The fitness function, which evaluates the quality of the different potential solutions, as well as other operators of the genetic algorithm are described. Finally, an experiment with a real data set is presented in order to show the performance of this approach. {\textcopyright} 2010 Springer-Verlag.},
address = {Berlin, Heidelberg},
author = {Valiente-Rocha, Pablo A. and Lozano-Tello, Adolfo},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-13022-9_66},
editor = {Garc{\'{i}}a-Pedrajas, Nicol{\'{a}}s and Herrera, Francisco and Fyfe, Colin and Ben{\'{i}}tez, Jos{\'{e}} Manuel and Ali, Moonis},
isbn = {978-3-642-13021-2},
issn = {03029743},
keywords = {Adaptive e-learning,competitive learning,expert system,fitness function,genetic algorithms,homeautomation,intelligent tutoring systems,ontology},
number = {PART 1},
pages = {661--670},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Ontology-Based Expert System for Home Automation Controlling}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-79551563646{\&}partnerID=tZOtx3y1 http://link.springer.com/10.1007/978-3-642-13022-9 http://link.springer.com/10.1007/978-3-642-13022-9{\_}66},
volume = {6096},
year = {2010}
}
@inproceedings{Fenz2009,
abstract = {Bayesian networks are indispensable for determining the probability of events which are influenced by various components. Bayesian probabilities encode degrees of belief about certain events and a dynamic knowledge body is used to strengthen, update, or weaken these assumptions. The creation of Bayesian networks requires at least three challenging tasks: (i) the determination of relevant influence factors, (ii) the determination of relationships between the identified influence factors, and (iii) the calculation of the conditional probability tables for each node in the Bayesian network.Based on existing domain ontologies, we propose a method for the ontology-based generation of Bayesian networks. The ontology is used to provide the necessary knowledge about relevant influence factors, their relationships, their weights, and the scale which represents potential states of the identified influence factors.The developed method enables, based on existing ontologies, the semi-automatic generation and alternation of Bayesian networks.},
annote = {not automatic generation. Needs to be done by an expert.},
author = {Fenz, Stefan and Tjoa, A. Min and Hudec, Marcus},
booktitle = {2009 International Conference on Complex, Intelligent and Software Intensive Systems},
doi = {10.1109/CISIS.2009.33},
isbn = {978-1-4244-3569-2},
keywords = {basenetwork,ontology,semi-automatic},
month = {3},
number = {i},
pages = {712--717},
publisher = {IEEE},
title = {{Ontology-Based Generation of Bayesian Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5066866},
year = {2009}
}
@article{Melorose2015,
address = {Cambridge},
archivePrefix = {arXiv},
author = {Cochran, C. D. and Hale, W. Daniel and Hissam, Christine P.},
doi = {10.1080/00223980.1984.9923667},
editor = {{Intergovernmental Panel on Climate Change}},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {0022-3980},
journal = {The Journal of Psychology},
keywords = {icle,indoor vs outdoor,multi person,proxemics,space requirements},
month = {5},
number = {1},
pages = {121--123},
publisher = {Cambridge University Press},
title = {{Personal Space Requirements in Indoor Versus Outdoor Locations}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107415324A009 http://www.tandfonline.com/doi/abs/10.1080/00223980.1984.9923667},
volume = {117},
year = {1984}
}
@inproceedings{Gunes2008,
abstract = {Head detection in images and videos plays an important role in a wide range of computer vision and multimedia applications. In this paper, we propose a new head detection algorithm that is capable of handling significantly variable conditions in terms of viewpoint (i.e. frontal, profile, back view, from -180 degrees to +180 degrees), tilt angle (i.e. from horizontal to aerial), scale and resolution. To this aim, we built a new model for the head based on appearance distributions and shape constraints. The appearance distribution models the colors of hair and skin by sets of Gaussian mixtures in the XYZ and HSV color spaces. The shape constraint fits an elliptical model to the candidate region and compares its parameters with priors based on the human anatomy. This presents a pixel-level measurement of accuracy for the proposed algorithm both prior and after applying the spatial constraints referenced by the elliptical model. The excellent accuracy at both levels confirms the accuracy of the appearance model and the appropriateness of the spatial and topological process.},
author = {Gunes, Hatice and Piccardi, Massimo},
booktitle = {2008 15th IEEE International Conference on Image Processing},
doi = {10.1109/ICIP.2008.4712087},
isbn = {978-1-4244-1765-0},
keywords = {Head detection,hair color,person tracking,shape constraints,skin color},
pages = {1644--1647},
publisher = {Ieee},
title = {{An accurate algorithm for head detection based on XYZ and HSV hair and skin color models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4712087},
year = {2008}
}
@article{Zhang2017,
abstract = {In this paper, we study the problem of addressee and response selection in multi-party conversations. Understanding multi-party conversations is challenging because of complex speaker interactions: multiple speakers exchange messages with each other, playing different roles (sender, addressee, observer), and these roles vary across turns. To tackle this challenge, we propose the Speaker Interaction Recurrent Neural Network (SI-RNN). Whereas the previous state-of-the-art system updated speaker embeddings only for the sender, SI-RNN uses a novel dialog encoder to update speaker embeddings in a role-sensitive way. Additionally, unlike the previous work that selected the addressee and response separately, SI-RNN selects them jointly by viewing the task as a sequence prediction problem. Experimental results show that SI-RNN significantly improves the accuracy of addressee and response selection, particularly in complex conversations with many speakers and responses to distant messages many turns in the past.},
archivePrefix = {arXiv},
author = {Zhang, Rui and Lee, Honglak and Polymenakos, Lazaros and Radev, Dragomir},
eprint = {1709.04005},
keywords = {addressee,chat,conversational roles,speaker},
month = {9},
number = {2016},
title = {{Addressee and Response Selection in Multi-Party Conversations with Speaker Interaction RNNs}},
url = {http://arxiv.org/abs/1709.04005},
year = {2017}
}
@article{Cafaro2016,
author = {Cafaro, Angelo and Ravenet, Brian and Almsson, Vilhj},
journal = {ACM Transactions on Interactive Intelligent Systems (TiiS)},
keywords = {hhi,multi person,proxemics},
number = {2},
title = {{The Effects of Interpersonal Attitude of a Group of Agents on User's Presence and Proxemics Behavior}},
volume = {6},
year = {2016}
}
@article{Manzalini2006,
abstract = {The complexity of modern networks raises several challenges in the design and development of communication services. The unbearable costs in configuration and management call for autonomic approaches, in which services are able to self- configure and self-adapt their activities without human intervention. The need for ubiquity of service provisioning calls for the capability of services of adapting their behavior depending on the current situation in which they are used. In this paper, after having discussed the need for innovative approaches facilitating the development and execution of autonomic and situation-aware services, we analyze the key features that should underly such a general approach, propose an architecture centered around the abstraction of “autonomic communication elements”, and sketch the main research thrusts to be pursued for the realization of the vision.},
author = {Manzalini, Antonio and Zambonelli, Franco},
isbn = {076952589X},
number = {1},
pages = {2--7},
title = {{Towards Autonomic and Situation-Aware Communication Services: the CASCADAS Vision}},
year = {2006}
}
@inproceedings{Traum2002,
abstract = {Immersive virtual worlds are increasingly being used for education, training, and entertainment, and virtual humans that can interact with human users in these worlds play many important roles. However, current computational models of dialogue do not address the issues that arise with face-to-face communication situated in three-dimensional worlds, such as the proximity and attentional focus of others, the ability to maintain multi-party conversations, and the interplay between speech and nonverbal signals. This paper presents a new model that integrates and extends prior work on spoken dialogue and embodied conversational agents, and describes an initial implementation that has been applied to training in virtual reality.},
address = {New York, New York, USA},
author = {Traum, David and Rickel, Jeff},
booktitle = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 2 - AAMAS '02},
doi = {10.1145/544862.544922},
isbn = {1581134800},
keywords = {dialog,multimodal,multiparty},
pages = {766},
publisher = {ACM Press},
title = {{Embodied agents for multi-party dialogue in immersive virtual worlds}},
url = {http://portal.acm.org/citation.cfm?id=544922{\&}dl= http://portal.acm.org/citation.cfm?doid=544862.544922},
volume = {21},
year = {2002}
}
@book{eulero1768,
author = {Euler, L},
isbn = {9781104266776},
publisher = {Kessinger Publishing},
series = {Institutionum Calculi Integralis},
title = {{Institutionum Calculi Integralis V1 (1768)}},
url = {http://books.google.de/books?id=LXJUPgAACAAJ},
year = {1768}
}
@article{Vascon2016,
abstract = {Detecting groups is becoming of relevant interest as an important step for scene (and especially activity) understanding. Differently from what is commonly assumed in the computer vision community, different types of groups do exist, and among these, standing conversational groups (a.k.a. F-formations) play an important role. An F-formation is a common type of people aggregation occurring when two or more persons sustain a social interaction, such as a chat at a cocktail party. Indeed, detecting and subsequently classifying such an interaction in images or videos is of considerable importance in many applicative contexts, like surveillance, social signal processing, social robotics or activity classification, to name a few. This paper presents a principled method to approach to this problem grounded upon the socio-psychological concept of an F-formation. More specifically, a game-theoretic framework is proposed, aimed at modeling the spatial structure characterizing F-formations. In other words, since F-formations are subject to geometrical configurations on how humans have to be mutually located and oriented, the proposed solution is able to account for these constraints while also statistically modeling the uncertainty associated with the position and orientation of the engaged persons. Moreover, taking advantage of video data, it is also able to integrate temporal information over multiple frames utilizing the recent notions from multi-payoff evolutionary game theory. The experiments have been performed on several benchmark datasets, consistently showing the superiority of the proposed approach over the state of the art, and its robustness under severe noise conditions.},
author = {Vascon, Sebastiano and Mequanint, Eyasu Z. and Cristani, Marco and Hung, Hayley and Pelillo, Marcello and Murino, Vittorio},
doi = {10.1016/j.cviu.2015.09.012},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Conversational groups,F-formation detection,Game-theory,Group detection,Scene understanding,conversational groups,f-formations,group detection,vision},
month = {2},
pages = {11--24},
title = {{Detecting Conversational Groups in Images and Sequences: A Robust Game-Theoretic Approach}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314215002076 https://linkinghub.elsevier.com/retrieve/pii/S1077314215002076},
volume = {143},
year = {2016}
}
@inproceedings{Zanotto2012,
author = {Zanotto, Matteo and Bazzani, Loris and Cristani, Marco and Murino, Vittorio},
booktitle = {BMVC 2012},
doi = {http://dx.doi.org/10.5244/C.26.111},
isbn = {1-901725-46-4},
keywords = {bayesian,computer vision,droup,tracking},
pages = {111.1----111.12},
title = {{Online Bayesian Nonparametrics for Group Detection}},
year = {2012}
}
@book{Endsley2003,
annote = {Situation Awarenes (SA)
* Def: “the perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future” (Endsley, 1988)
* Bewusstsein dar{\"{u}}ber, was um einen herum passiert und Verst{\"{a}}ndnis was diese Information Nun und in Zukunft bedeutet.
* SA ist verkn{\"{u}}pft mit einem Ziel (z.B. Auto fahren, Operieren)
** Informationen die relevant f{\"{u}}r die aktuelle Aufgabe sind, sind auch relevant f{\"{u}}r SA
* Typischer weise enth{\"{a}}lt eine Situation mehrere Ziele mit sich {\"{a}}ndernden Priorit{\"{a}}ten gleichzeitig.
** SA hilft zu entscheiden welches Ziel aktuell zu bearbeiten ist.
* Grundlage f{\"{u}}r Entscheidungsfindung und Performanz "SA is the real-world changing knowledge that is critical for effective decision making and action."
* Drei Level von SA
1. Perzeption der Elemente der Umgebung
** Wahrnehmung der Umgebung
** Kommunikation mit anderen
** Anzeigen
** Zuverl{\"{a}}ssigkeit der Information
** Manchmal sind Informationen nicht zug{\"{a}}nglich oder bewusst von anderen verf{\"{a}}lscht-
2. Verst{\"{a}}ndnis der aktuellen Situation
** Verstehen was wahrgenommene Hinweise und Daten f{\"{u}}r die Relevanten Ziele und Aufgaben bedeuten
** Bei unge{\"{u}}bten Situationen k{\"{o}}nnen relevante Informationen {\"{u}}bersehen werden.
3. Projektion in einen zuk{\"{u}}nftigen Zustand
** Vorhersagen wie die relevanten Informationen sich in  naher Zukunft ver{\"{a}}ndern werden.
** Ben{\"{o}}tigt ein gutes Verst{\"{a}}ndnis f{\"{u}}r die Situation {\&} die dahinter liegende Dynamik des Systems.
** Projektion in die Zukunft erm{\"{o}}glicht es schnell und ad{\"{a}}quat auf Ereignisse zu reagieren und unerw{\"{u}}nschte zu umgehen.
* Zeit: 
** Wie viel Zeit ist f{\"{u}}r eine Entscheidungsfindung verf{\"{u}}gbar? Beeinflusst die Auswahl der verarbeiteten Informationen.
** Hohe Zeitliche Dynamik des Systems
** Erschwert Projektion: inakkurate SA
** Erzwingt viele Strategien gleichzeitig vor zu halten.
* Mentales Modell: Systematisches Verst{\"{a}}ndnis wie etwas funktioniert.
** Schl{\"{u}}ssel zu Level 2{\&}3 von SA
** H{\"{a}}lt memory-load niedrig
** F{\"{u}}llt Informationsl{\"{u}}cken durch Grundwerte
** Voll entwickeltes mentales Modell f{\"{u}}hrt zu:
** dynamische Aufmerksamkeitssteuerung zu kritischen Umgebungsinformationen
** Erwartungen an zuk{\"{u}}nftige Umgebungszust{\"{a}}nde
** Direkte Verbindung von Situations-Klassifikation und typischen (Re)aktionen f{\"{u}}r schnelle Entscheidungsfindung.
* Schemata
** Prototypische Zust{\"{a}}nde des Mentalen Modells
** Hohe SA durch simples Pattern-matching (Das hab ich schon mal gesehen.)
* Scripte 
** Prototypische L{\"{o}}sungsabl{\"{a}}ufe f{\"{u}}r bekannte Schemata
* Ziel vs. Daten getriebene SA
** Zielgetrieben: Informationen werden f{\"{u}}r erreichen des Ziels gefiltert und interpretiert
** Datengetrieben: Informationen tauchen auf. Auf ihrer Grundlage muss die Situation neu erfasst und ggf. das Ziel angepasst werden.
** Es muss m{\"{o}}glich sein zwischen Ziel und Daten getriebener Verarbeitung zu wechseln, wenn die Daten das erfordern.
* Erwartungen
** Steuern die Aufmerksamkeit und Interpretation der Daten: effizient
** Falsche Erwartungen k{\"{o}}nnen zu Missinterpretation f{\"{u}}hren.},
author = {Endsley, Mica and Jones, Debra},
doi = {10.1201/9780203485088},
edition = {2},
isbn = {978-0-7484-0966-2},
keywords = {situation awareness},
month = {7},
publisher = {CRC Press},
title = {{Designing for Situation Awareness}},
url = {http://www.crcnetbase.com/doi/book/10.1201/9780203485088},
year = {2003}
}
@article{Brdiczka2009a,
abstract = {This paper addresses learning and recognition of human behavior models from multimodal observation in a smart home environment. The proposed approach is part of a framework for acquiring a high-level contextual model for human behavior in an augmented environment. A 3-D video tracking system cre- ates and tracks entities (persons) in the scene. Further, a speech activity detector analyzes audio streams coming from head set microphones and determines for each entity, whether the entity speaks or not. An ambient sound detector detects noises in the environment. An individual role detector derives basic activity like “walking” or “interacting with table” from the extracted entity properties of the 3-D tracker. From the derived multimodal observations, different situations like “aperitif” or “presentation” are learned and detected using statistical models (HMMs). The objective of the proposed general framework is two-fold: the automatic offline analysis of human behavior recordings and the online detection of learned human behavior models. To evaluate the proposed approach, several multimodal recordings showing different situations have been conducted. The obtained results, in particular for offline analysis, are very good, showing that multimodality as well as multiperson observation generation are beneficial for situation recognition.},
author = {Brdiczka, Oliver and Langet, Matthieu and Maisonnasse, J{\'{e}}r{\^{o}}me and Crowley, J.L.},
doi = {10.1109/TASE.2008.2004965},
issn = {1545-5955},
journal = {IEEE Transactions on Automation Science and Engineering},
month = {10},
number = {4},
pages = {588--597},
title = {{Detecting Human Behavior Models From Multimodal Observation in a Smart Home}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4695941},
volume = {6},
year = {2009}
}
@inproceedings{VanSon2008,
abstract = {Research into spoken language has become more visual over the years. Both fundamental and applied research have progressively included gestures, gaze, and facial expression. Corpora of multi-modal conversational speech are rare and frequently difficult to use due to privacy and copyright restrictions. A freely available annotated corpus is presented, gratis and libre, of high quality video recordings of face-to-face conversational speech. Annotations include orthography, POS tags, and automatically generated phonemes transcriptions and word boundaries. In addition, labeling of both simple conversational function and gaze direction has been a performed. Within the bounds of the law, everything has been done to remove copyright and use restrictions. Annotations have been processed to RDBMS tables that allow SQL queries and direct connections to statistical software. From our experiences we would like to advocate the formulation of “best practises” for both legal handling and database storage of recordings and annotations.},
author = {van Son, Rob and Wesseling, Wieneke and Sanders, Eric and {van Den Heuvel}, Henk},
booktitle = {Proceedings of the Sixth International Conference on Language Resources and Evaluation},
isbn = {2-9517408-4-0},
keywords = {corpus,dialog,gaze,speech},
number = {1},
pages = {501--508},
title = {{The IFADV corpus: A free dialog corpus}},
volume = {2},
year = {2008}
}
@article{Althaus2004,
abstract = {One major design goal in human-robot interaction is that the robots behave in an intelligent manner, preferably in a similar way as humans. This constraint must also be taken into consideration when the navigation system for the platform is developed. However, research in human-robot interaction is often restricted to other components of the system including gestures, manipulation, and speech. On the other hand, research for mobile robot navigation focuses primarily on the task of reaching a certain goal point in an environment. We believe that these two problems can not be treated separately for a personal robot that coexists with humans in the same surrounding. Persons move constantly while they are interacting with each other. Hence, also a robot should do that, which poses constraints on the navigation system. This type of navigation is the focus of this paper. Methods have been developed for a robot to join a group of people engaged in a conversation. Preliminary results show that the platform's moving patterns are very similar to the ones of the persons. Moreover, this dynamic interaction has been judged naturally by the test subjects, which greatly increases the perceived intelligence of the robot.},
author = {Althaus, Philipp and Ishiguro, Hiroshi and Kanda, Takayuki and Miyashita, Takahiro and Christensen, Henrik I.},
doi = {10.1109/ROBOT.2004.1308100},
issn = {1050-4729},
journal = {International Conference on Robotics and Automation (ICRA)},
pages = {1894--1900},
title = {{Navigation for Human-Robot Interaction Tasks}},
year = {2004}
}
@inproceedings{Sheikhi,
abstract = {The paper investigates the problem of addressee recognition -to whom a speaker's utterance is intended- in a setting in- volving a humanoid robot interacting with multiple persons. More specifically, as it is well known that addressee can pri- marily be derived fromthe speaker's visual focus of attention (VFOA) defined as whom or what a person is looking at, we address the following questions: how much does the perfor- mance degrade when using automatically extracted VFOA from head pose instead of the VFOA ground-truth? Can the conversational context improve addressee recognition by using it either directly as a side cue in the addressee clas- sifier, or indirectly by improving the VFOA recognition, or in both ways? Finally, from a computational perspective, which VFOA features and normalizations are better and does it matter whether the VFOA recognition module only monitors whether a person looks at potential addressee tar- gets (the robot, people) or if it also considers objects of interest in the environment (paintings in our case) as addi- tional VFOA targets? Experiments on the public Vernissage database where the humanoid Nao robots make a quiz to two participants shows that reducing VFOA confusion (either through context, or by ignoring VFOA targets) improves addressee recognition.},
address = {New York, New York, USA},
author = {Sheikhi, Samira and {Babu Jayagopi}, Dinesh and Khalidov, Vasil and Odobez, Jean-Marc},
booktitle = {Proceedings of the 6th workshop on Eye gaze in intelligent human machine interaction: gaze in multimodal interaction - GazeIn '13},
doi = {10.1145/2535948.2535958},
isbn = {9781450325639},
keywords = {addressee,addressee estimation,all or part of,context,hu-,humavips,man robot interaction,or,or hard copies of,permission to make digital,this work for personal,vfoa,visual focus of attention},
pages = {1--6},
publisher = {ACM Press},
title = {{Context aware addressee estimation for human robot interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2535948.2535958},
year = {2013}
}
@article{Jovanovic2006,
abstract = {We present results on addressee identification in four-participants face-to-face meetings using Bayesian Network and Naive Bayes classifiers. First, we investigate how well the addressee of a dialogue act can be predicted based on gaze, utterance and conversational context features. Then, we explore whether information about meeting context can aid classifiers' performances. Both classifiers perform the best when conversational context and utterance features are combined with speaker's gaze information. The classifiers show little gain from information about meeting context.},
author = {Jovanovic, Natasa and op den Akker, Rieks and Nijholt, Anton},
journal = {Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
pages = {169--176},
title = {{Addressee Identification in Face-to-Face Meetings}},
url = {http://www.aclweb.org/anthology/E/E06/E06-1022.pdf},
year = {2006}
}
@inproceedings{Sardar2012,
abstract = {Many communities are realizing that even benign social disorder can lead to hostile neighborhoods, fear among law-abiding citizens and an increased likelihood of crime. Efforts to rid neighborhoods of incivility with legislation and ordinances that address vagrancy, drunkenness and loitering are discussed.},
address = {New York, NY, USA},
author = {Sardar, Aziez and Joosse, Michiel and Weiss, Astrid and Evers, Vanessa},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1145/2157689.2157769},
isbn = {9781450310635},
number = {January},
pages = {229},
publisher = {ACM Press},
title = {{Don't Stand so Close to Me: Users' Attitudinal and Behavioral Responses to Personal Space Invasion by Robots}},
url = {http://dl.acm.org/citation.cfm?doid=3119881.3119887 http://dl.acm.org/citation.cfm?doid=2157689.2157769},
year = {2012}
}
@book{Solomon2011,
author = {Solomon, C and Breckon, T},
isbn = {9781119957003},
publisher = {Wiley},
title = {{Fundamentals of Digital Image Processing: A Practical Approach with Examples in Matlab}},
url = {http://books.google.de/books?id=NoJ15jLdy7YC},
year = {2011}
}
@inproceedings{Kuzuoka2010,
abstract = {—An information-presenting robot is expected to establish an appropriate spatial relationship with people. Drawing upon sociological studies of spatial relationships involving " F-formation " and " body torque, " we examined the effect of a robot rotating its body on the reconfiguration of the F-formation arrangement. The results showed that a robot can change the position of a visitor by rotating its body. We also confirmed that to reconfigure the F-formation arrangement, it is more effective to rotate the whole body of the robot than only its head.},
address = {New York, New York, USA},
author = {Kuzuoka, Hideaki and Suzuki, Yuya and Yamashita, Jun and Yamazaki, Keiichi},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1145/1734454.1734557},
isbn = {9781424448937},
keywords = {communication robot,f-formation,interaction},
pages = {285},
publisher = {ACM Press},
title = {{Reconfiguring Spatial Formation Arrangement by Robot Body Orientation}},
url = {http://portal.acm.org/citation.cfm?doid=1734454.1734557},
year = {2010}
}
@inproceedings{Johansson2014,
address = {New York, New York, USA},
author = {Johansson, Martin and Skantze, Gabriel and Gustafson, Joakim},
booktitle = {Proceedings of the 2014 workshop on Understanding and Modeling Multiparty, Multimodal Interactions - UM3I '14},
doi = {10.1145/2666242.2666249},
isbn = {9781450306522},
keywords = {dialog,multiparty human-robot dialogue,situated dialogue,turn taking,turn-taking},
pages = {21--26},
publisher = {ACM Press},
title = {{Comparison of Human-Human and Human-Robot Turn-Taking Behaviour in Multiparty Situated Interaction}},
url = {http://dl.acm.org/citation.cfm?doid=2666242.2666249},
year = {2014}
}
@article{Kokar2009,
abstract = {The notions of "situation" and "situation awareness" have been formulated by many authors in various contexts. In this paper, we present a formalization of situations that is compatible with the interpretation of situation awareness in terms of human awareness as well as the situation theory of Barwise and Devlin. The purpose of this paper is to capture the situation theory of Barwise in terms of an OWL ontology. This allows one to express situations in a commonly supported language with computer processable semantics. The paper provides a description of the classes and the properties in the ontology, and illustrates the formalization with some simple examples. ?? 2007 Elsevier B.V. All rights reserved.},
author = {Kokar, Mieczyslaw M. and Matheus, Christopher J. and Baclawski, Kenneth},
doi = {10.1016/j.inffus.2007.01.004},
isbn = {9780769550060},
issn = {15662535},
journal = {Information Fusion},
keywords = {Formalization,Ontology,Situation,Situation awareness,Situation theory,reading},
month = {1},
number = {1},
pages = {83--98},
publisher = {Elsevier B.V.},
title = {{Ontology-based situation awareness}},
url = {http://dx.doi.org/10.1016/j.inffus.2007.01.004 http://linkinghub.elsevier.com/retrieve/pii/S1566253507000218},
volume = {10},
year = {2009}
}
@article{Fraune2019,
author = {Fraune, Marlena R. and {\v{S}}abanovi{\'{c}}, Selma and Kanda, Takayuki},
doi = {10.3389/frobt.2019.00048},
issn = {2296-9144},
journal = {Frontiers in Robotics and AI},
keywords = {entitativity,gender,group dynamics,group norms,human-robot interaction,social robotics},
month = {6},
number = {June},
pages = {1--16},
title = {{Human Group Presence, Group Characteristics, and Group Norms Affect Human-Robot Interaction in Naturalistic Settings}},
url = {https://www.frontiersin.org/article/10.3389/frobt.2019.00048/full},
volume = {6},
year = {2019}
}
@inproceedings{Zhang2016,
abstract = {In this paper, we present the first attempt to analyse dif-fering levels of social involvement in free standing convers-ing groups (or the so-called F-formations) from static im-ages. In addition, we enrich state-of-the-art F-formation modelling by learning a frustum of attention that accounts for the spatial context. That is, F-formation configurations vary with respect to the arrangement of furniture and the non-uniform crowdedness in the space during mingling sce-narios. The majority of prior works have considered the la-belling of conversing group as an objective task, requiring only a single annotator. However, we show that by embrac-ing the subjectivity of social involvement, we not only gen-erate a richer model of the social interactions in a scene but also significantly improve F-formation detection. We carry out extensive experimental validation of our proposed ap-proach by collecting a novel set of multi-annotator labels of involvement on the publicly available Idiap Poster Data; the only multi-annotator labelled database of free standing conversing groups that is currently available.},
author = {Zhang, Lu and Hung, Hayley},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2016.123},
isbn = {978-1-4673-8851-1},
keywords = {F-formations,groups},
month = {6},
pages = {1086--1095},
publisher = {IEEE},
title = {{Beyond F-Formations: Determining Social Involvement in Free Standing Conversing Groups from Static Images}},
url = {http://ieeexplore.ieee.org/document/7780492/},
year = {2016}
}
@article{Rafferty2017,
abstract = {The global population is aging; projections show that by 2050, more than 20{\%} of the population will be aged over 64. This will lead to an increase in aging related illness, a decrease in informal support, and ultimately issues with providing care for these individuals. Assistive smart homes provide a promising solu- tion to some of these issues. Nevertheless, they currently have issues hindering their adoption. To help address some of these issues, this study introduces a novel approach to implementing assistive smart homes. The devised approach is based upon an intention recogni- tion mechanism incorporated into an intelligent agent architecture. This approach is detailedandevaluated.Evaluationwasperformed across three scenarios. Scenario 1 involved awebinterface, focusing on testing the intention recognition mechanism. Scenarios 2 and 3 involved retrofitting a home with sensors and providing assistance with activities over a period of 3 months. The average accuracy for these three scenarios was 100{\%}, 64.4{\%}, and 83.3{\%}, respec- tively. Future will extend and further evaluate this approach by implementing advanced sensor-filtering rules and evaluating more complex activities.},
author = {Rafferty, Joseph and Nugent, Chris D and Liu, Jun and Chen, Liming},
doi = {10.1109/THMS.2016.2641388},
issn = {2168-2291},
journal = {IEEE Transactions on Human-Machine Systems},
keywords = {aal,activity recognition,intention,smart home},
pages = {1--12},
title = {{From Activity Recognition to Intention Recognition for Assisted Living Within Smart Homes}},
url = {http://ieeexplore.ieee.org/document/7807210/},
year = {2017}
}
@phdthesis{Gilman2015,
abstract = {Ubiquitous computing transforms physical environments into smart spaces, supporting users in an unobtrusive fashion. Such support requires sensing and interpreting the situation of the user, and providing the required functionality utilizing resources available. In other words, context acquisition, context modelling, and context reasoning are required. This thesis explores rule-based context reasoning from three perspectives: to implement the functionality of ubiquitous applications, to support the creation of ubiquitous applications, and to achieve self-adaptation. First, implementing functionality with reasoning is studied by comparing an application equipped with rule-based reasoning with an application providing similar functionality with hard coded application logic. The scalability of rule-based reasoning is studied with a large-scale student assistant scenario. Reasoning with constrained resources is explored with an application that performs reasoning partially on mobile devices. Finally, distributing a reasoning component that supports smart space interaction is explored with centralized, hybrid, and distributed architectures. Second, the creation of applications with rule-based reasoning is explored. In the first study, rules support building applications from available services and resources based on the instructions that users give via physical user interfaces. The second study supports developers, by proposing middleware that dynamically selects services and data based on the rules written by application developers. Third, self-adaptation is explored with a conceptual framework that adds self-introspective monitoring and control to smart space applications. This framework is verified with simulation and theoretical studies, and an application that fuses diverse data to provide fuel-efficient driving recommendations and adapts decision-making based on the driver's progress and feedback. The thesis' contributions include demonstrative cases on using rule-based reasoning from different perspectives, different scales, and with different architectures. Frameworks, a middleware, simulations, and prototypes provide the concrete contribution of the thesis. Generally, the thesis contributes to understanding how rule-based reasoning can be used in ubiquitous computing. The results presented can be used as guidelines for developers of ubiquitous applications},
author = {Gilman, Ekaterina},
isbn = {9789526209579},
keywords = {context,context reasoning,context-awareness,jokapaikan tietotekniikka,p{\"{a}}{\"{a}}ttely tilannetiedosta,reasoning,rule-based reasoning,scalability,s{\"{a}}{\"{a}}nt{\"{o}}pohjainen p{\"{a}}{\"{a}}ttely,tilannetietoisuus,ubiquitous computing},
school = {University of Oulu},
title = {{Exploring the use of rule-based reasoning in ubiquitous computing applications}},
year = {2015}
}
@article{Bhargava2017,
abstract = {We present an integrated framework for simultaneous tracking, group detection and multi-level activity recognition in crowd videos. Instead of solving these problems independently and sequentially, we solve them together in a unified framework to utilize the strong correlation that exists among individual motion, groups, and activities. We explore the hierarchical structure hidden in the video that connects individuals over time to produce tracks, connects individuals to form groups and also connects groups together to form a crowd. We show that estimation of this hidden structure corresponds to track association and group detection. We estimate this hidden structure under a linear programming formulation. The obtained graphical representation is further explored to recognize the node values that corresponds to multi-level activity recognition. This problem is solved under a structured SVM framework. The results on publicly available dataset show very competitive performance at all levels of granularity with the state-of-the-art batch processing methods despite the proposed technique being an online (causal) one.},
archivePrefix = {arXiv},
author = {Bhargava, Neha and Chaudhuri, Subhasis},
eprint = {1710.11087},
keywords = {activity,group detection,tracking},
month = {10},
title = {{An Integrated Approach to Crowd Video Analysis: From Tracking to Multi-level Activity Recognition}},
url = {http://arxiv.org/abs/1710.11087},
year = {2017}
}
@book{Liggins2008,
annote = {"template model is appropriate for problem domains in which the relevant variables, their state spaces, and their probabilistic relationships do not vary from problem instance to problem instance"

"Even when a domain can be represented by a template model, its size and complexity may make it necessary to represent it implicitly as a collection of modular subunits [3]"

Network Fragments
* represent shared elements of a probabilistic knowledge base
* small set of random variables
* may represent only portion of cpd for variable given its prents
* Entyty structure/behaviour/relationships encoded as fragments
-{\textgreater} Knowledge base of Fragments provides building blocks for assembling situation specific BN

MEBN extends BN
* allow replication and combination
-{\textgreater} reason with variable numbers of entities
* implicit joint probability over object-level domain entities
* ontology for higher order probability
* object language and meta-language

Hierarchical Type Model
* most specific evident type
* can be refined using observations,situation,interest

Hierarchical Activities
* individual activity related to platoon activity related to campany activity

Clusters of observations trigger Suggestor
-{\textgreater} SSN situation specific network is constructed
-{\textgreater} inference about target hypothesis
-{\textgreater} decision nodes are evaluated

Suggestors
* Construction suggestors add hypothesis to model
* Revision suggestors change model
* reason about 
** entity esistence
** relationships among entities
** entity subtypes
** variable resolution
** dependency relation (modification,adaptation,replacement)},
author = {Liggins, Martin E. and Hall, David L. and Llinas, James},
edition = {2},
isbn = {9781420053081},
pages = {872},
title = {{Handbook of multisensor data fusion: theory and practice}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=7ZqiHyc-RhUC{\&}oi=fnd{\&}pg=PR13{\&}dq=Handbook+of+Multisensor+Data+Fusion+Theory+and+Practice{\&}ots=9KEV1RveAO{\&}sig=xmysT5AfJ2ix5AN9A03HFUKCiGU{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=7ZqiHyc-RhUC{\&}oi=fnd{\&}pg=PR13{\&}dq=Han},
year = {2008}
}
@phdthesis{Lupiana2015,
abstract = {As the number of devices increases, it becomes a challenge for the users to use them effectively. Interacting with these devices becomes difficult and more time consuming. This is more challenging when the majority of these devices are mobile. The users and their devices enter and leave different environments where different settings and computing needs may be required. To effectively use these devices in such environments means to constantly be aware of their whereabouts, functionalities and desirable working conditions. This is impractical and hence it is imperative to increase seamless interactions between the users and devices, and to make these devices less intrusive. To},
author = {Lupiana, Dennis},
title = {{A Knowledge-driven Distributed Architecture for Context-Aware Systems}},
year = {2015}
}
@inproceedings{Bauer2009,
abstract = {To survey large areas or buildings, it is necessary to install an extensive amount of sensors. In conventional surveillance systems, all signals from various sensors, such as cameras, microphones, RFID detectors and light barriers, have to be evaluated manually by human operators, which is cost-intensive and error-prone. Present approaches for a more automated surveillance mainly focus on the automated exploitation of a single sensor signal, enabling the system to notify the operator of primitive events. The most critical threats however, such as terrorism and industrial espionage, can only be detected and avoided, if information from multiple sensor signals is fused together. Therefore, an approach to represent the relevant information extracted from sensor signals, fused into a single comprehensive, dynamic model of the monitored area is presented. The proposed object-oriented world model (OOWM) is part of the semi-autonomous surveillance system NEST, developed at Fraunhofer IITB. The main focus of this research project is to migrate from a sensor-centered view to a task- and object- centered view, with the aim to focus the system on the application- relevant information. The OOWM is preconfigured with static prior knowledge, such as the spatial data of the area or building to be supervised. Dynamic information is collected by arbitrary signal exploitation algorithms (e. g. person tracking) and fused into a consistent representation inside the world model by means of Bayesian fusion. As an obvious benefit, the resulting object-oriented representation of the dynamic situation provides the operator with a tool to easily overview the situation, detect critical situations early and initiate appropriate countermeasures. Furthermore, the world model is a key component to enable an easy and seamless integration of new sensors and to develop high-level algorithms that are able to execute surveillance tasks autonomously.},
address = {Karlsruhe},
author = {Bauer, A. and Emter, T. and Vagts, H.},
booktitle = {Security Research Conference "Future Security"},
keywords = {nest,sensor fustion,surveillance,world model},
pages = {1--8},
title = {{Object oriented world model for surveillance systems}},
url = {http://www-alt.iosb.fraunhofer.de/servlet/is/30393/Paper{\_}FutureSecurity09{\_}WorldModel.pdf},
year = {2009}
}
@article{Tombari2010,
abstract = {This paper deals with local 3D descriptors for surface matching. First, we categorize existing methods into two classes: Signatures and Histograms. Then, by discussion and experiments alike, we point out the key issues of unique- ness and repeatability of the local reference frame. Based on these observations, we formulate a novel comprehensive proposal for surface representation, which encompasses a new unique and repeatable local reference frame as well as a new 3D descriptor. The latter lays at the intersection between Signatures and His- tograms, so as to possibly achieve a better balance between descriptiveness and robustness. Experiments on publicly available datasets as well as on range scans obtained with Spacetime Stereo provide a thorough validation of our proposal.},
author = {Tombari, Federico and Salti, Samuele and Stefano, Luigi Di},
journal = {ECCV'10 Proceedings of the 11th European conference on computer vision conference on Computer vision: Part III},
keywords = {SHOT},
pages = {356--369},
title = {{Unique Signatures of Histograms for Local Surface Description}},
year = {2010}
}
@inproceedings{Kafer2010a,
abstract = {The recognition and prediction of situations is an indispensable skill of future driver assistance systems. This study focuses on the recognition of situations involving two vehicles at intersections. For each vehicle, a set of possible future motion trajectories is estimated and rated based on a motion database for a time interval of 2–4 seconds ahead. Realistic situations are generated by a pairwise combination of these individual motion trajectories and classified according to nine categories with a polynomial classifier. In the proposed framework, situations are penalised for which the time to collision significantly exceeds the typical human reaction time. The correspondingly favoured situations are combined by a probabilistic framework, resulting in a more reliable situation recognition and collision detection than obtained based on inde- pendent motion hypotheses. The proposed method is evaluated on a real-world differential GPS data set acquired during a test drive of 10 km, including three road intersections. Our method is typically able to recognise the situation correctly about 1–2 seconds before the distance to the intersection centre becomes minimal.},
author = {K{\"{a}}fer, Eugen and Hermes, Christoph and W{\"{o}}hler, Christian and Ritter, Helge and Kummert, Franz},
booktitle = {2010 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2010.5509919},
isbn = {978-1-4244-5038-1},
month = {5},
pages = {3960--3965},
publisher = {IEEE},
title = {{Recognition of situation classes at road intersections}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5509919},
year = {2010}
}
@article{Goldfeder2007,
abstract = {Planning realizable and stable grasps on 3D objects is crucial for many robotics applications, but grasp planners often ignore the relative sizes of the robotic hand and the object being grasped or do not account for physical joint and positioning limitations. We present a grasp planner that can consider the full range of parameters of a real hand and an arbitrary object, including physical and material properties as well as environmental obstacles and forces, and produce an output grasp that can be immediately executed. We do this by decomposing a 3D model into a superquadric 'decomposition tree' which we use to prune the intractably large space of possible grasps into a subspace that is likely to contain many good grasps. This subspace can be sampled and evaluated in GraspIt!, our 3D grasping simulator, to find a set of highly stable grasps, all of which are physically realizable. We show grasp results on various models using a Barrett hand.},
author = {Goldfeder, Corey and Allen, Peter K and Lackner, Claire and Pelossof, Raphael},
isbn = {1424406021},
journal = {2007 IEEE International Conference on Robotics and Automation},
number = {April},
pages = {10--14},
title = {{Grasp Planning via Decomposition Trees}},
year = {2007}
}
@article{Sun2016,
abstract = {Beyond recognizing actions of individuals, activity group localization in videos aims to localize groups of persons in spatiotemporal spaces and recognize what activity the group performs. In this paper, we propose a latent graph model to simultaneously address the problem of multi-target tracking, group discovery and activity recognition. Our key insight is to exploit the contextual relations among people. We present them as a latent relational graph, which hierarchically encodes the association potentials between tracklets, intra-group interactions, correlations, and inter-group compatibilities. Our model is capable of propagating multiple evidences among different layers of the latent graph. Particularly, associated tracklets assist accurate group discovery, activity recognition can benefit from knowing the whole structured groups, and the group and activity information in turn provides strong cues for establishing coherent associations between tracklets. Experiments on five datasets demonstrate that our model achieves both significant improvements in activity group localization and competitive performance on activity recognition.},
author = {Sun, Lei and Ai, Haizhou and Lao, Shihong},
doi = {10.1016/j.cviu.2015.10.009},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {Activity recognition,Group activity,Latent graph model,activity recognition,group detection,vision},
month = {3},
pages = {144--154},
publisher = {Elsevier Inc.},
title = {{Localizing activity groups in videos}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314215002192},
volume = {144},
year = {2016}
}
@article{Li2015,
abstract = {The effects of physical embodiment and physical presence were explored through a survey of 33 experimental works comparing how people interacted with physical robots and virtual agents. A qualitative assessment of the direction of quantitative effects demonstrated that robots were more persuasive and perceived more positively when physically present in a users environment than when digitally-displayed on a screen either as a video feed of the same robot or as a virtual character analog; robots also led to better user performance when they were collocated as opposed to shown via video on a screen. However, participants did not respond differently to physical robots and virtual agents when both were displayed digitally on a screen - suggesting that physical presence, rather than physical embodiment, characterizes peoples responses to social robots. Implications for understanding psychological response to physical and virtual agents and for methodological design are discussed.},
archivePrefix = {arXiv},
author = {Li, Jamy},
doi = {10.1016/j.ijhcs.2015.01.001},
eprint = {arXiv:1011.1669v3},
isbn = {1071-5819},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {Embodiment,Human-robot interaction,Physical Virtual,Presence,Social robot,Virtual agent,embodiment,real,virtual},
month = {5},
pages = {23--37},
publisher = {Elsevier},
title = {{The benefit of being physically present: A survey of experimental works comparing copresent robots, telepresent robots and virtual agents}},
url = {http://dx.doi.org/10.1016/j.ijhcs.2015.01.001 http://linkinghub.elsevier.com/retrieve/pii/S107158191500004X https://linkinghub.elsevier.com/retrieve/pii/S107158191500004X},
volume = {77},
year = {2015}
}
@inproceedings{Brdiczka2005,
abstract = {This paper addresses the problem of detecting interaction groups in an intelligent environment. To understand human activity, we need to identify human actors and their interpersonal links. An interaction group can be seen as basic entity, within which individuals collaborate in order to achieve a common goal. In this regard, the dynamic change of interaction group configuration, i.e. the split and merge of interaction groups, can be seen as indicator of new activities. Our approach takes speech activity detection of individuals forming interaction groups as input. A classical HMM-based approach learning different HMM for the different group configurations did not produce promising results. We propose an approach for detecting interaction group configurations based on the assumption that conversational turn taking is synchronized inside groups. The proposed detector is based on one HMM constructed upon conversational hypotheses. The approach shows good results and thus confirms our conversational hypotheses.},
address = {New York, New York, USA},
author = {Brdiczka, Oliver and Maisonnasse, J{\'{e}}r{\^{o}}me and Reignier, Patrick},
booktitle = {Proceedings of the 7th international conference on Multimodal interfaces - ICMI '05},
doi = {10.1145/1088463.1088473},
isbn = {1595930280},
keywords = {clustering interaction groups,conversational analysis,detection,group detection,hidden markov model,intelligent environment,interaction,speech,ubiquitous computing},
pages = {32},
publisher = {ACM Press},
title = {{Automatic detection of interaction groups}},
url = {http://portal.acm.org/citation.cfm?doid=1088463.1088473},
year = {2005}
}
@article{Daly2011,
abstract = {Bayesian networks have become a widely used method in the modelling of uncertain knowledge. Owing to the difficulty domain experts have in specifying them, techniques that learn Bayesian networks from data have become indispensable. Recently, however, there have been many important new developments in this field. This work takes a broad look at the literature on learning Bayesian networks—in particular their structure—from data. Specific topics are not focused on in detail, but it is hoped that all the major fields in the area are covered. This article is not intended to be a tutorial—for this, there are many books on the topic, which will be presented. However, an effort has been made to locate all the relevant publications, so that this paper can be used as a ready reference to find the works on particular sub-topics.},
author = {Daly, R{\'{o}}n{\'{a}}n and Shen, Qiang and Aitken, Stuart},
doi = {10.1017/S0269888910000251},
issn = {0269-8889},
journal = {The Knowledge Engineering Review},
keywords = {Bayesian Network,learning},
month = {5},
number = {02},
pages = {99--157},
title = {{Learning Bayesian networks: approaches and issues}},
url = {http://www.journals.cambridge.org/abstract{\_}S0269888910000251},
volume = {26},
year = {2011}
}
@phdthesis{Ping2016,
abstract = {Probabilistic graphical models such as Markov random fields provide a powerful framework and tools for machine learning, especially for structured output learning. Latent variables naturally exist in many applications of these models; they may arise from partially labeled data, or be introduced to enrich model flexibility. However, the presence of latent variables presents challenges for learning and inference. For example, the standard approach of using maximum a posteriori (MAP) prediction is complicated by the fact that, in latent variable models (LVMs), we typically want to first marginalize out the latent variables, leading to an inference task called marginal MAP. Un- fortunately, marginal MAP prediction can be NP-hard even on relatively simple models such as trees, and few methods have been developed in the literature. This thesis presents a class of variational bounds for marginal MAP that generalizes the popular dual-decomposition method for MAP inference, and enables an efficient block coordinate descent algorithm to solve the corresponding optimization. Similarly, when learning LVMs for structured pre- diction, it is critically important to maintain the effect of uncertainty over latent variables by marginalization. We propose the marginal structured SVM, which uses marginal MAP inference to properly handle that uncertainty inside the framework of max-margin learning. We then turn our attention to an important subclass of latent variable models, restricted Boltzmann machines (RBMs). RBMs are two-layer latent variable models that are widely used to capture complex distributions of observed data, including as building block for deep probabilistic models. One practical problem in RBMs is model selection: we need to determine the hidden (latent) layer size before performing learning. We propose an infinite RBM model and apply the Frank-Wolfe algorithm to solve the resulting learning problem. The resulting algorithm can be interpreted as inserting a hidden variable into a RBM model at each iteration, to easily and efficiently perform model selection during learning. We also study the role of approximate inference in RBMs and conditional RBMs. In particular, there is a common assumption that belief propagation methods do not work well on RBM-based models, especially for learning. In contrast, we demonstrate that for conditional models and structured prediction, learning RBM-based models with belief propagation and its variants can provide much better results than the state-of-the-art contrastive divergence methods. xiii},
author = {Ping, Wei},
keywords = {graphical models},
school = {UNIVERSITY OF CALIFORNIA, IRVINE},
title = {{Learning and Inference in Latent Variable Graphical Models}},
year = {2016}
}
@article{Jegou2018,
abstract = {We propose a computational model that endows conversational agents with the capability to coordinate their speaking turns (turn-taking management) in the context of mixed-initiative two-party dialogs. In human conversations, participants are continuously adjusting their verbal and non-verbal productions for ensuring the effective coordination of speaking turns. In our model, the decision making is a continuous process based on the intrinsic current goal of the agent with respect to turn-taking, namely its motivation to keep-or to leave-its current role (speaker or listener), and on its perception of the intentions of its partner. Concurrently, the agent is also producing signals indicating its willingness to maintain or leave its current role. Our model is based on two models from cognitive psychology: the drift-diffusion model and the theory of behavioral dynamics. After presenting simulations showing how our model makes the coordination emerge from the interactions, we propose a SAIBA-Compliant architecture, named BeAware, created to support the implementation of our model. Finally, using our model, we investigate how an agent's turn-taking strategy may impact the user's experience and the effectiveness of the coordination.},
author = {J{\'{e}}gou, Mathieu and Chevaillier, Pierre},
doi = {10.1007/s12193-018-0265-3},
issn = {1783-7677},
journal = {Journal on Multimodal User Interfaces},
keywords = {Dynamical systems,Embodied conversational agents,Prosody,Turn-taking,dynamical systems,embodied conversational agents,prosody,turn-taking},
month = {9},
number = {3},
pages = {199--223},
publisher = {Springer International Publishing},
title = {{A computational model for the emergence of turn-taking behaviors in user-agent interactions}},
url = {https://doi.org/10.1007/s12193-018-0265-3 http://link.springer.com/10.1007/s12193-018-0265-3},
volume = {12},
year = {2018}
}
@article{Pohling2019,
author = {Pohling, Marian and Leichsenring, Christian and Hermann, Thomas},
doi = {10.3233/AIS-190533},
issn = {18761372},
journal = {Journal of Ambient Intelligence and Smart Environments},
month = {9},
number = {5},
pages = {373--401},
title = {{Base Cube One: A location-addressable service-oriented smart environment framework}},
url = {https://www.medra.org/servlet/aliasResolver?alias=iospress{\&}doi=10.3233/AIS-190533},
volume = {11},
year = {2019}
}
@incollection{Schmidt2005,
author = {Schmidt, Albrecht},
booktitle = {Ambient Intelligence},
pages = {159--178},
title = {{Interactive Context-Aware Systems Interacting with Ambient Intelligence}},
year = {2005}
}
@article{Kopp2010,
abstract = {Human natural face-to-face communication is characterized by inter-personal coordination. In this paper, phenomena are analyzed that yield coordination of behaviors, beliefs, and attitudes between interaction partners, which can be tied to a concept of establishing social resonance. It is discussed whether these mechanisms can and should be transferred to conversation with artificial interlocutors like ECAs or humanoid robots. It is argued that one major step in this direction is embodied coordination, mutual adaptations that are mediated by flexible modules for the top-down production and bottom-up perception of expressive conversational behavior that ground in and, crucially, coalesce in the same sensorimotor structures. Work on modeling this for ECAs with a focus on coverbal gestures is presented. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Kopp, Stefan},
doi = {10.1016/j.specom.2010.02.007},
isbn = {0167-6393},
issn = {01676393},
journal = {Speech Communication},
keywords = {Coordination,Embodied conversational agents,Gesture,Social Resonance,coordingation,interaction,social resonance},
month = {6},
number = {6},
pages = {587--597},
publisher = {Elsevier B.V.},
title = {{Social resonance and embodied coordination in face-to-face conversation with artificial interlocutors}},
url = {http://dx.doi.org/10.1016/j.specom.2010.02.007 http://linkinghub.elsevier.com/retrieve/pii/S0167639310000312},
volume = {52},
year = {2010}
}
@techreport{Andersen2012,
abstract = {This technical report describes our evaluation of the Kinect depth sensor by Microsoft for Computer Vision applications. The depth sensor is able to return images like an ordinary camera, but instead of color, each pixel value represents the distance to the point. As such, the sensor can be seen as a range- or 3D-camera. We have used the sensor in several different computer vision projects and this document collects our experiences with the sensor. We are only focusing on the depth sensing capabilities of the sensor since this is the real novelty of the product in relation to computer vision. The basic technique of the depth sensor is to emit an infrared light pattern (with an IR laser diode) and calculate depth from the reflection of the light at different positions (using a traditional IR sensitive camera). In this report, we perform an extensive evaluation of the depth sensor and investigate issues such as 3D resolution and precision, structural noise, multi-cam setups and transient response of the sensor. The purpose is to give the reader a well-founded background to choose whether or not the Kinect sensor is applicable to a specific problem.},
author = {Andersen, M.R. and Jensen, T. and Lisouski, P. and Mortensen, A.K. and Hansen, M.K. and Gregersen, T. and Ahrendt, P.},
title = {{Kinect Depth Sensor Evaluation for Computer Vision Applications}},
year = {2012}
}
@inproceedings{Hara2018,
abstract = {We address prediction of turn-taking considering related behaviors such as backchannels and fillers. Backchannels are used by the listeners to acknowledge that the current speaker can hold the turn. On the other hand, fillers are used by the prospective speakers to indicate a will to take a turn. We propose a turn-taking model based on multitask learning in conjunction with prediction of backchannels and fillers. The multitask learning of LSTM neural networks shared by these tasks allows for efficient and generalized learning, and thus improves prediction accuracy. Evaluations with two kinds of dialogue corpora of human-robot interaction demonstrate that the proposed multi-task learning scheme outperforms the conventional single-task learning.},
address = {ISCA},
author = {Hara, Kohei and Inoue, Koji and Takanashi, Katsuya and Kawahara, Tatsuya},
booktitle = {Interspeech 2018},
doi = {10.21437/Interspeech.2018-1442},
issn = {19909772},
keywords = {Backchannel,Filler,Multitask learning,Neural networks,Turn-taking},
month = {9},
number = {September},
pages = {991--995},
publisher = {ISCA},
title = {{Prediction of Turn-taking Using Multitask Learning with Prediction of Backchannels and Fillers}},
url = {http://www.isca-speech.org/archive/Interspeech{\_}2018/abstracts/1442.html},
volume = {2018-Septe},
year = {2018}
}
@inproceedings{Minh2018,
address = {California},
annote = {Vielleicht ein ander mal.},
author = {{Le Minh}, Thao and Shimizu, Nobuyuki and Miyazaki, Takashi and Shinoda, Koichi},
booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2018/214},
isbn = {9780999241127},
keywords = {Computer Vision: Language and Vision,Humans and AI: Human-Computer Interaction,Machine Learning: Deep Learning},
month = {7},
pages = {1546--1553},
publisher = {International Joint Conferences on Artificial Intelligence Organization},
title = {{Deep Learning Based Multi-modal Addressee Recognition in Visual Scenes with Utterances}},
url = {https://www.ijcai.org/proceedings/2018/214},
year = {2018}
}
@article{dlib09,
author = {King, Davis E.},
journal = {Journal of Machine Learning Research},
number = {10},
pages = {1755--1758},
title = {{Dlib-ml: A Machine Learning Toolkit}},
year = {2009}
}
@article{Hirmer2016,
author = {Hirmer, Pascal and Wieland, Matthias and Schwarz, Holger and Mitschang, Bernhard and Breitenb{\"{u}}cher, Uwe and S{\'{a}}ez, Santiago G{\'{o}}mez and Leymann, Frank},
doi = {10.1007/s00607-016-0522-9},
issn = {0010-485X},
journal = {Computing},
keywords = {Cloud computing,Context,Integration,IoT,Middleware,Situation recognition,Workflows,situation,templates},
month = {10},
publisher = {Springer Vienna},
title = {{Situation recognition and handling based on executing situation templates and situation-aware workflows}},
url = {http://link.springer.com/10.1007/s00607-016-0522-9},
year = {2016}
}
@inproceedings{iros_2014_topol,
abstract = {This paper presents a new approach for topo- logical localisation of service robots in dynamic indoor envi- ronments. In contrast to typical localisation approaches that rely mainly on static parts of the environment, our approach makes explicit use of information about changes by learning and modelling the spatio-temporal dynamics of the environment where the robot is acting. The proposed spatio-temporal world model is able to predict environmental changes in time, allowing the robot to improve its localisation capabilities during long- term operations in populated environments. To investigate the proposed approach, we have enabled a mobile robot to autonomously patrol a populated environment over a period of one week while building the proposed model representation. We demonstrate that the experience learned during one week is applicable for topological localization even after a hiatus of three months by showing that the localization error rate is significantly lower compared to static environment representa- tions.},
author = {Krajnik, Tomas and Fentanes, Jaime P. and Mozos, Oscar M. and Duckett, Tom and Ekekrantz, Johan and Hanheide, Marc},
booktitle = {2014 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2014.6943205},
isbn = {978-1-4799-6934-0},
keywords = {localisation,long term},
month = {9},
pages = {4537--4542},
publisher = {IEEE},
title = {{Long-term topological localisation for service robots in dynamic environments using spectral maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6943205},
year = {2014}
}
@article{Srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{Das2004,
abstract = {The number of probability distributions required to populate a conditional probability table (CPT) in a Bayesian network, grows exponentially with the number of parent-nodes associated with that table. If the table is to be populated through knowledge elicited from a domain expert then the sheer magnitude of the task forms a considerable cognitive barrier. In this paper we devise an algorithm to populate the CPT while easing the extent of knowledge acquisition. The input to the algorithm consists of a set of weights that quantify the relative strengths of the influences of the parent-nodes on the child-node, and a set of probability distributions the number of which grows only linearly with the number of associated parent-nodes. These are elicited from the domain expert. The set of probabilities are obtained by taking into consideration the heuristics that experts use while arriving at probabilistic estimations. The algorithm is used to populate the CPT by computing appropriate weighted sums of the elicited distributions. We invoke the methods of information geometry to demonstrate how these weighted sums capture the expert's judgemental strategy.},
archivePrefix = {arXiv},
author = {Das, Balaram},
eprint = {0411034},
keywords = {bayes networks,cpt},
pages = {1--24},
primaryClass = {cs},
title = {{Generating Conditional Probabilities for Bayesian Networks: Easing the Knowledge Acquisition Problem}},
url = {http://arxiv.org/abs/cs/0411034},
year = {2004}
}
@inproceedings{Dahlbom2009a,
abstract = {Research on information fusion and situation management within the military domain, is often focused on data-driven approaches for aiding decision makers in achieving situation awareness. We have in a companion paper identified situation recognition as an important topic for further studies on knowledge-driven approaches. When developing new algorithms it is of utmost importance to have data for studying the problem at hand (as well as for evaluation purposes). This often become a problem within the military domain as there is a high level of secrecy, resulting in a lack of data, and instead one often needs to resort to artificial data. Many tools and simulation environments can be used for constructing scenarios in virtual worlds. Most of these are however data-centered, that is, their purpose is to simulate the real-world as accurately as possible, in contrast to simulating complex scenarios. In high-level information fusion we can however often assume that lower-level problems have already been solved – thus the separation of abstraction – and we should instead focus on solving problems concerning complex relationships, i.e. situations and threats. In this paper we discuss requirements that research on situation recognition puts on simulation tools. Based on these requirements we present a component-based simulator for quickly adapting the simulation environment to the needs of the research problem},
author = {Dahlbom, Anders and Niklasson, Lars and Falkman, G{\"{o}}ran},
booktitle = {Intelligent Sensing, Situation Management, Impact Assessment, and Cyber-Sensing},
doi = {10.1117/12.818537},
editor = {Mott, Stephen and Buford, John F. and Jakobson, Gabe and Mendenhall, Michael J.},
keywords = {component-based architecture,simulation,situation assessment,situation recognition},
month = {5},
pages = {735206--735206--12},
title = {{A Component-based Simulator for Supporting Research on Situation Recognition}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=779703},
volume = {7352},
year = {2009}
}
@article{Baranauskas2015,
abstract = {This paper intends to precisiate the well-known and widespread definitions of both smart and intelligent agent (SA; IA), as well as the smart and intelligent multi-agent system (SS/II{\_}MAS). The use of a unified and standardized agent and multi-agent system description based on definitions of the general systems theory is delivered and proposed as well. The intellectics of multi-agent systems is considered as a kind of an extension of the agent intelligence. Three typical features of human intellectual activities are proposed to be implemented and simulated in an agent/multi-agent system as the basic paradigms for agent and multi-agent system intellectics. As underlined in the paper, operation according to those paradigms (recognition and classification, behavior according to a set of fuzzy rules, and operation according to some prescribed tendency) is solidly mathematically based (correspondingly: mathematical programing, fuzzy logic and stochastic approximation). Finally, results of computerized modeling and simulation are delivered demonstrating the practical vitality and efficiency of the theoretical approach to the realization of the intelligent environment of the Internet of Things and Services (IoT{\&}S) for user‘s comfort in two projects: “Research and Development of Internet Infrastructure for IoT{\&} S in the Smart Environment (IDAPI)” and “Research on Smart Home Environment and Development of Intelligent Technologies (BIATech)”.},
author = {Baranauskas, Rimantas and Janaviciute, Audrone and Jasinevicius, Raimundas and Jukavicius, Vaidas and Kazanavicius, Egidijus and Petrauskas, Vytautas and Vrubliauskas, Arunas},
doi = {10.5755/j01.itc.44.1.8768},
issn = {2335-884X},
journal = {Information Technology And Control},
keywords = {1,an agent-based approach to,applications,especially to those which,fuzzy systems,intelligent agents,intelligent environment,introduction,involve it-,multi-agent systems,related work and motivation,smart agents,various engineering},
number = {1},
pages = {112--121},
title = {{On Multi Agent Systems Intellectics}},
url = {http://www.itc.ktu.lt/index.php/ITC/article/view/8768},
volume = {44},
year = {2015}
}
@article{Yan2011,
abstract = {Person tracking is an important topic in ambient living systems as well as in computer vision. In particular, detecting a person from a ceiling-mounted camera is a challenge since the person's appearance is very different from the top or from the side view, and the shape of the person changes significantly when moving around the room. This article presents a novel approach for a real-time person tracking system based on particle filters with input from different visual streams.Anewarchitecture is developed that integrates different vision streams by means of a Sigma-Pi-like network. Moreover, a short-term memory mechanism is modeled to enhance the robustness of the tracking system. Based on this architecture, the system can start localizing a person with several cues and learn the features of other cues online. The experimental results show that robust real-time person tracking can be achieved.},
author = {Yan, Wenjie and Weber, Cornelius and Wermter, Stefan},
doi = {10.3233/AIS-2011-0111},
journal = {Journal of Ambient Intelligence and Smart Environments},
keywords = {neural network,particle filter,person detection,person recognition,person tracking},
pages = {237--252},
title = {{A hybrid probabilistic neural model for person tracking based on a ceiling-mounted camera}},
volume = {3},
year = {2011}
}
@inproceedings{Lopatovska2018,
abstract = {The conversational nature of intelligent personal assistants (IPAs) has the potential to trigger personification tendencies in users, which in turn can translate into consumer loyalty and satisfaction. We conducted a study of Amazon Alexa usage and explored the manifestations and possible correlates of users' personification of Alexa. The data were collected via diary instrument from nineteen Alexa users over four days. Less than half of the participants reported personification behaviors. Most of the personification reports can be characterized as mindless politeness (saying 'thank you' and 'please' to Alexa). Two participants expressed deeper personification by confessing their love and reprimanding Alexa. A new study is underway to understand whether expressions of personifications are caused by users' emotional attachments or skepticism about technology's intelligence.},
address = {New York, New York, USA},
author = {Lopatovska, Irene and Williams, Harriet},
booktitle = {Conference on Human Information Interaction {\&} Retrieval (CHIIR)},
doi = {10.1145/3176349.3176868},
isbn = {9781450349253},
keywords = {conversational agent,digital personal assistants,intelligent personal assistants,voice-powered personal assistants},
pages = {265--268},
publisher = {ACM Press},
title = {{Personification of the Amazon Alexa}},
url = {http://dl.acm.org/citation.cfm?doid=3176349.3176868},
year = {2018}
}
@inproceedings{Surie2013,
abstract = {Spatial relationships or proxemics play an important role in how humans interact with other people and objects in an environment, yet spatial relationships are not thoroughly exploited within smart environments. Objects designed to be aware of its proxemics facilitate implicit and explicit interaction with humans. Kitchen As-A-Pal is an interactive smart kitchen that provides an infrastructure for sensing and modeling proxemics among objects and human using a sonar network and RFID technology. Position, movement, identity and location are the proxemics dimensions explored in kitchen As-A-Pal. A pilot study of a breakfast scenario comprising of 9 everyday activities in Kitchen As-A-Pal using 2 subjects yielded promising proximity tracking results with a precision of 100{\%} and a recall of 68.3{\%} for spatial zones with high and medium average time percentages (ATP). Also, 53.21{\%} ATP has more than 95{\%} recall values.},
author = {Surie, Dipak and Baydan, Berker and Lindgren, Helena},
booktitle = {2013 9th International Conference on Intelligent Environments},
doi = {10.1109/IE.2013.43},
isbn = {978-0-7695-5038-1},
keywords = {Proxemics awareness,proxemics,proximity tracking,smart environments,smart home,smart objects,spatial modeling},
month = {7},
pages = {157--164},
publisher = {IEEE},
title = {{Proxemics Awareness in Kitchen As-a-Pal: Tracking Objects and Human in Perspective}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6597805},
year = {2013}
}
@incollection{Brock_intelligent-object-exploration_2012,
annote = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/InTech-Intelligent{\_}object{\_}exploration{\_}2012.pdf},
author = {{Robert Gaschler Dov Katz}, Martin Grund Peter A Frensch and Brock, Oliver},
booktitle = {In: Human Machine Interaction-Getting Closer, Edited by Maurtua Inaki},
chapter = {12},
doi = {10.5772/25836},
editor = {Inaki, Maurtua},
isbn = {978-953-307-890-8},
month = {1},
pages = {236--260},
publisher = {InTech},
title = {{Intelligent Object Exploration}},
url = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/InTech-Intelligent{\_}object{\_}exploration{\_}2012.pdf},
year = {2012}
}
@inproceedings{Vanus2013,
abstract = {This article is aimed to describe the method of testing the implementation of voice control over operating and technical functions of Smart Home Come. Custom control over operating and technical functions was implemented into a model of Smart Home that was equipped with KNX technology. A sociological survey focused on the needs of seniors has been carried out to justify the implementation of voice control into Smart Home Care. In the real environment of Smart Home Care, there are usually unwanted signals and additive noise that negatively affect the voice communication with the control system. This article describes the addition of a sophisticated system for filtering the additive background noise out of the voice communication with the control system. The additive noise significantly lowers the success of recognizing voice commands to control operating and technical functions of an intelligent building. Within the scope of the proposed application, a complex system based on fuzzy-neuron networks, specifically the ANFIS (Adaptive Neuro-Fuzzy Interference System) for adaptive suppression of unwanted background noises was created. The functionality of the designed system was evaluated both by subjective and by objective criteria (SSNR, DTW). Experimental results suggest that the studied system has the potential to refine the voice control of technical and operating functions of Smart Home Care even in a very noisy environment.},
author = {Vanus, Jan and Koziorek, Jiri and Hercik, Radim},
booktitle = {International Conference on Telecommunications and Signal Processing (TSP)},
doi = {10.1109/TSP.2013.6613996},
isbn = {978-1-4799-0404-4},
keywords = {Smart home,adaptive filter,elderly people,noise cancelation,recognition,voice},
month = {7},
pages = {561--564},
publisher = {IEEE},
title = {{The Design of the Voice Communication in Smart Home Care}},
url = {http://dx.doi.org/10.1186/s13673-015-0035-0 http://ieeexplore.ieee.org/document/6613996/},
year = {2013}
}
@inproceedings{Rusu09ICRA,
abstract = {In our recent work [1], [2], we proposed Point Feature Histograms (PFH) as robust multi dimensional features which describe the local geometry around a point p for 3D point cloud datasets. In this paper, we modify their mathematical expressions and perform a rigorous analysis on their robustness and complexity for the problem of 3D registration for overlapping point cloud views. More concretely, we present several optimizations that reduce their computation times drastically by either caching previously computed values or by revising their theoretical formulations. The latter results in a new type of local features, called Fast Point Feature Histograms (FPFH), which retain most of the discriminative power of the PFH. Moreover, we propose an algorithm for the online computation of FPFH features for realtime applications. To validate our results we demonstrate their efficiency for 3D registration and propose a new sample consensus based method for bringing two datasets into the convergence basin of a local non-linear optimizer: SAC-IA (SAmple Consensus Initial Alignment).},
address = {Kobe, Japan},
author = {Rusu, Radu Bogdan and Blodow, Nico and Beetz, Michael},
booktitle = {The IEEE International Conference on Robotics and Automation (ICRA)},
keywords = {perception},
title = {{Fast Point Feature Histograms (FPFH) for 3D Registration}},
url = {http://files.rbrusu.com/publications/Rusu09ICRA.pdf},
year = {2009}
}
@phdthesis{Singh2012a,
abstract = {With the growth in internet-of-things, social media, mobile devices, and planetary- scale sensing, there is an unprecedented opportunity to assimilate spatio-temporally distributed streams into actionable situations. Detecting situations in realtime can be used to benefit human lives and resources in multiple applications. However, the progress in the field of situation recognition, is still sluggish because: (a) the notion of situations is still vague and ill-defined, (b) there is a lack of abstractions and techniques to help users model their situations of interest, and (c) there is a lack of computational tools to rapidly implement, refine, and personalize these situation models to build various situation-based applications. This dissertation computationally defines situations and presents a framework for personalized situation recognition by providing support for conceptual situation modeling, data unification, real-time situation recognition, personalization, and action taking. The proposed framework defines a situation as “An actionable abstraction of observed spatio-temporal descriptors”, and identifies a data representation, a set of analysis operations, and lays out a workflow for modeling different situations of interest. Con- sidering Space and Time as the unifying axes, it represents data in a grid-based E- xv mage data structure. It defines an algebra of operations (viz. Selection, Aggregation, Classification, Spatio-temporal Characterization, and Spatio-temporal Pattern Match- ing) for situation recognition; and defines a step-by-step guide to help domain experts model their situations based on the data, the operations, and the transformations. The framework is operationalized via EventShop - a web based system which lets users graphically select, import, combine, and operate, on real-time data streams to recognize situations for generating appropriate information and actions. EventShop allows different designers to quickly configure their situation models, evaluate the results, and refine the models until a satisfactory level of performance for supporting various applications is achieved. The detected situations can also be personalized and used for undertaking control actions via Situation-Action rule templates. The framework has been used to build multiple applications including flu monitoring and alerting, wildfire recognition, business decision making, flood alerts, asthma rec- ommendation system, seasonal characteristics analysis, and hurricane monitoring.},
author = {Singh, Vk and Adviser-Jain, R},
school = {UNIVERSITY OF CALIFORNIA, IRVINE},
title = {{Personalized situation recognition}},
url = {http://dl.acm.org/citation.cfm?id=2519816},
year = {2012}
}
@inproceedings{Kalkan2017a,
abstract = {This study aims at investigating gaze aversion behavior in human- human dyads during the course of a conversation. Our goal is to identify the parametric infrastructure, which will underlie the development of gaze behavior in Human Robot Interaction. We employed a job interview setting, where pairs (an interviewer and an interviewee) conducted mock job interviews. Three pairs of native speakers took part in the experiment. Two eye-tracking glasses recorded the scene video, the audio and the eye gaze positions of the participants. The analyses involved synchronization of multimodal data, including video recording data for face tracking, gaze data from the eye trackers, and the audio data for speech segmentation. We investigated frequency, duration, timing and spatial positions of gaze aversions relative to interlocutor's face. The results revealed that the interviewees perform more frequent gaze aversion compared to the interviewers. Moreover, gaze aversion takes longer when accompanied by speech. Also, specific speech instances, such as pause and speech- end signals have significant impact on gaze aversion behavior.},
author = {Acarturk, Cengiz and Kalkan, Sinan and {Arslan Aydin}, Ulku},
booktitle = {2017 3rd IEEE International Conference on Cybernetics (CYBCONF)},
doi = {10.1109/CYBConf.2017.7985753},
isbn = {978-1-5386-2201-8},
month = {6},
pages = {1--6},
publisher = {IEEE},
title = {{A Gaze-Centered Multimodal Approach to Human-Human Social Interaction}},
url = {http://ieeexplore.ieee.org/document/7985753/},
year = {2017}
}
@article{Carabalona2012,
abstract = {Brain–computer interface (BCI) systems aim to enable interaction with other people and the environment without muscular activation by the exploitation of changes in brain signals due to the execution of cognitive tasks. In this context, the visual P300 potential appears suited to control smart homes through BCI spellers. The aim of this work is to evaluate whether the widely used character-speller is more sustainable than an icon-based one, designed to operate smart home environment or to communicate moods and needs. Nine subjects with neurodegenerative diseases and no BCI experience used both speller types in a real smart home environment. User experience during BCI tasks was evaluated recording concurrent physiological signals. Usability was assessed for each speller type immediately after use. Classification accuracy was lower for the icon-speller, which was also more attention demanding. However, in subjective evaluations, the effect of a real feedback partially counterbalanced the difficulty in BCI use. Practitioner Summary: Since inclusive BCIs require to consider interface sustainability, we evaluated different ergonomic aspects of the interaction of disabled users with a character-speller (goal: word spelling) and an icon-speller (goal: operating a real smart home). We found the first one as more sustainable in terms of accuracy and cognitive effort.},
annote = {PMID: 22455346},
author = {Carabalona, Roberta and Grossi, Ferdinando and Tessadri, Adam and Castiglioni, Paolo and Caracciolo, Antonio and de Munari, Ilaria},
doi = {10.1080/00140139.2012.661083},
issn = {0014-0139},
journal = {Ergonomics},
month = {5},
number = {5},
pages = {552--563},
title = {{Light on! Real world evaluation of a P300-based brain-computer interface (BCI) for environment control in a smart home}},
url = {http://dx.doi.org/10.1080/00140139.2012.661083 http://www.tandfonline.com/doi/abs/10.1080/00140139.2012.661083},
volume = {55},
year = {2012}
}
@inproceedings{Breazeal1999b,
abstract = {This paper discusses the role that synthetic emotions could play in building autonomous robots which engage people in human-style so- cial exchange. We present a control architec- ture which integrates synthetic emotions and highlight how they influence the internal dy- namics of the robots controller biasing at- tention, motivation, behavior, learning, and the expression of motor acts. We present results illustrating how this control architecture, em- bodied within an expressive robot and situated in a social environment, enables the robot to socially influence its human caregiver into sat- isfying its goals.},
author = {Breazeal, Cynthia and Velasquez, J},
booktitle = {Autonomous Agents Workshop on Emotion-Based Agent Architectures},
pages = {18--26},
title = {{Robot in Society: Friend or Appliance}},
url = {http://eecs.wsu.edu/{~}holder/courses/cse6362/pubs/Breazeal99.pdf},
year = {1999}
}
@article{Kendon1967,
abstract = {Films of two-person conversations were transcribed and analyzed from the point of view of how gaze direction is related to utterance and silence. It was found that patterns of looking were systematically related to features of talk and could be accounted for in terms of the monitoring functions of gaze. At the same time, evidence was presented that suggested that gaze direction may also play a role in the regulation of turn-taking in conversation.},
author = {Kendon, Adam},
doi = {10.1016/0001-6918(67)90005-4},
issn = {00016918},
journal = {Acta Psychologica},
keywords = {gaze,interaction},
pages = {22--63},
title = {{Some Functions of Gaze-Direction in Social Interaction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0001691867900054 https://linkinghub.elsevier.com/retrieve/pii/0001691867900054},
volume = {26},
year = {1967}
}
@inproceedings{Bleiweiss2009,
abstract = {We present an improved framework for real-time segmenta- tion and tracking by fusing depth and RGB color data. We are able to solve common problems seen in tracking and segmentation of RGB im- ages, such as occlusions, fast motion, and objects of similar color. Our proposed real-time mean shift based algorithm outperforms the current state of the art and is significantly better in difficult scenarios.},
author = {Bleiweiss, Amit and Werman, Michael},
booktitle = {Proceedings of the DAGM 2009 Workshop on Dynamic 3D Imaging},
pages = {58--69},
title = {{Fusing Time-of-Flight Depth and Color for Real-Time Segmentation and Tracking}},
year = {2009}
}
@article{Bourobou2015,
abstract = {This paper discusses the possibility of recognizing and predicting user activities in the IoT (Internet of Things) based smart environment. The activity recognition is usually done through two steps: activity pattern clustering and activity type decision. Although many related works have been suggested, they had some limited performance because they focused only on one part between the two steps. This paper tries to find the best combination of a pattern clustering method and an activity decision algorithm among various existing works. For the first step, in order to classify so varied and complex user activities, we use a relevant and efficient unsupervised learning method called the K-pattern clustering algorithm. In the second step, the training of smart environment for recognizing and predicting user activities inside his/her personal space is done by utilizing the artificial neural network based on the Allen's temporal relations. The experimental results show that our combined method provides the higher recognition accuracy for various activities, as compared with other data mining classification algorithms. Furthermore, it is more appropriate for a dynamic environment like an IoT based smart home.},
author = {Bourobou, Serge and Yoo, Younghwan},
doi = {10.3390/s150511953},
issn = {1424-8220},
journal = {Sensors},
keywords = {Allen's temporal relations,activity recognition,allen,anomaly prediction,clustering,iot,neural,neural network,pattern,pattern clustering,s temporal relations,smart home},
month = {5},
number = {5},
pages = {11953--11971},
title = {{User Activity Recognition in Smart Homes Using Pattern Clustering Applied to Temporal ANN Algorithm}},
url = {http://www.mdpi.com/1424-8220/15/5/11953/},
volume = {15},
year = {2015}
}
@book{Hebb1961,
author = {Hebb, D O},
publisher = {Science Editions, John Wiley and sons},
series = {Science Editions},
title = {{The Organization of Behavior: A Neuropsychological Theory}},
url = {https://books.google.de/books?id=YpEHAQAAIAAJ},
year = {1961}
}
@article{Velik2014,
abstract = {Human activity recognition is a prerequisite for many innovative$\backslash$napplications including elderly activity monitoring and support in order$\backslash$nto enable elderly people to live longer independently in their own$\backslash$nhomes. Over the past decade, a diversity of different activity$\backslash$nrecognition approaches has been developed from which the majority$\backslash$nfocuses on the processing of data from one sensor modality only (e.g.,$\backslash$nvision). Nonetheless, a merging of data from multiple disparate sources$\backslash$nhas the potential of offering more accurate, robust, descriptive,$\backslash$nintuitive, and meaningful results due to the availability of$\backslash$ncomplementary and partially redundant information. This article (1)$\backslash$ngives a review of existing multimodal approaches for elderly activity$\backslash$nrecognition in home settings, (2) introduces a powerful activity$\backslash$nrecognition model based on brain-inspired multimodal data mining$\backslash$nmethods, (3) employs this model for the purpose of daily activity$\backslash$nrecognition in a home setting using a publically available real world$\backslash$ndataset, and (4) quantitatively compares the obtained results with state$\backslash$nof the art multimodal activity recognition methods including hidden$\backslash$nMarkov models, conditional random fields, decision trees, a Bayes$\backslash$napproach, and a context lattice.},
author = {Velik, Rosemarie},
doi = {10.3233/AIS-140266},
issn = {1876-1364},
journal = {Journal of Ambient Intelligence and Smart Environments},
keywords = {Ambient assisted living,activity recognition,multimodal,neu,review},
number = {4},
pages = {447--468},
title = {{A brain-inspired multimodal data mining approach for human activity recognition in elderly homes}},
volume = {6},
year = {2014}
}
@phdthesis{Stefanov2018,
author = {Stefanov, Kalin},
isbn = {9789177298106},
school = {KTH Royal Institute of Technology},
title = {{Recognition and Generation of Communicative Signals: Modeling of Hand Gestures, Speech Activity and Eye-Gaze in Human-Machine Interaction}},
year = {2018}
}
@inproceedings{Bo2012,
abstract = {Recently introduced RGB-D cameras are capable of providing high quality synchronized videos of both color and depth. With its advanced sensing capabilities, this technology represents an opportunity to dramatically increase the capabilities of object recognition. It also raises the problem of developing expressive features for the color and depth channels of these sensors. In this paper we introduce hierarchical matching pursuit (HMP) for RGB-D data. HMP uses sparse coding to learn hierarchical feature representations from raw RGB-D data in an unsupervised way. Extensive experiments on various datasets indicate that the features learned with our approach enable superior object recognition results using linear support vector machines.},
author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
booktitle = {ISER},
title = {{Unsupervised Feature Learning for RGB-D Based Object Recognition}},
url = {http://www.cs.washington.edu/homes/lfb/paper/iser12.pdf},
year = {2012}
}
@article{Ricci2017,
author = {Varadarajan, Jagannadan and Subramanian, Ramanathan and Bul{\`{o}}, Samuel Rota and Ahuja, Narendra and Lanz, Oswald and Ricci, Elisa},
doi = {10.1007/s11263-017-1026-6},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {Conversational groups,Convex optimization,F-formation estimation,Head and body pose estimation,Semi-supervised learning,Video surveillance,conversational groups,convex optimiza-,estimation,f-formation,head and body pose,semi-supervised learning,tion,video surveillance},
month = {4},
number = {2-4},
pages = {410--429},
title = {{Joint Estimation of Human Pose and Conversational Groups from Social Scenes}},
url = {http://link.springer.com/10.1007/s11263-017-1026-6},
volume = {126},
year = {2018}
}
@article{Lehman2016,
abstract = {When the goal is entertainment, designing language-based interactions between characters and small groups of young children is a balancing act. On the one hand, an autonomous character should support the freedom of expression and natural behaviors of children having fun. On the other hand, an autonomous character is only capable of supporting the activity it's designed for and the behavior it anticipates. In the last five years we have watched this tension between freedom and constraint play out in hundreds of small groups in a variety of activities. Using two of the activities as examples, we chart the ups and downs of turn-taking and other language behaviors along the Fun Curve.},
author = {Lehman, Jill Fain and Leite, Iolanda},
doi = {10.1609/aimag.v37i4.2685},
issn = {0738-4602},
journal = {AI Magazine},
keywords = {Winter 2016},
month = {1},
number = {4},
pages = {55},
title = {{Turn-Taking, Children, and the Unpredictability of Fun}},
url = {https://aaai.org/ojs/index.php/aimagazine/article/view/2685},
volume = {37},
year = {2017}
}
@inproceedings{Laudy2006,
abstract = {We present a summary of the drawbacks and deficiencies that we noticed in the currently available Command Support Systems (CSS) and the methodology we propose to improve them: Situation Awareness support through Cognitive Fusion of Information stemmed out of document analysis. Our approach is divided into two parts: a methodology for situation representation out of document analysis and a methodology for situation analysis and reasoning to support decision-making. The situation representation part is based on the use of conceptual graphs and fusion of nodes in graph structures, whereas the situation analysis part follows Complex Event Processing methodology. 1.0},
annote = {Command Support Systems (CSS)
* Unterst{\"{u}}tzen Entscheidungsprozesse
* Liefern {\"{U}}bersicht {\"{u}}ber DatenSituation ben{\"{o}}tigt
* Summe relevanter Informationen
* Hinsichtlich Aufgaben und Zielzust{\"{a}}nden
* Extrahiere und Gruppiere relevante Informationen
Informationsfusion
* Fusion aus mehreren Quellen
* Erreichen einer qualitativ neuen Bedeutung in fusionierten Daten


Hypothese
* Situations-Repr{\"{a}}sentation
** Relevante (f{\"{u}}r definierte Mission) Daten werden gesammelt
** in Dom{\"{a}}nenwissen interpretiert
** keine Inferenz
** keine Projektion
** Darstellung als Entit{\"{a}}ten, Ereignisse und Relationen
** Fusion
*** Aggregation in gemeinsame Struktur 
*** Repr{\"{a}}sentiert fortlaufende Situation
* Situations-Analyse
** Dynamik der fortlaufenden Situation
** Missions und Wissensmuster liefern Ereignisse
*** Missionsmuster beschreiben die Aufgabe und den Zustand der Abarbeitung
*** Wissensmuster sind das gleiche aber aus Dom{\"{a}}nenwissen
** Situationsanalyse vergleicht diese mit Situation und liefert Entscheidungsereignisse
** Situationsereignisse werden generiert wenn Situation sich {\"{a}}ndert
** Kontrollereignisse Korrelation mehrerer Ereignisse
** Entscheidungsereignisse folgen aus Situationsanalyse (Vorschl{\"{a}}ge f{\"{u}}r Operator)
* Abstrakte Situation (Pattern)
** Konzeptgraph
*** Knoten sind Akteure und Ereignisse der Situation
*** Kanten sind Relationen zwischen diesen (z.B. teil{\_}von ...)
** Instanziierung
*** Erstellen von Teilgraphen auf Daten
*** Fusionieren},
author = {Laudy, Claire and Mattioli, Juliette and Museux, Nicolas},
booktitle = {Information Fusion for Command Support},
keywords = {command support systems,dokumentenanalyse,fusion,military,situation awareness},
number = {Bsik 03024},
pages = {3--1 -- 3--12},
title = {{Cognitive Situation Awareness for Information Superiority}},
year = {2006}
}
@inproceedings{Kalkan2017,
abstract = {This study aims at investigating gaze aversion behavior in human- human dyads during the course of a conversation. Our goal is to identify the parametric infrastructure, which will underlie the development of gaze behavior in Human Robot Interaction. We employed a job interview setting, where pairs (an interviewer and an interviewee) conducted mock job interviews. Three pairs of native speakers took part in the experiment. Two eye-tracking glasses recorded the scene video, the audio and the eye gaze positions of the participants. The analyses involved synchronization of multimodal data, including video recording data for face tracking, gaze data from the eye trackers, and the audio data for speech segmentation. We investigated frequency, duration, timing and spatial positions of gaze aversions relative to interlocutor's face. The results revealed that the interviewees perform more frequent gaze aversion compared to the interviewers. Moreover, gaze aversion takes longer when accompanied by speech. Also, specific speech instances, such as pause and speech- end signals have significant impact on gaze aversion behavior.},
address = {Exeter, UK},
author = {Kalkan, Sinan},
booktitle = {IEEE International Conference on Cybernetics Workshop on Cognition in Mixed Realities},
keywords = {dyadic interaction,gaze aversion},
title = {{A Gaze-Centered Multimodal Approach to Human-Human Social Interaction}},
year = {2017}
}
@inproceedings{Satake2009,
abstract = {This paper proposes a model of approach behavior with which a robot can initiate conversation with people who are walking. We developed the model by learning from the failures in a simplistic approach behavior used in a real shopping mall. Sometimes people were unaware of the robot's presence, even when it spoke to them. Sometimes, people were not sure whether the robot was really trying to start a conversation, and they did not start talking with it even though they displayed interest. To prevent such failures, our model includes the following functions: predicting the walking behavior of people, choosing a target person, planning its approaching path, and nonverbally indicating its intention to initiate a conversation. The approach model was implemented and used in a real shopping mall. The field trial demonstrated that our model significantly improves the robot's performance in initiating conversations.},
address = {New York, New York, USA},
author = {Satake, Satoru and Kanda, Takayuki and Glas, Dylan F. and Imai, Michita and Ishiguro, Hiroshi and Hagita, Norihiro},
booktitle = {International Conference on Human Robot Interaction (HRI)},
doi = {10.1145/1514095.1514117},
isbn = {9781605584041},
keywords = {anticipation,approaching behavior,position-based interaction},
pages = {109},
publisher = {ACM Press},
title = {{How to Approach Humans? – Strategies for Social Robots to Initiate Interaction}},
url = {http://portal.acm.org/citation.cfm?doid=1514095.1514117},
year = {2009}
}
@article{Leite2013,
abstract = {As the field of HRI evolves, it is important to un- derstand how users interact with robots over long periods. This paper reviews the current research on long-term interaction between users and social robots. We describe the main features of these robots and highlight the main find- ings of the existing long-term studies.We also present a set of directions for future research and discuss some open issues that should be addressed in this field},
author = {Leite, Iolanda and Martinho, Carlos and Paiva, Ana},
doi = {10.1007/s12369-013-0178-y},
isbn = {1875-4791},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Human-robot interaction,Long-term interaction,Longitudinal studies,Social robots},
month = {4},
number = {2},
pages = {291--308},
title = {{Social Robots for Long-Term Interaction: A Survey}},
url = {http://link.springer.com/10.1007/s12369-013-0178-y},
volume = {5},
year = {2013}
}
@article{Lohan2012,
abstract = {From learning by observation, robotic research has moved towards investigations of learning by interaction. This research is inspired by findings from developmental studies on human children and primates pointing to the fact that learning takes place in a social environment. Recently, driven by the idea that learning through observation or imitation is limited because the observed action not always reveals its meaning, scaffolding or bootstrapping processes supporting learning received increased attention. However, in order to take advantage of teaching strategies, a system needs to be sensitive to a tutor as children are. We therefore developed a module allowing for spotting the tutor by monitoring her or his gaze and detecting modifications in object presentation in form of a looming action. In this article, we will present the current state of the development of our contingency detection system as a set of features. {\textcopyright} 2011 Springer Science {\&} Business Media BV.},
author = {Lohan, Katrin S. and Rohlfing, Katharina J. and Pitsch, Karola and Saunders, Joe and Lehmann, Hagen and Nehaniv, Chrystopher L. and Fischer, Kerstin and Wrede, Britta},
doi = {10.1007/s12369-011-0125-8},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Contingency,Feedback,Human robot interaction,Joint attention,Parent child interaction,Tutoring,Tutoring behavior},
month = {4},
number = {2},
pages = {131--146},
title = {{Tutor Spotter: Proposing a Feature Set and Evaluating It in a Robotic System}},
url = {http://link.springer.com/10.1007/s12369-011-0125-8},
volume = {4},
year = {2012}
}
@inproceedings{Janicke2016,
abstract = {Activity Recognition (AR) systems increasingly per- vade our daily lives, reaching from the monitoring of daily activities to the support in medical care. However, such systems are created with narrow system specifications and a very specific field of application in mind, leading to application-dependent setup and configuration by their users. To overcome these limitations, research focuses on autonomous solutions that are able to work with no (or minimal) user interaction. A major step towards this goal is the integration of novel input sources (i.e., sensors) at runtime, leading to a dynamic input-space (i.e., variable dimensionality). This paper presents an approach to systematically investigate methods necessary for the creation of self-adapting classification systems. This includes an architec- ture, based on Organic Computing principles, as well as the development of measures for comparing probabilistic models and procedures for evaluating classifiers of different dimensionality. With such evaluation techniques, systems are enabled to adapt their system model at runtime in a self-organized manner. Besides self-improvement (adding a new sensor), we also address the problem of self-healing (replacing a sensor that dropped out).},
author = {Janicke, Martin and Tomforde, Sven and Sick, Bernhard},
booktitle = {2016 IEEE International Conference on Autonomic Computing (ICAC)},
doi = {10.1109/ICAC.2016.22},
isbn = {978-1-5090-1654-9},
keywords = {activity recognition,adaptation,generative models,probabilistic},
month = {7},
pages = {285--291},
publisher = {IEEE},
title = {{Towards Self-Improving Activity Recognition Systems Based on Probabilistic, Generative Models}},
url = {http://ieeexplore.ieee.org/document/7573154/},
year = {2016}
}
@article{Chen2009,
abstract = {Smart Homes (SH) have emerged as a realistically viable solution capable of providing technology-driven assistive living for the elderly and disabled. Nevertheless, it still remains a challenge to provide situation-aware assistance for those in need in their Activity of Daily Living (ADL). This paper introduces a systematic approach to providing situation-aware ADL assistances in a smart home environment. The approach makes use of semantic technologies for sensor data modeling, fusion and management, thus creating machine understandable and processable situational data. It exploits intelligent agents for interpreting and reasoning semantic situational (meta)data to enhance situation-aware decision support. We analyze the nature and issues of SH-based healthcare for cognitively deficient inhabitants. We discuss the ways in which semantic technologies enhance situation comprehension. We describe a cognitive agent for realizing high-level cognitive capabilities such as prediction and explanation. We outline the implementation of a prototype assistive system and illustrate the proposed approach through simulated and real-time ADL assistance scenarios in the context of situation aware assistive living.},
address = {New York, New York, USA},
author = {Chen, Liming and Nugent, Chris and Al-Bashrawi, Ahmad},
doi = {10.1145/1806338.1806394},
isbn = {9781605586601},
journal = {Proceedings of the 11th International Conference on Information Integration and Web-based Applications {\&} Services - iiWAS '09},
pages = {298},
publisher = {ACM Press},
title = {{Semantic data management for situation-aware assistance in ambient assisted living}},
url = {http://portal.acm.org/citation.cfm?doid=1806338.1806394},
year = {2009}
}
@inproceedings{Richter,
abstract = {In this paper we present our humanoid robot “Meka”, partici- pating in a multi party human robot dialogue scenario. Active arbitration of the robot's attention based-on multi-modal stim- uli is utilised to attain persons which are outside of the robots field of view. We investigate the impact of this attention management and an addressee recognition on the robot's capability to distinguish utterances directed at it from communication between humans. Based on the results of a user study, we show that mutual gaze at the end of an utterance, as a means of yielding a turn, is a substantial cue for addressee recognition. Verification of a speaker through the detection of lip movements can be used to further increase precision. Further- more, we show that even a rather simplistic fusion of gaze and lip movement cues allows a considerable enhancement in addressee estimation, and can be altered to adapt to the requirements of a particular scenario.},
address = {Singapore},
author = {Richter, Viktor and Carlmeyer, Birte and Lier, Florian and {Meyer zu Borgsen}, Sebastian and Schlangen, David and Kummert, Franz and Wachsmuth, Sven and Wrede, Britta},
booktitle = {International Conference on Human Agent Interaction (HAI)},
doi = {10.1145/2974804.2974823},
isbn = {9781450345088},
keywords = {addressee,attention management,autonomous robot,dialogue systems,interaction,multi-modal,multi-party,speaker},
pages = {43--50},
publisher = {ACM Press},
title = {{Are You Talking to Me? Improving the Robustness of Dialogue Systems in a Multi Party HRI Scenario by Incorporating Gaze Direction and Lip Movement of Attendees}},
url = {http://dl.acm.org/citation.cfm?doid=2974804.2974823},
year = {2016}
}
@article{Cook2010,
abstract = {The data mining and pervasive computing technologies found in smart homes offer unprecedented opportunities for providing context-aware services, including health monitoring and assistance to individuals experiencing difficulties living independently at home. In order to provide these services, smart environment algorithms need to recognize and track activities that people normally perform as part of their daily routines. However, activity recognition has typically involved gathering and labeling large amounts of data in each setting to learn a model for activities in that setting. We hypothesize that generalized models can be learned for common activities that span multiple environment settings and resident types. We describe our approach to learning these models and demonstrate the approach using eleven CASAS datasets collected in seven environments.},
author = {Cook, Diane},
doi = {10.1109/MIS.2010.112},
issn = {1541-1672},
journal = {IEEE Intelligent Systems},
keywords = {activities of daily living,activity recognition,generalizes model,machine learning,pervasive computing,ubiquitous computing},
month = {1},
number = {1},
pages = {32--38},
title = {{Learning Setting-Generalized Activity Models for Smart Spaces}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3068197{\&}tool=pmcentrez{\&}rendertype=abstract http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5567086},
volume = {27},
year = {2012}
}
@inproceedings{Salam2015,
abstract = {In this paper, we address the problematic of automatic detection of engagement in multi-party Human-Robot Interaction scenarios. The aim is to investigate to what extent are we able to infer the engagement of one of the entities of a group based solely on the cues of the other entities present in the interaction. In a scenario featuring 3 entities: 2 participants and a robot, we extract behavioural cues that concern each of the entities, we then build models based solely on each of these entities' cues and on combinations of them to predict the engagement level of each of the participants. Person-level cross validation shows that we are capable of detecting the engagement of the participant in question using solely the behavioural cues of the robot with a high accuracy compared to using the participant's cues himself (75.91{\%} vs. 74.32{\%}). Moreover using the behavioural cues of the other participant is also informative where it permits the detection of the engagement of the participant in question at an accuracy of 62.15{\%} on average. The correlation between the features of the other participant with the engagement labels of the participant in question suggests a high cohesion between the two participants. In addition, the similarity of the most significantly correlated features among the two participants suggests a high synchrony between the two parties.},
annote = {Engagement features:
* Nods
* VFOA
* Speaking
* Addressee (annotated)
* Proximity (Face size)},
author = {Salam, Hanan and Chetouani, Mohamed},
booktitle = {2015 International Conference on Affective Computing and Intelligent Interaction (ACII)},
doi = {10.1109/ACII.2015.7344593},
isbn = {978-1-4799-9953-8},
keywords = {Context,Face,Feature extraction,Human-Robot Interaction,Robot kinematics,Speech,Visualization,attention,automatic engagement inference,multi-party interaction,social signal processing},
month = {9},
pages = {341--347},
publisher = {IEEE},
title = {{Engagement detection based on mutli-party cues for human robot interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7344593},
year = {2015}
}
@inproceedings{Vertegaal2001,
abstract = {In multi-agent, multi-user environments, users as well as agents should have a means of establishing who is talking to whom. In this paper, we present an experiment aimed at evaluating whether gaze directional cues of users could be used for this purpose. Using an eye tracker, we measured subject gaze at the faces of conversational partners during four-person conversations. Results indicate that when someone is listening or speaking to individuals, there is indeed a high probability that the person looked at is the person listened (p=88{\%}) or spoken to (p=77{\%}). We conclude that gaze is an excellent predictor of conversational attention in multiparty conversations. As such, it may form a reliable source of input for conversational systems that need to establish whom the user is speaking or listening to. We implemented our findings in FRED, a multi-agent conversational system that uses eye input to gauge which agent the user is listening or speaking to.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
author = {Vertegaal, Roel and Slagter, Robert and van der Veer, Gerrit and Nijholt, Anton},
booktitle = {Conference on Human Gactors in Computing Systems (CHI)},
doi = {10.1145/365024.365119},
eprint = {01/0003},
isbn = {1581133278},
keywords = {attention-based interfaces,attentive agents,communication,conversational attention,eye tracking,gaze,multiparty},
pages = {301--308},
primaryClass = {1-58113-327-8},
publisher = {ACM Press},
title = {{Eye Gaze Patterns in Conversations: There is More to Conversational Ggents than Meets the Eyes}},
url = {http://portal.acm.org/citation.cfm?doid=365024.365119},
year = {2001}
}
@article{RosenthalvonderPutten2018,
author = {{Rosenthal-von der P{\"{u}}tten}, Astrid M. and Kr{\"{a}}mer, Nicole C. and Herrmann, Jonathan},
doi = {10.1007/s12369-018-0466-7},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Affective nonverbal behavior,Emotional state,Experimental study,Humanoid robot,Human–robot interaction,Self-disclosure,affect,affective nonverbal behavior,behaviour,effect,experimental study,human,humanoid robot,led,nao,robot interaction,self-disclosure},
month = {11},
number = {5},
pages = {569--582},
title = {{The Effects of Humanlike and Robot-Specific Affective Nonverbal Behavior on Perception, Emotion, and Behavior}},
url = {http://link.springer.com/10.1007/s12369-018-0466-7},
volume = {10},
year = {2018}
}
@inproceedings{Tao2005,
abstract = {Affective computing is currently one of the most active research topics, furthermore, having increasingly intensive attention. This strong interest is driven by a wide spectrum of promising applications in many areas such as virtual reality, smart surveillance, perceptual interface, etc. Affective computing concerns multidisciplinary knowledge background such as psychology, cognitive, physiology and computer sciences. The paper is emphasized on the several issues involved implicitly in the whole interactive feedback loop. Various methods for each issue are discussed in order to examine the state of the art. Finally, some research challenges and future directions are also discussed. 1},
author = {Tao, Jianhua and Tan, Tieniu},
booktitle = {First International Conference, ACII 2005},
keywords = {affective computing},
pages = {981--995},
publisher = {Springer Berlin Heidelberg},
title = {{Affective Computing : A Review}},
year = {2005}
}
@inproceedings{Kuwertz2013,
abstract = {Many systems rely on the integration of environment observations provided by sensor systems to fulfill their tasks. The Object-Oriented World Model (OOWM) is an information fusion architecture allowing to integrate observations from heterogeneous sensing systems and to provide consolidated information to higher level processing modules in a compound system. Both, data integration and the provision of consolidated information require to exchange information in a meaningful and semantic interoperable way. To promote the semantic interoperability in the OOWM, an ontology-based meta model is presented, allowing to structure the knowledge used to represent application domains as well as interface information objects. This meta model defines an upper level ontology for world modeling, which can be extended by specific domain models and facilitates the integration of additional sensor systems. To allow semantic access to OOWM information, the use of OGC Web Feature Services and Sensor Observation Services based on the meta model is proposed.},
author = {Kuwertz, Achim and Schneider, G},
booktitle = {ICONS 2013, The Eighth International Conference on Systems},
isbn = {9781612082462},
keywords = {-object-oriented world modeling,knowledge repre-,ontology,oowm,semantic interoperability,sentation,sos,wfs},
number = {c},
pages = {185--191},
title = {{Ontology-Based Meta Model in Object-Oriented World Modeling for Interoperable Information Access}},
url = {http://www.thinkmind.org/index.php?view=article{\&}articleid=icons{\_}2013{\_}8{\_}20{\_}20142},
year = {2013}
}
@inproceedings{1246,
address = {Nice, France},
author = {Rusu, Radu Bogdan and Blodow, Nico and Marton, Zoltan Csaba and Beetz, Michael},
booktitle = {Proceedings of the 21st IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title = {{Aligning Point Cloud Views using Persistent Feature Histograms}},
year = {2008}
}
@book{Szeliski2010,
author = {Szeliski, Richard},
publisher = {Springer},
title = {{Computer Vision : Algorithms and Applications}},
url = {http://szeliski.org/Book},
year = {2010}
}
@inproceedings{Nishino2009,
abstract = {In this paper, we propose a system that tracks a ball stably and accurately and detects the events of the game by using the 3D positional information for automatic soccer video production. We use 3D particle filter with the state vector of nine dimensions in the ball tracking. Since the ball tracking by particle filter is a local search, it is difficult to continue tracking when it fails. Thus, we solve this problem by switching the local search to 3D global search, and by interpolating the lost coordinates. As a result, the tracking accuracy was improved by about 19.8{\%}, and the events like the goal or the goal kick was detected with high accuracy.},
author = {Nishino, Takuro and Ariki, Yasuo and Takiguchi, Tetsuya},
booktitle = {The 2009 International Conference on Multimedia, Information Technology and its Applications (MITA2009)},
number = {1},
pages = {1--4},
title = {{Situation Recognition Using 3D Positional Information of Ball from Monocular Soccer Image Sequence}},
year = {2009}
}
@article{Setti2015,
abstract = {Detection of groups of interacting people is a very interesting and useful task in many modern technologies, with application fields spanning from video-surveillance to social robotics. In this paper we first furnish a rigorous definition of group considering the background of the social sciences: this allows us to specify many kinds of group, so far neglected in the Computer Vision literature. On top of this taxonomy, we present a detailed state of the art on the group detection algorithms. Then, as a main contribution, we present a brand new method for the automatic detection of groups in still images, which is based on a graph-cuts framework for clustering individuals; in particular we are able to codify in a computational sense the sociological definition of F-formation, that is very useful to encode a group having only proxemic information: position and orientation of people. We call the proposed method Graph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms all the state of the art methods in terms of different accuracy measures (some of them are brand new), demonstrating also a strong robustness to noise and versatility in recognizing groups of various cardinality.},
archivePrefix = {arXiv},
author = {Setti, Francesco and Russell, Chris and Bassetti, Chiara and Cristani, Marco},
doi = {10.1371/journal.pone.0123783},
editor = {Ji, Rongrong},
eprint = {arXiv:1409.2702v1},
issn = {1932-6203},
journal = {PLOS ONE},
keywords = {f-formations},
month = {5},
number = {5},
pages = {e0123783},
title = {{F-Formation Detection: Individuating Free-Standing Conversational Groups in Images}},
url = {http://dx.plos.org/10.1371/journal.pone.0123783 https://dx.plos.org/10.1371/journal.pone.0123783},
volume = {10},
year = {2015}
}
@inproceedings{Kazemi2014,
abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with ef- ficient feature selection. Different regularization strategies and its importance to combat overfitting are also investi- gated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
author = {Kazemi, Vahid and Sullivan, Josephine},
booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.241},
isbn = {978-1-4799-5118-5},
issn = {10636919},
keywords = {Decision Trees,Face Alignment,Gradient Boosting,Real-Time,face detection,landmarks,vision},
month = {6},
pages = {1867--1874},
publisher = {IEEE},
title = {{One millisecond face alignment with an ensemble of regression trees}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909637},
year = {2014}
}
@article{Vaufreydaz2015,
abstract = {Recognition of intentions is a subconscious cognitive process vital to human communication. This skill enables anticipation and increases the quality of interactions between humans. Within the context of engagement, non-verbal signals are used to communicate the intention of starting the interaction with a partner. In this paper, we investigated methods to detect these signals in order to allow a robot to know when it is about to be addressed. Originality of our approach resides in taking inspiration from social and cognitive sciences to perform our perception task. We investigate meaningful features, i.e. human readable features, and elicit which of these are important for recognizing someone's intention of starting an interaction. Classically, spatial information like the human position and speed, the human-robot distance are used to detect the engagement. Our approach integrates multimodal features gathered using a companion robot equipped with a Kinect. The evaluation on our corpus collected in spontaneous conditions highlights its robustness and validates the use of such a technique in a real environment. Experimental validation shows that multimodal features set gives better precision and recall than using only spatial and speed features. We also demonstrate that 7 selected features are sufficient to provide a good starting engagement detection score. In our last investigation, we show that among our full 99 features set, the space reduction is not a solved task. This result opens new researches perspectives on multimodal engagement detection.},
archivePrefix = {arXiv},
author = {Vaufreydaz, Dominique and Johal, Wafa and Combe, Claudine},
doi = {10.1016/j.robot.2015.01.004},
eprint = {1503.03732},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Affective computing,Companion robots,Healthcare technologies,Multimodal perception,adressing,intention,interaction start},
month = {1},
number = {0},
pages = {--},
publisher = {Elsevier B.V.},
title = {{Starting engagement detection towards a companion robot using multimodal features}},
url = {http://www.sciencedirect.com/science/article/pii/S0921889015000111 http://arxiv.org/abs/1503.03732 http://dx.doi.org/10.1016/j.robot.2015.01.004 http://linkinghub.elsevier.com/retrieve/pii/S0921889015000111},
year = {2015}
}
@inproceedings{Levashova,
abstract = {With the increasing use of sensors and actuators in technical systems and knowledge-intensive services the need for processing the information captured by these sensors and “making sense” out of it increases. Knowledge fusion is supposed to contribute to this field since it aims at integrating knowledge from different sources. Development of knowledge fusion solutions is a complex task which can be compared to systems and software development. As in other development areas there is a need for efficient development processes which can be supported by reusing solution parts, such as patterns or components. The paper brings together experiences from knowledge fusion sub- system development and from design of knowledge fusion patterns. The main contributions of this paper are (1) a real-world application scenario presenting typical requirements to knowledge fusion systems, (2) application of knowledge fusion patterns from context-based decision support to situation recognition, (3) recommendations from this application case.},
author = {Levashova, Tatiana and Sandkuhl, Kurt and Shilov, Nikolay and Smirnov, Alexander},
booktitle = {InternationalWorkshop on Ontologies and Information Systems Situation},
keywords = {detection,knowledge fusion,knowledge fusion pattern,knowledge logistics,knowledge-fusion,ointology,situation},
pages = {18--29},
title = {{Situation Detection Based on Knowledge Fusion Patterns}},
year = {2014}
}
@inproceedings{Salam2015a,
abstract = {In this paper, we consider engagement in the context of Human-Robot Interaction (HRI). Previous studies in HRI relate engagement to emotion and attention independently from the context.We propose a model of engagement in Human- Robot Interaction depending on the context in which human and robot act. In our model, the mental and emotional states of the user related to engagement vary during the interaction according to the current context. Knowing the context of the interaction, the robot would know what to expect regarding the mental and the emotional state of the user and thus if it perceives a state that is not in accordance with its expectations, this might signal disengagement.},
author = {Salam, Hanan and Chetouani, Mohamed},
booktitle = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)},
doi = {10.1109/FG.2015.7284845},
isbn = {978-1-4799-6026-2},
keywords = {attention},
month = {5},
number = {MAY},
pages = {1--6},
publisher = {IEEE},
title = {{A multi-level context-based modeling of engagement in Human-Robot Interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7284845},
year = {2015}
}
@article{Lowe2004,
abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination.},
author = {Lowe, David G.},
doi = {10.1023/B:VISI.0000029664.99615.94},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {SIFT},
month = {11},
number = {2},
pages = {91--110},
title = {{Distinctive Image Features from Scale-Invariant Keypoints}},
url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
volume = {60},
year = {2004}
}
@inproceedings{Roddy2018,
archivePrefix = {arXiv},
author = {Roddy, Matthew and Skantze, Gabriel and Harte, Naomi},
booktitle = {Proc. Interspeech 2018},
doi = {10.21437/Interspeech.2018-2124},
eprint = {1806.11461v1},
pages = {586----590},
title = {{Investigating Speech Features for Continuous Turn-Taking Prediction Using LSTMs}},
url = {http://dx.doi.org/10.21437/Interspeech.2018-2124},
year = {2018}
}
@inproceedings{Marquardt2011,
abstract = {People naturally understand and use proxemic relationships (e.g., their distance and orientation towards others) in every- day situations. However, only few ubiquitous computing (ubicomp) systems interpret such proxemic relationships to mediate interaction (proxemic interaction). A technical prob- lem is that developers find it challenging and tedious to ac- cess proxemic information from sensors. Our Proximity Toolkit solves this problem. It simplifies the exploration of interaction techniques by supplying fine-grained proxemic information between people, portable devices, large interac- tive surfaces, and other non-digital objects in a room-sized environment. The toolkit offers three key features. 1) It fa- cilitates rapid prototyping of proxemic-aware systems by supplying developers with the orientation, distance, motion, identity, and location information between entities. 2) It in- cludes various tools, such as a visual monitoring tool, that allows developers to visually observe, record and explore proxemic relationships in 3D space. (3) Its flexible architec- ture separates sensing hardware from the proxemic data model derived from these sensors, which means that a variety of sensing technologies can be substituted or combined to derive proxemic information. We illustrate the versatility of the toolkit with proxemic-aware systems built by students},
address = {New York, New York, USA},
author = {Marquardt, Nicolai and Diaz-Marino, Robert and Boring, Sebastian and Greenberg, Saul},
booktitle = {ACM Symposium on User Interface Software and Technology (UIST)},
doi = {10.1145/2047196.2047238},
isbn = {9781450307161},
keywords = {Proximity,development,prototyping,proxemic interactions,proxemics,toolkit,ubiquitous computing},
pages = {315},
publisher = {ACM Press},
title = {{The Proximity Toolkit: Prototyping Proxemic Interactions in Ubiquitous Computing Ecologies}},
url = {http://portal.acm.org/citation.cfm?doid=1979742.1979691 http://dl.acm.org/citation.cfm?doid=2047196.2047238},
year = {2011}
}
@article{Morwald2011,
abstract = {In robotics object tracking is needed to steer towards objects, check if grasping is successful, or investigate objects more closely by poking or handling them. While many 3D object tracking approaches have been proposed in the past, real world settings pose challenges such as automatically detecting tracking failure, real-time processing, and robustness to occlusion, illumination, and view point changes. This paper presents a 3D tracking system that is capable of overcoming these difficulties using a monocular camera. We present a method of Tracking-State-Detection (TSD) that takes advantage of commercial graphics processors to map textures onto object geometry, to learn textures online, and to recover object pose in real-time. Our system is able to handle 6 DOF object motion during changing lighting conditions, partial occlusion and motion blur while maintaining an accuracy of a few millimetres. Furthermore using TSD we are able to automatically detect occlusions or whether we lost track, and can then trigger a SIFT-based recognition system that is trained during tracking to recover the pose. Evaluations are presented in relation to ground truth pose data and examples present TSD on real- world scenes presented in video sequences.},
author = {Morwald, Thomas and Zillich, Michael and Prankl, Johann and Vincze, Markus},
doi = {10.1109/ROBIO.2011.6181734},
isbn = {978-1-4577-2138-0},
journal = {2011 IEEE International Conference on Robotics and Biomimetics},
month = {12},
pages = {2830--2837},
publisher = {Ieee},
title = {{Self-monitoring to improve robustness of 3D object tracking for robotics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6181734},
year = {2011}
}
@inproceedings{Bicocchi2012,
abstract = {Pervasive services may have to rely on multimodal classification to implement situation-recognition. However, the effectiveness of current multimodal classifiers is often not satisfactory. In this paper, we describe a novel approach to multimodal classification based on integrating a vision sensor with a commonsense knowledge base. Specifically, our ap- proach is based on extracting the individual objects perceived by a camera and classifying them individually with non- parametric algorithms; then, using a commonsense knowledge base, classifying the overall scene with high effectiveness. Such classification results can then be fused together with other sensors, again on a commonsense basis, for both improving classification accuracy and dealing with missing labels. Ex- perimental results are presented to assess, under different configurations, the effectiveness of our vision sensor and its integration with other kinds of sensors, proving that the approach is effective and able to correctly recognize a number of situations in open-ended environments.},
author = {Bicocchi, Nicola and Lasagni, Matteo and Zambonelli, Franco},
booktitle = {2012 IEEE International Conference on Pervasive Computing and Communications},
doi = {10.1109/PerCom.2012.6199848},
isbn = {978-1-4673-0258-6},
keywords = {-pervasive computing,activity recognition,com-,image analysis,monsense knowledge,scene-recognition,situation recognition,vision},
month = {3},
number = {March},
pages = {48--56},
publisher = {IEEE},
title = {{Bridging vision and commonsense for multimodal situation recognition in pervasive systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6199848},
year = {2012}
}
@inproceedings{Nick2007,
abstract = {IT-based living assistance systems focusing on the support of people with special needs in their daily routine have to continuously monitor and assist them in an appropriate way. To this end, we have developed a Monitoring and Assistance component including a hybrid reasoner that is able to adapt planned and running treatments according to the current situation and context. In this paper, we explain the underlying approaches followed in the reasoner, describe the reasoning technologies used for this task and its sub- tasks, and present some first evaluation results.},
author = {Nick, Markus and Becker, Martin},
booktitle = {International Conference on Hybrid Intelligent Systems (HIS)},
doi = {10.1109/HIS.2007.69},
isbn = {0-7695-2946-1},
month = {9},
pages = {283--289},
publisher = {IEEE},
title = {{A Hybrid Approach to Intelligent Living Assistance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4344065},
year = {2007}
}
@inproceedings{Armstrong1995,
abstract = {We describe an object tracker robust to a number of ambient conditions which often severely degrade performance, for example partial occlusion. The robustness is achieved by describing the object as a set of related geometric primitives (lines, conics, etc.), and using redundant measurements to facilitate the detection of outliers. This improves the overall tracking performance. Results are given for frame rate tracking on image sequences.},
author = {{Armstrong, M. and Zisserman}, A.},
booktitle = {Asian Conference on Computer Vision},
pages = {58----61},
title = {{Robust Object Tracking}},
url = {http://www.robots.ox.ac.uk:5000/{~}vgg/publications/1995/Armstrong95/armstrong95.pdf},
year = {1995}
}
@article{Rosten2005,
abstract = {This paper addresses the problem of real-time 3D modelbased tracking by combining point-based and edge-based tracking systems. We present a careful analysis of the properties of these two sensor systems and show that this leads to some non-trivial design choices that collectively yield extremely high performance. In particular, we present a method for integrating the two systems and robustly combining the pose estimates they produce. Further we show how on-line learning can be used to improve the performance of feature tracking. Finally, to aid real-time performance, we introduce the FAST feature detector which can perform full-frame feature detection at 400Hz. The combination of these techniques results in a system which is capable of tracking average prediction errors of 200 pixels. This level of robustness allows us to track very rapid motions, such as 50° camera shake at 6Hz.},
author = {Rosten, E. and Drummond, T.},
doi = {10.1109/ICCV.2005.104},
isbn = {0-7695-2334-X},
journal = {Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1},
pages = {1508--1515 Vol. 2},
publisher = {Ieee},
title = {{Fusing points and lines for high performance tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544896},
year = {2005}
}
@inproceedings{Bohus2010,
abstract = {We study how synchronized gaze, gesture and speech rendered by an embodied conversational agent can influence the flow of conversations in multiparty settings. We review a computational framework for turn taking that provides the foundation for tracking and communicating intentions to hold, release, or take control of the conversational floor. We then present details of the implementation of the approach in an embodied conversational agent and describe experiments with the system in a shared task setting. Finally, we discuss results showing how the verbal and non-verbal cues used by the avatar can shape the dynamics of multiparty conversation.},
address = {New York, New York, USA},
annote = {A model of cooperatice turn-taking
* conversational floor
** hold,release,take,null
** how to achieve these},
author = {Bohus, Dan and Horvitz, Eric},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/1891903.1891910},
isbn = {9781450304146},
keywords = {behavioral models,dialog,floor management,gaze,gesture,interaction,multimodal systems,multiparty,multiparty interaction,multiparty turn taking,situated,speech,spoken dialog},
pages = {1},
publisher = {ACM Press},
title = {{Facilitating multiparty dialog with gaze, gesture, and speech}},
url = {http://portal.acm.org/citation.cfm?doid=1891903.1891910},
year = {2010}
}
@article{Byom2013,
abstract = {Theory of Mind (ToM) has received significant research attention. Traditional ToM research has provided important understanding of how humans reason about mental states by utilizing shared world knowledge, social cues, and the interpretation of actions; however, many current behavioral paradigms are limited to static, "third-person" protocols. Emerging experimental approaches such as cognitive simulation and simulated social interaction offer opportunities to investigate ToM in interactive, "first-person" and "second-person" scenarios while affording greater experimental control. The advantages and limitations of traditional and emerging ToM methodologies are discussed with the intent of advancing the understanding of ToM in socially mediated situations.},
author = {Byom, Lindsey J. and Mutlu, Bilge},
doi = {10.3389/fnhum.2013.00413},
issn = {1662-5161},
journal = {Frontiers in Human Neuroscience},
keywords = {cognitive simulation,simulated social interaction,social cognition,social perception,theory of mind (ToM)},
number = {August},
pages = {413},
title = {{Theory of mind: mechanisms, methods, and new directions}},
url = {http://journal.frontiersin.org/article/10.3389/fnhum.2013.00413/abstract http://www.ncbi.nlm.nih.gov/pubmed/23964218 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3737477},
volume = {7},
year = {2013}
}
@article{Khoshelham2012,
abstract = {Consumer-grade range cameras such as the Kinect sensor have the potential to be used in mapping applications where accuracy requirements are less strict. To realize this potential insight into the geometric quality of the data acquired by the sensor is essential. In this paper we discuss the calibration of the Kinect sensor, and provide an analysis of the accuracy and resolution of its depth data. Based on a mathematical model of depth measurement from disparity a theoretical error analysis is presented, which provides an insight into the factors influencing the accuracy of the data. Experimental results show that the random error of depth measurement increases with increasing distance to the sensor, and ranges from a few millimeters up to about 4 cm at the maximum range of the sensor. The quality of the data is also found to be influenced by the low resolution of the depth measurements.},
author = {Khoshelham, Kourosh and Elberink, Sander Oude},
doi = {10.3390/s120201437},
issn = {1424-8220},
journal = {Sensors (Basel, Switzerland)},
keywords = {Equipment Design,Equipment Failure Analysis,Geographic Information Systems,Geographic Information Systems: instrumentation,Housing,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: instrumen,Image Interpretation, Computer-Assisted: methods,Maps as Topic,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Radar,Radar: instrumentation,Transducers},
month = {1},
number = {2},
pages = {1437--54},
title = {{Accuracy and resolution of Kinect depth data for indoor mapping applications.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3304120{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {12},
year = {2012}
}
@incollection{Rossano2009,
address = {Cambridge},
author = {Rossano, Federico and Brown, Penelope and Levinson, Stephen C.},
booktitle = {Conversation Analysis},
doi = {10.1017/CBO9780511635670.008},
editor = {Sidnell, Jack},
keywords = {culture,gaze},
number = {Rossano},
pages = {187--249},
publisher = {Cambridge University Press},
title = {{Gaze, Questioning, and Culture}},
url = {http://ebooks.cambridge.org/ref/id/CBO9780511635670A018 https://www.cambridge.org/core/product/identifier/CBO9780511635670A018/type/book{\_}part},
year = {2009}
}
@inproceedings{Leichsenring2016,
address = {Tokyo, Japan},
author = {Leichsenring, Christian and Yang, Jiajun and Hammerschmidt, Jan and Hermann, Thomas},
booktitle = {Workshop on Embodied Interaction with Smart Environments (EISE)},
doi = {10.1145/3008028.3008033},
isbn = {9781450345552},
keywords = {activities performed there but,ambient intelligence,maybe distinguished by the,not,smart environments,smart home,sonification,soundscape,tangible interaction},
pages = {1--7},
publisher = {ACM Press},
title = {{Challenges for Smart Environments in Bathroom Contexts}},
url = {http://dl.acm.org/citation.cfm?doid=3008028.3008033},
year = {2016}
}
@book{Altman,
author = {Altman, Douglag G.},
isbn = {0-412-27630-5},
title = {{Practical Statistics for Medical Research}},
year = {1991}
}
@inproceedings{Carlmeyer2014,
address = {New York, New York, USA},
author = {Carlmeyer, Birte and Schlangen, David and Wrede, Britta},
booktitle = {Workshop on Multimodal, Multi-Party, Real-World Human-Robot Interaction (MMRWHRI)},
doi = {10.1145/2666499.2666500},
isbn = {9781450305518},
keywords = {dialog management,hri,incremental processing,modal systems,multi-,spoken dialog},
pages = {1--6},
publisher = {ACM Press},
title = {{Towards Closed Feedback Loops in HRI}},
url = {http://dl.acm.org/citation.cfm?id=2666500 http://dl.acm.org/citation.cfm?doid=2666499.2666500},
year = {2014}
}
@inproceedings{Breazeal1999,
abstract = {This paper presents part of an on-going project to integrate perception, attention, drives, emo- tions, behavior arbitration, and expressive acts for a robot designed to interact socially with humans. We present the design of a visual at- tention system based on a model of human vi- sual search behavior from Wolfe (1994). The attention system integrates perceptions (mo- tion detection, color saliency, and face pop- outs) with habituation effects and influences from the robot's motivational and behavioral state to create a context-dependent attention activation map. This activation map is used to direct eye movements and to satiate the drives of the motivational system.},
annote = {Motivation: Salienz aus Menschlicher Aufmerksamkeitsforschung (Wolfe)

Feature:

* Farbe
* Bewegung
* Gesichter
+ Habituation

Gewichtung generiert verschiedene Verhaltensweisen},
author = {Breazeal, C. and Scassellati, B.},
booktitle = {International Joint Conference on Artificial Intelligence},
keywords = {attention},
title = {{A context-dependent attention system for a social robot}},
year = {1999}
}
@article{Member2002,
author = {Member, Senior and Ma, Topi},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {person tracking},
number = {7},
pages = {971--987},
title = {{Multiresolution Gray-Scale and Rotation Invariant Texture Classification with Local Binary Patterns}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1017623{\&}tag=1},
volume = {24},
year = {2002}
}
@inproceedings{Sun2010,
abstract = {Driving is a complex process influenced by a wide range of factors, especially complex interactions between the driver, the vehicle and the environment. This paper aims at the representation of complex situations in smart car domain and the middleware support. A general architecture of the smart car is presented from the point of view of context-awareness. A hierarchical context model is proposed for description of the complex driving environment. A software platform is built to provide the running environment for the context model and applications. Performance evaluation shows that our approach is effective and applicable in a smart car.},
author = {Sun, Jie and Zhang, Yongping and He, Kejia},
booktitle = {2010 10th IEEE International Conference on Computer and Information Technology},
doi = {10.1109/CIT.2010.47},
isbn = {978-1-4244-7547-6},
keywords = {-context modeling,middleware,smart car,ubiquitous},
month = {6},
number = {Cit},
pages = {13--19},
publisher = {IEEE},
title = {{Providing Context-awareness in the Smart Car Environment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5578420},
year = {2010}
}
@misc{Ascheberg2012,
author = {Ascheberg, Christian},
institution = {Universitaet Bielefeld},
number = {November},
title = {{Analyse unterschiedlicher 3D-Feature-Typen f{\"{u}}r das Tracking}},
year = {2012}
}
@article{Rusu2011,
author = {Rusu, Radu Bogdan and Cousins, Steve},
doi = {10.1109/ICRA.2011.5980567},
isbn = {978-1-61284-386-5},
journal = {2011 IEEE International Conference on Robotics and Automation},
month = {5},
pages = {1--4},
publisher = {Ieee},
title = {{3D is here: Point Cloud Library (PCL)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5980567},
year = {2011}
}
@article{Wrede2017,
abstract = {The emergence of cognitive interaction technology offering intuitive and personalized support for humans in daily routines is essential for the success of future smart environments. Social robotics and ambient assisted living are well-established, active research fields but in the real world the number of smart environments that support humans efficiently on a daily basis is still rather low. We argue that research on ambient intelligence and human-robot interaction needs to be conducted in a strongly interdisciplinary process to facilitate seamless integration of assistance technologies into the users' daily lives. With the Cognitive Service Robotics Apartment (CSRA), we are developing a novel kind of laboratory following this interdisciplinary approach. It combines a smart home with ambient intelligence functionalities with a cognitive social robot with advanced manipulation capabilities to explore the all day use of cognitive interaction technology for human assistance. This lab in conjunction with our development approach opens up new lines of inquiry and allows us to address new research questions in human-machine, -agent and -robot interaction.},
author = {Wrede, Sebastian and Leichsenring, Christian and Holthaus, Patrick and Hermann, Thomas and Wachsmuth, Sven},
doi = {10.1007/s13218-017-0492-x},
issn = {0933-1875},
journal = {K{\"{u}}nstliche Intelligenz (KI)},
keywords = {Human–machine interaction,Smart environments,Social robotics,Ubiquitous computing,human,machine interaction,smart environments,social robotics,ubiquitous computing},
month = {8},
number = {3},
pages = {299--304},
title = {{The Cognitive Service Robotics Apartment}},
url = {http://link.springer.com/10.1007/s13218-017-0492-x},
volume = {31},
year = {2017}
}
@inproceedings{Cao,
abstract = {Understanding human activities in video is a fundamental problem in computer vision. In real life, human activities are composed of temporal and spatial arrangement of ac- tions. Understanding such complex activities requires rec- ognizing not only each individual action, but more impor- tantly, capturing their spatio-temporal relationships. This paper addresses the problem of complex activity recognition with a unified hierarchical model. We expand triangular- chain CRFs (TriCRFs) to the spatial dimension. The pro- posed architecture can be perceived as a spatio-temporal version of the TriCRFs, in which the labels of actions and activity are modeled jointly and their complex dependencies are exploited. Experiments show that our model generates promising results, outperforming competing methods signif- icantly. The framework also can be applied to model other structured sequential data. Categories},
author = {Cao, Congqi and Zhang, Yifan and Lu, Hanqing},
booktitle = {Proceedings of the 23rd ACM International Conference on Multimedia},
doi = {10.1145/2733373.2806304},
isbn = {9781450334594},
keywords = {activity recognition,crf,hierarchical model,joint learning,spatio-,temporal dependencies,triangular-chain crfs,video},
pages = {1151--1154},
title = {{Spatio-Temporal Triangular-Chain CRF for Activity Recognition}},
year = {2015}
}
@article{Flammini2015,
abstract = {The use of smart-sensors to recognize automatically complex situations (anomalous behaviors, Abstract: The use of smart-sensors to recognize automatically complex situations (anomalous behaviors, Abstract: The use of smart-sensors to recognize automatically complex situations (anomalous behaviors, physical security threats, etc.) requires ‘intelligent' methods to improve the trustworthiness of automatic physical security threats, etc.) requires ‘intelligent' methods to improve the trustworthiness of automatic decisions. Voting and consensus mechanisms can be employed whether supported by probabilistic physical security threats, etc.) requires ‘intelligent' methods to improve the trustworthiness of automatic decisions. Voting and consensus mechanisms can be employed whether supported by probabilistic decisions. Voting and consensus mechanisms can be employed whether supported by probabilistic formalisms to correlate event occurrence, to merge local events and to estimate the likelihood of overall decisions. This paper presents the results of a quantitative comparison of three different voting schemes formalisms to correlate event occurrence, to merge local events and to estimate the likelihood of overall formalisms to correlate event occurrence, to merge local events and to estimate the likelihood of overall decisions. This paper presents the results of a quantitative comparison of three different voting schemes decisions. This paper presents the results of a quantitative comparison of three different voting schemes based on Bayesian Networks. These models present a growing complexity and they are able to provide a trustworthiness estimation based on single nodes detection reliability in terms of false alarm probabilities. based on Bayesian Networks. These models present a growing complexity and they are able to provide a based},
author = {Flammini, Francesco and Marrone, Stefano and Mazzocca, Nicola and Nardone, Roberto and Vittorini, Valeria},
doi = {10.1016/j.ifacol.2015.09.606},
issn = {24058963},
journal = {IFAC-PapersOnLine},
keywords = {2-out-of-3 Voting,Bayesian Networks,Cyber-Physical Protection Systems 7,Decision Fusion,Sensor Reliability,sensor fusion,situaion recognition,voting},
number = {21},
pages = {682--687},
publisher = {Elsevier Ltd.},
title = {{Using Bayesian Networks to evaluate the trustworthiness of ‘2 out of 3' decision fusion mechanisms in multi-sensor applications}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S2405896315017358},
volume = {48},
year = {2015}
}
@article{Portet2013,
abstract = {Smart homes equipped with ambient intelli- gence technology constitute a promising direction to enable the growing number of elderly to continue to live in theirown home as long as possible. However, this calls for techno- logical solutions that suit their specific needs and capabili- ties. The SWEET-HOME project aims at developing a new user friendly technology for home automation based on voice command. This paper reports a user evaluation assessing the acceptance and fear of this new technology. Eight healthy persons between 71 and 88 years old, 7 relatives (child, grandchild or friend) and 3 professional carers participated in a user evaluation. During about 45 min, the persons were questioned in co-discovery in the DOMUS smart home alter- nating between interview and wizard ofOz periods followed by a debriefing. The experience aimed at testing four important aspects of the project: voice command, commu- nication with the outside world, domotics system interrupt- ing a person's activity, and electronic agenda. Voice interface appeared to have a great potential to ease daily living for elderly and frail persons and would be better accepted than more intrusive solutions. By considering still healthy and independent elderly people in the user evalua- tion, an interesting finding that came up is their overall acceptance provided the system does not drive them to a lazy lifestyle by taking control of everything. This particular fear must be addressed for the development of smart homes that support daily living by giving them more ability to control rather than putting them away from the daily routine.},
author = {Portet, Fran{\c{c}}ois and Vacher, Michel and Golanski, Caroline and Roux, Camille and Meillon, Brigitte},
doi = {10.1007/s00779-011-0470-5},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {Gerontechnology,Smart home,Ubiquitous computing,User evaluation,Voice interface},
month = {1},
number = {1},
pages = {127--144},
title = {{Design and Evaluation of a Smart Home Voice Interface for the Elderly: Acceptability and Objection Aspects}},
url = {http://link.springer.com/10.1007/s00779-011-0470-5},
volume = {17},
year = {2013}
}
@inproceedings{Abadi2016,
address = {Savannah, GA},
author = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
booktitle = {Usenix Symposium on Operating Systems Design and Implementation (OSDI)},
isbn = {978-1-931971-33-1},
month = {11},
pages = {265--283},
publisher = {Usenix Association},
title = {{TensorFlow: A System for Large-Scale Machine Learning}},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
year = {2016}
}
@article{Rizzi2016,
abstract = {This work proposes a novel Situation-Aware FEar Learning (SAFEL) model for robots. SAFEL combines concepts of situation-aware expert systems with well-known neuroscientific findings on the brain fear-learning mechanism to allow companion robots to predict undesirable or threatening situations based on past experiences. One of the main objectives is to allow robots to learn complex temporal patterns of sensed environmental stimuli and create a representation of these patterns. This memory can be later associated with a negative or positive “emotion”, analogous to fear and confidence. Experiments with a real robot demonstrated SAFEL's success in generating contextual fear conditioning behaviour with predictive capabilities based on situational information.},
author = {Rizzi, Caroline and Johnson, Colin G. and Fabris, Fabio and Vargas, Patricia A.},
doi = {10.1016/j.neucom.2016.09.035},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Affective computing,Amygdala and hippocampus modeling,Autonomous robotics,Brain Emotional Learning,Contextual fear conditioning,Temporal pattern,brain emotional learning,contextual fear conditioning,emotions,fear,learninig,robotics,situation},
month = {1},
number = {September 2016},
pages = {32--47},
publisher = {Elsevier},
title = {{A Situation-Aware Fear Learning (SAFEL) model for robots}},
url = {http://dx.doi.org/10.1016/j.neucom.2016.09.035 http://linkinghub.elsevier.com/retrieve/pii/S0925231216310529},
volume = {221},
year = {2017}
}
@article{Kuhn1950,
author = {Kuhn, H. W.},
journal = {Naval Research Logistics Quarterly},
pages = {83--97},
title = {{The Hungarian method for the assignment problem}},
volume = {2},
year = {1950}
}
@article{Spexard2007,
abstract = {A very important aspect in developing robots capable of human-robot interaction (HRI) is the research in natural, human-like communication, and subsequently, the development of a research platform with multiple HRI capabilities for evaluation. Besides a flexible dialog system and speech understanding, an anthropomorphic appearance has the potential to support intuitive usage and understanding of a robot, e.g., human-like facial expressions and deictic gestures can as well be produced and also understood by the robot. As a consequence of our effort in creating an anthropomorphic appearance and to come close to a human- human interaction model for a robot, we decided to use human-like sensors, i.e., two cameras and two microphones only, in analogy to human perceptual capabilities too. Despite the challenges resulting from these limits with respect to perception, a robust attention system for tracking and interacting with multiple persons simultaneously in real time is presented. The tracking approach is sufficiently generic to work on robots with varying hardware, as long as stereo audio data and images of a video camera are available. To easily implement different interaction capabilities like deictic gestures, natural adaptive dialogs, and emotion awareness on the robot, we apply a modular integration approach utilizing XML-based data exchange. The paper focuses on our efforts to bring together different interaction concepts and perception capabilities integrated on a humanoid robot to achieve comprehending human-oriented interaction.},
author = {Spexard, Thorsten P. and Hanheide, Marc and Sagerer, Gerhard},
doi = {10.1109/TRO.2007.904903},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {Anthropomorphic robots,Human-robot interaction (HRI),Joint attention,Learning,Memory,Multimodal interaction,System integration},
month = {10},
number = {5},
pages = {852--862},
title = {{Human-Oriented Interaction With an Anthropomorphic Robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4339549 http://ieeexplore.ieee.org/document/4339549/},
volume = {23},
year = {2007}
}
@article{Loke2006,
abstract = {To create context-aware artifacts, developers can choose a self-supported or infrastructure-based approach. This article surveys these approaches and their implementations in various devices, also considering associated challenges and future research directions.},
author = {Loke, S.W.},
doi = {10.1109/MPRV.2006.27},
issn = {1536-1268},
journal = {IEEE Pervasive Computing},
month = {4},
number = {2},
pages = {48--53},
title = {{Context-Aware Artifacts: Two Development Approaches}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1626207},
volume = {5},
year = {2006}
}
@misc{Bei2018,
author = {Bei, Datenschutz and Echo, Amazon and Home, Google},
keywords = {amazon,datenschutz,echo,smart assistants},
title = {{Auf durchzug oder ganz ohr? Datenschutz Bei Amazon Echo Und Google Home}},
year = {2018}
}
@inproceedings{Sorensen2013,
address = {New York, New York, USA},
author = {S{\o}rensen, Henrik and Kristensen, Mathies G. and Kjeldskov, Jesper and Skov, Mikael B.},
booktitle = {Australian Computer-Human Interaction Conference on Augmentation, Application, Innovation, Collaboration (OzCHI)},
doi = {10.1145/2541016.2541046},
isbn = {9781450325257},
pages = {153--162},
publisher = {ACM Press},
title = {{Proxemic Interaction in a Multi-Room Music System}},
url = {http://dl.acm.org/citation.cfm?doid=2541016.2541046},
year = {2013}
}
@inproceedings{Akhtiamov2019,
abstract = {Acoustic addressee detection (AD) is a modern paralinguistic and dialogue challenge that especially arises in voice assistants. In the present study, we distinguish addressees in two settings (a conversation between several people and a spoken dialogue system, and a conversation between several adults and a child) and introduce the first competitive baseline (unweighted average recall equals 0.891) for the Voice Assistant Conversation Corpus that models the first setting. We jointly solve both classification problems, using three models: a linear support vector machine dealing with acoustic functionals and two neural networks utilising raw waveforms alongside with acoustic low-level descriptors. We investigate how different corpora influence each other, applying the mixup approach to data augmentation. We also study the influence of various acoustic context lengths on AD. Two-second speech fragments turn out to be sufficient for reliable AD. Mixup is shown to be beneficial for merging acoustic data (extracted features but not raw waveforms) from different domains that allows us to reach a higher classification performance on human-machine AD and also for training a multipurpose neural network that is capable of solving both human-machine and adult-child AD problems.},
address = {Stockholm, Sweden},
author = {Akhtiamov, Oleg and Siegert, Ingo and Karpov, Alexey and Minker, Wolfgang},
booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
month = {9},
pages = {274--283},
publisher = {Association for Computational Linguistics},
title = {{Cross-Corpus Data Augmentation for Acoustic Addressee Detection}},
url = {https://www.aclweb.org/anthology/W19-5933},
year = {2019}
}
@article{Auer2017,
abstract = {In this paper, I argue that gaze behavior in multiparty interaction is essential for two aspects of turn-taking: for addressee selection and for next-speaker selection by current speaker. The two conversational tasks are related, but – at least in longer turns – not identical and should be distinguished analytically. In multiparty interaction, addressee selection by gaze is a non-trivial issue, as most bodily arrangements make it hard or impossible for the current speaker to look at all (intended) addressees at the same time. Rather, current speakers alternatingly look at the co-participants they want to address. Further details of this pattern of gaze alternation are discussed.},
author = {Auer, Peter},
journal = {InLiSt - Interaction and Linguistic Structures},
keywords = {addressee selection,current speaker selects next,eye-tracking,gaze,multi-party interaction,multiparty,turn-taking,turn-taking and gaze},
number = {60},
title = {{Gaze, Addressee Selection and Turn-Taking in Three-Party Interaction}},
url = {http://www.inlist.uni-bayreuth.de/issues/60/index.htm},
year = {2017}
}
@inproceedings{Cha2016,
abstract = {{\textcopyright} 2016 ACM.We present Gleamy, a lamp supporting ambient information display in everyday environment by the shade with controllable transparency. Information can be displayed abstractly by static or animated patterns on the shade. A preliminary study with twelve users showed that Gleamy naturally blended in a home environment. It satisfied practicality both as an ambient display and as a lighting device. We found that representation of information by Gleamy was unobtrusive and informative. The changes of the pattern on the shade delivered information in an emotionally rich and private manner, and enhanced aesthetic value as well. In this paper, we discuss design considerations that could improve peripheral information display using the shade with changeable transparency.},
address = {New York, New York, USA},
author = {Cha, Seijin and Lee, Moon-Hwan and Nam, Tek-Jin},
booktitle = {International Conference on Tangible, Embedded, and Embodied Interaction (TEI)},
doi = {10.1145/2839462.2839501},
isbn = {9781450335829},
pages = {304--307},
publisher = {ACM Press},
title = {{Gleamy: An Ambient Display Lamp with a Transparency-Controllable Shade}},
url = {http://dl.acm.org/citation.cfm?doid=2839462.2839501},
year = {2016}
}
@inproceedings{Storf2009,
abstract = {One central challenge of Ambient Assisted Living systems is reliable recognition of the assisted person's current behavior, so that adequate assistance services can be offered in a specific situation. In the context of emergency sup- port, such a situation might be an acute emergency situation or a deviation from the usual behavior. To optimize prevention of emergencies, reliable recognition of characteristic Activities of Daily Living (ADLs) is promising. In this paper, we present our approach to processing information for the detection of ADLs in the EMERGE project. The approach is based on our multi-agent activity recog- nition framework EARS with its special definition language EARL. An evalua- tion with controlled experiments has proven its suitability.},
author = {Storf, Holger and Kleinberger, Thomas and Becker, Martin and Schmitt, Mario and Bomarius, Frank and Prueckner, Stephan},
booktitle = {European Conference, AmI 2009},
keywords = {activity recognition,ambient assisted,complex event processing,information processing,living,multi-agent systems},
pages = {123--132},
title = {{An Event-Driven Approach to Activity Recognition in Ambient Assisted Living}},
year = {2009}
}
@inproceedings{Grossvogt2018,
author = {Gro{\ss}-Vogt, Katharina and Weger, Marian and H{\"{o}}ldrich, Robert and Hermann, Thomas and Bovermann, Till and Reichmann, Stefan},
booktitle = {International Conference on Auditory Display (ICAD)},
pages = {105--112},
title = {{Augmentation of an Institute's Kitchen: an Ambient Auditory Display of Electric Power Consumption}},
url = {http://hdl.handle.net/1853/60088},
year = {2018}
}
@inproceedings{Checka2003,
abstract = {In this paper, we present a probabilistic tracking frame- work that combines sound and vision to achieve more robust and accurate tracking of multiple objects. In a cluttered or noisy scene, our measurements have a non-Gaussian, multi- modal distribution. We apply a particle filter to track mul- tiple people using combined audio and video observations. We have applied our algorithm to the domain of tracking people with a stereo-based visual foreground detection al- gorithm and audio localization using a beamforming tech- nique. Our model also accurately reflects the number of people present. We test the efficacy of our system on a se- quence of multiple people moving and speaking in an indoor environment.},
author = {Checka, Neal and Wilson, Kevin and Rangarajan, Vibhav and Darrell, Trevor},
booktitle = {2003 Conference on Computer Vision and Pattern Recognition Workshop},
doi = {10.1109/CVPRW.2003.10099},
keywords = {person tracking},
month = {6},
pages = {100--100},
publisher = {Ieee},
title = {{A Probabilistic Framework for Multi-modal Multi-Person Tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4624364},
year = {2003}
}
@book{Reeves1996,
address = {New York, NY, US},
author = {Reeves, Byron and Nass, Clifford},
isbn = {9781575860534},
publisher = {Cambridge University Press},
title = {{The Media Equation - How People Treat Computers, Television, and New Media Like Real People and Places}},
year = {1996}
}
@inproceedings{Papazov2011,
abstract = {In this paper, we present an efficient algorithm for 3D object recognition in presence of clutter and occlusions in noisy, sparse and unsegmented range data. The method uses a robust geometric descriptor, a hashing technique and an efficient RANSAC-like sampling strategy. We assume that each object is represented by a model consisting of a set of points with corresponding surface normals. Our method recognizes multiple model instances and estimates their position and orientation in the scene. The algorithm scales well with the number of models and its main procedure runs in linear time in the number of scene points. Moreover, the approach is conceptually simple and easy to implement. Tests on a variety of real data sets show that the proposed method performs well on noisy and cluttered scenes in which only small parts of the objects are visible.},
author = {Papazov, Chavdar and Burschka, Darius},
booktitle = {Proceedings of the 10th Asian conference on Computer vision - Volume Part I},
number = {ii},
pages = {135--148},
title = {{An Efficient RANSAC for 3D Object Recognition in Noisy and Occluded Scenes}},
url = {http://www6.in.tum.de/Main/Publications/Papazov2010.pdf},
year = {2011}
}
@article{Matsuyama2015,
abstract = {In this paper, we present a framework for facilitation robots that regulate imbalanced engagement density in a four-participant conversation as the forth participant with proper procedures for obtaining initiatives. Four is the special number in multiparty conversations. In three-participant conversations, the minimum unit for multiparty conversations, social imbalance, in which a participant is left behind in the current conversation, sometimes occurs. In such scenarios, a conversational robot has the potential to objectively observe and control situations as the fourth participant. Consequently, we present model procedures for obtaining conversational initiatives in incremental steps to harmonize such four-participant conversations. During the procedures, a facilitator must be aware of both the presence of dominant participants leading the current conversation and the status of any participant that is left behind. We model and optimize these situations and procedures as a partially observable Markov decision process (POMDP), which is suitable for real-world sequential decision processes. The results of experiments conducted to evaluate the proposed procedures show evidence of their acceptability and feeling of groupness.},
author = {Matsuyama, Yoichi and Akiba, Iwao and Fujie, Shinya and Kobayashi, Tetsunori},
doi = {10.1016/j.csl.2014.12.001},
issn = {08852308},
journal = {Computer Speech {\&} Language},
keywords = {Conversational strategy,Engagement density control,Facilitation robot,Markov decision process,Multimodal processing,Multiparty conversation,Partially observable},
month = {9},
number = {1},
pages = {1--24},
title = {{Four-Participant Group Conversation: A Facilitation Robot Controlling Engagement Density as the Fourth Participant}},
url = {http://dx.doi.org/10.1016/j.csl.2014.12.001 http://linkinghub.elsevier.com/retrieve/pii/S0885230814001260 https://linkinghub.elsevier.com/retrieve/pii/S0885230814001260},
volume = {33},
year = {2015}
}
@inproceedings{rsb,
abstract = {This paper presents the Robotics Service Bus (RSB), a new message-oriented, event-driven middleware based on a logically unified bus with hierarchical structure. Major goals for the development of RSB were openness and scalability in order to integrate diverse components in the context of robotics and intelligent systems. This includes the ability to operate on embedded platforms as well as desktop computers, reduction of framework lock-in, and the integration with other middlewares. We describe the design of the RSB middleware and explain how it meets requirements which lead to a scalable and open middleware concept. These requirements are based on several application scenarios which are used to verify the applicability of RSB. Furthermore, we relate RSB to other middlewares in the robotics domain.},
author = {Wienke, Johannes and Wrede, Sebastian},
booktitle = {International Symposium on System Integration (SII)},
doi = {10.1109/SII.2011.6147617},
isbn = {978-1-4577-1524-2},
month = {12},
pages = {1183--1190},
publisher = {IEEE},
title = {{A Middleware for Collaborative Research in Experimental Robotics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6147617 http://ieeexplore.ieee.org/document/6147617/},
year = {2011}
}
@inproceedings{Huang2011,
abstract = {In multi-user human-agent interaction, the agent should respond to the user when an utterance is addressed to it. To do this, the agent needs to be able to judge whether the utterance is addressed to the agent or to another user. This study proposes a method for estimating the addressee based on the prosodic features of the user's speech and head direction (approximate gaze direction). First, a WOZ experiment is conducted to collect a corpus of human-humanagent triadic conversations. Then, analysis is performed to find out whether the prosodic features as well as head direction information are correlated with the addressee-hood. Based on this analysis, a SVM classifier is trained to estimate the addressee by integrating both the prosodic features and head movement information. Finally, a prototype agent equipped with this real-time addressee estimation mechanism is developed and evaluated. {\textcopyright} 2011 ACM.},
address = {New York, New York, USA},
author = {Huang, Hung-Hsuan and Baba, Naoya and Nakano, Yukiko},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/2070481.2070557},
isbn = {9781450306416},
pages = {401},
publisher = {ACM Press},
title = {{Making Virtual Conversational Agent Aware of the Addressee of Users' Utterances in Multi-User Conversation using Nonverbal Information}},
url = {http://dl.acm.org/citation.cfm?doid=2070481.2070557},
year = {2011}
}
@inproceedings{Bauer2010,
abstract = {From the advances in computer vision methods for the detection, tracking and recognition of objects in video streams, new opportunities for video surveillance arise: In the future, automated video surveillance systems will be able to detect critical situations early enough to enable an operator to take preventive actions, instead of using video material merely for forensic investigations. However, problems such as limited computational resources, privacy regulations and a constant change in potential threads have to be addressed by a practical automated video surveillance system. In this paper, we show how these problems can be addressed using a task-oriented approach. The system architecture of the task-oriented video surveillance system NEST and an algorithm for the detection of abnormal behavior as part of the system are presented and illustrated for the surveillance of guests inside a video-monitored building. {\textcopyright} 2010 Copyright SPIE - The International Society for Optical Engineering.},
author = {Bauer, Alexander and Fischer, Yvonne},
booktitle = {Proceedings of SPIE - The International Society for Optical Engineering},
doi = {10.1117/12.849646},
editor = {Buford, John F. and Jakobson, Gabriel and Erickson, John and Tolone, William J. and Ribarsky, William},
isbn = {0277786X (ISSN); 9780819481733 (ISBN)},
issn = {0277-786X},
keywords = {Abnormal behavior,Automated video surveillance,Computational resources,Computer crime,Computer hardware description languages,Computer vision,Forensic investigation,Mathematical operators,Monitoring,New opportunities,Preventive action,Privacy regulation,Security of data,Security systems,Syntactic pattern recognition,Syntactics,System architectures,Video cameras,Video material,Video streams,Video surveillance,Video surveillance systems,Visualization,abnormal behavior detection,situation recognition},
month = {4},
number = {0},
pages = {770906--770906--9},
title = {{Task-oriented situation recognition}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1345120 http://www.scopus.com/inward/record.url?eid=2-s2.0-79953699368{\&}partnerID=40{\&}md5=0f6cabf599c4e4b22c1ed485e57500b9},
volume = {7709},
year = {2010}
}
@inproceedings{elan,
author = {Brugman, Hennie and Russel, Albert},
booktitle = {International Conference on Language Resources and Language Evaluation (LREC)},
number = {October},
pages = {2065--2068},
title = {{Annotating Multi-Media / Multi-Modal Resources with ELAN}},
year = {2009}
}
@inproceedings{Tran2012,
abstract = {Understanding of traffic situations is an essential part of future advanced driver assistance systems (ADAS). This has to handle spatio-temporal dependencies of multiple traffic participants and uncertainties from different sources. Most existing approaches use probabilistic generative joint structures like HiddenMarkovModels (HMM), which have long been used for dealing with activity recognition problems. Two significant limitations of these models are the assumption of conditional independence of observations and the availability of prior infor- mation. In this study, we present a probabilistic discriminative approach based on undirected probabilistic graphical models (Markov Networks). We combine two well-studied models: the log-linear model and the Conditional Random Field (CRF), which use dynamic programming for efficient, exact inference and their parameters can be learned via convex optimization. Since CRF conditions on entire observation sequences, we can avoid the requirement of independence between observations. Additionally, with discriminative models prior information of each activity is not necessary when performing a classification step. These two advantages of the discriminative models are very useful for our focusing problem of traffic scene un- derstanding. We evaluate our approach with real data and show that it is able to recognize different driving maneuvers occurring at an urban intersection.},
author = {Tran, Quan and Firl, Jonas},
booktitle = {2012 IEEE Intelligent Vehicles Symposium},
doi = {10.1109/IVS.2012.6232279},
isbn = {978-1-4673-2118-1},
keywords = {CRF,Decision and Expert Systems,Driver Assistance Systems,Vehicle Environment Perception,activity recognition,car assistance,prediction,sensor fusion,temporal},
month = {6},
pages = {147--152},
publisher = {IEEE},
title = {{A probabilistic discriminative approach for situation recognition in traffic scenarios}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6232279},
year = {2012}
}
@book{baker2002matrix,
author = {Baker, A},
isbn = {9781852334703},
publisher = {Springer},
series = {Springer undergraduate mathematics series},
title = {{Matrix Groups: An Introduction to Lie Group Theory}},
url = {http://books.google.de/books?id=KzH0y4esAfIC},
year = {2002}
}
@book{kendon1990,
address = {Cambridge},
author = {Kendon, Adam},
edition = {Studies in},
editor = {Gumperz, John J.},
isbn = {0521389380},
publisher = {Press Syndicate of the University of Cambridge},
series = {Studies in Interactional Socio},
title = {{Conducting Interaction: Patterns of Behavior in Focused Encounters}},
url = {https://books.google.de/books?id=7-8zAAAAIAAJ},
year = {1990}
}
@article{Jokinen2013,
abstract = {Eye gaze is an important means for controlling interaction and coordinating the participants' turns smoothly. We have studied how eye gaze correlates with spoken interaction and especially focused on the combined effect of the speech signal and gazing to predict turn taking possibilities. It is well known that mutual gaze is important in the coordination of turn taking in two-party dialogs, and in this article, we investigate whether this fact also holds for three-party conversations. In group interactions, it may be that different features are used for managing turn taking than in two-party dialogs.We collected casual conversational data and used an eye tracker to systematically observe a participant's gaze in the interactions. By studying the combined effect of speech and gaze on turn taking, we aimed to answer our main questions: How well can eye gaze help in predicting turn taking? What is the role of eye gaze when the speaker holds the turn? Is the role of eye gaze as important in three-party dialogs as in two-party dialogue? We used Support Vector Machines (SVMs) to classify turn taking events with respect to speech and gaze features, so as to estimate how well the features signal a change of the speaker or a continuation of the same speaker. The results confirm the earlier hypothe- sis that eye gaze significantly helps in predicting the partner's turn taking activity, and wealso get supporting evidence for our hypothesis that the speaker is a prominent coordinator of the interaction space. Such a turn taking model could be used in interactive applications to improve the system's conversational performance.},
author = {Jokinen, Kristiina and Furukawa, Hirohisa and Nishida, Masafumi and Yamamoto, Seiichi},
doi = {10.1145/2499474.2499481},
issn = {21606455},
journal = {ACM Transactions on Interactive Intelligent Systems},
keywords = {dialog,gaze,turn-taking},
month = {7},
number = {2},
pages = {1},
title = {{Gaze and turn-taking behavior in casual conversational interactions}},
url = {http://dl.acm.org/citation.cfm?doid=2499474.2499481},
volume = {3},
year = {2013}
}
@inproceedings{Yen2015,
abstract = {The physical world is becoming smarter and smarter due to the advances in smart devices and CPS/IoT technologies. In this paper, we investigate the roles various cutting-edge technologies, such as service computing, big data analytics, crowd sourcing, gaming technologies, etc., can play to significantly enhance the intelligence of our physical environment and subsequently benefit the society. We consider a smart physical world (SPW) consisting of physical entities, cyber entities, and human. Service technologies can be used to model the entities in SPW and specify their capabilities. Service discovery and composition techniques can be used to, based on the modeling, compose these capabilities to solve real-world problems. To enable higher level intelligence, we further discuss how various technologies, such as big data analytics and artificial intelligence, can be used in the smart world to reason from sensor inputs to derive the situation facts, and from the situation facts to derive the reactive actions. From the derived actions (tasks), the service technologies can be used to compose the capabilities of the entities to realize the task. However, current technologies and artificial intelligence may fall short in many situations. We further propose a gaming-based crowd sourcing platform to make use of human intelligence to enable successful completion of some challenging reasoning and control tasks.},
author = {Yen, I-Ling and Zhou, Guang and Zhu, Wei and Bastani, Farokh and Hwang, San-Yih},
booktitle = {2015 IEEE International Conference on Web Services},
doi = {10.1109/ICWS.2015.111},
isbn = {978-1-4673-7272-5},
keywords = {big data,crowd,cyber physical systems,gaming platforms,internet of things,iot,reasoning,service,service computing,smart physical world,sourcing},
month = {6},
pages = {765--772},
publisher = {IEEE},
title = {{A Smart Physical World Based on Service Technologies, Big Data, and Game-Based Crowd Sourcing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7195646},
year = {2015}
}
@book{Pfeffer,
author = {Pfeffer, Avi and Ruttenberg, Brian and Howard, Michael and O'Connor, Alison},
keywords = {framework,probabilistic reasoning,scala,tutorial},
publisher = {Charles River Analytics},
title = {{Figaro Tutorial}}
}
@inproceedings{Yumak2013,
abstract = {Research on interactive virtual characters and social robots focuses mainly on one-to-one interactions and multi-party interactions con- cept are rather less explored. As we are developing these characters to be helpful to us in our daily lives as guides, companions, assis- tants or receptionists, they should be aware of the existence of mul- tiple people and address their requirements in a natural way and act according to the social rules and norms. In contrast with previous work, we are interested in multi-party and multi-modal interactions between 3D virtual characters, real humans and social robots. This means that any of these participants can interact with each other. In this paper we present our on-going work, provide a discussion on multi-party interaction, describe the overall system architecture and mention our future work.},
address = {New York, New York, USA},
author = {Yumak, Zerrin and Magnenat-Thalmann, Nadia},
booktitle = {Proceedings of the 19th ACM Symposium on Virtual Reality Software and Technology - VRST '13},
doi = {10.1145/2503713.2503736},
isbn = {9781450323796},
keywords = {hai,hri,mmi,multi-modal interaction,multi-party interaction,multiparty,social robots,socially interactive virtual characters},
pages = {153},
publisher = {ACM Press},
title = {{Multi-party interaction with a virtual character and a human-like robot}},
url = {http://doi.acm.org/10.1145/2503713.2503736 http://dl.acm.org/citation.cfm?doid=2503713.2503736},
year = {2013}
}
@techreport{Amon2015,
abstract = {This report discusses the mental processes that govern the way people make decisions and react to their environment, with an emphasis on emergency incidents. Among the various methods of predicting the behaviour of different groups of people there is a dominant concept that can be applied to all groups of people that are affected by an incident and whose actions may have an effect on the progression of the incident; this concept is called dual process theory. In dual process theory, decision making is divided into System 1 (fast, automatic responses to stimuli) and System 2 (slow, deliberate) thinking processes. Additional},
author = {Amon, Francine and Lindstr{\"{o}}m, Johan and Lindstr{\"{o}}m, Peder and Lange, David and Svensson, Stefan and Ronchi, Enrico and Criel, Xavier},
keywords = {decision making,dual process theory,human},
pages = {1--62},
title = {{Effects of human activities on the progression and development of large scale crises Table of Contents}},
year = {2015}
}
@article{Kurschl2008,
abstract = {Due to increasing anticipated average life and health ex- penditure ambient assisted living (AAL) systems attract the attention of researchers. To successfully build and deploy AAL systems knowledge from different fields of computer science is needed: pervasive computing to gain the raw data, machine learning and pattern recognition to interpret these data and HCI knowledge to allow implicit interaction with the system. In this paper we propose a reference architecture for building AAL systems. Based on this reference architec- ture we introduce a toolbox that simplifies the development of AAL systems. The toolbox consists of a meta-model for pipeline systems, a low-level context model, high-level con- text ontologies, customizable components and tool support.},
author = {Kurschl, Werner and Mitsch, Stefan and Schoenboeck, Johannes},
doi = {10.1109/BROADCOM.2008.36},
isbn = {978-1-4244-3281-3},
journal = {2008 Third International Conference on Broadband Communications, Information Technology {\&} Biomedical Applications},
pages = {110--116},
publisher = {Ieee},
title = {{An Engineering Toolbox to Build Situation Aware Ambient Assisted Living Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4696094},
year = {2008}
}
@inproceedings{Varadarajan2011,
abstract = {Building upon the paradigm of Recognition by Components (RBC), which has been one of the conceptually most significant frameworks for modeling human visual object recognition, Recognition by Component Affordances (RBCA) has been recently introduced in the context of grasping. Practical applicability of the model have been traditionally limited by the lack of good response in texture-less areas in the case of conventional inexpensive stereo cameras as well as by the need for expensive laser based sensor systems to compensate for this deficiency. The recent availability of RGB-D sensors such as the PrimeSense sensor has opened new avenues for practical usage of these sensors for robotic applications such as grasping. In this paper, we present our recent research summary and work in progress for segmentation of objects and parts from range images based on semantic cues to yield robust part detection. This is followed by part parameterization using superquadric fitting and classification or component identification. A hierarchy of rules for grasping each classified shape primitive provides a scalable framework for grasping of objects, including novel ones in the scene.},
author = {Varadarajan, Karthik Mahesh and Zhou, Kai and Vincze, Markus},
booktitle = {Proceedings of the 2011 Robotics: Science and Systems Conference},
title = {{Holistic Shape Detection and Analysis using RGB-D Range Data for Grasping}},
url = {http://www.google.de/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=1{\&}ved=0CC8QFjAA{\&}url=http{\%}3A{\%}2F{\%}2Fwww.cs.washington.edu{\%}2Fai{\%}2FMobile{\_}Robotics{\%}2Frgbd-workshop-2011{\%}2Fcamera{\_}ready{\%}2Fvaradarajan-rgbd11-grasping.pdf{\&}ei=Hs4VUqnMCoPNtAbd4oGoDQ{\&}usg=AFQjCNHYzG9N-PLkzwqc-ulX3Sz8SSNSGQ{\&}sig2=gelE7NyK{\_}By6g0p-1sZ4pw{\&}bvm=bv.51156542,d.Yms{\&}cad=rja},
year = {2011}
}
@inproceedings{Bilinski2013,
abstract = {This paper addresses the problem of recognizing human actions in video sequences for home care applications. Recent studies have shown that approaches which use a bag- of-words representation reach high action recognition accuracy. Unfortunately, these approaches have problems to discriminate similar actions, ignoring spatial information of features. As we focus on recognizing subtle differences in behaviour of patients, we propose a novel method which significantly enhances the discriminative properties of the bag-of-words technique. Our approach is based on a dynamic coordinate system, which introduces spatial information to the bag-of-words model, by computing relative tracklets. We perform an extensive evalua- tion of our approach on three datasets: popular KTH dataset, challenging ADL dataset and our collected Hospital dataset. Experiments show that our representation enhances the dis- criminative power of features and bag-of-words model, bringing significant improvements in action recognition performance.},
author = {Bilinski, Piotr and Bremond, Francois and Lucioles, Route and Antipolis, Sophia and Corvee, Etienne and Bak, Slawomir},
booktitle = {Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops},
keywords = {activity recognition,person tracking},
pages = {1--7},
title = {{Relative Dense Tracklets for Human Action Recognition}},
url = {https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6553699{\&}sortType{\%}3Dasc{\_}p{\_}Sequence{\%}26filter{\%}3DAND{\%}28p{\_}IS{\_}Number{\%}3A6553693{\%}29},
year = {2013}
}
@inproceedings{Takayama2016,
author = {Takayama, Leila and Pantofaru, Caroline},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2009.5354145},
isbn = {978-1-4244-3803-7},
month = {10},
number = {NOVEMBER 2009},
pages = {5495--5502},
publisher = {IEEE/RSJ},
title = {{Influences on Proxemic Behaviors in Human-Robot Interaction}},
url = {http://ieeexplore.ieee.org/document/5354145/},
year = {2009}
}
@article{Bailenson2001,
abstract = {During the last half of the twentieth century, psychologists and anthropologists have studied proxemics, or spacing behavior, among people in many contexts. As we enter the twenty-first century, immersive virtual environment technology promises new experimental venues in which researchers can study proxemics. Immersive virtual environments provide realistic and compelling experimental settings without sacrificing experimental control. The experiment reported here tested Argyle and Dean's (1965) equilibrium theory specification of an inverse relationship between mutual gaze, a non-verbal cue signaling intimacy, and interpersonal distance. Participants were immersed in a three-dimensional virtual room in which a virtual human representation (i.e., an embodied agent) stood. Under the guise of a memory task, participants walked towards and around the agent. Distance between the participant and agent was tracked automatically via our immersive virtual environment system. All participants maintained more space around agents than around similarly sized and shaped but non-human like objects. Female participants maintained more interpersonal distance between themselves and agents who engaged them in eye contact (i.e., mutual gaze behavior) than agents who did not engage them in eye contact while male participants did not. Implications are discussed for the study of proxemics via immersive virtual environment technology as well as the design of virtual environments and virtual humans},
archivePrefix = {arXiv},
author = {Bailenson, Jeremy N. and Blascovich, Jim and Beall, Andrew C. and Loomis, Jack M.},
doi = {10.1162/105474601753272844},
eprint = {arXiv:1011.1669v3},
issn = {10547460},
journal = {Presence: Teleoperators and Virtual Environments},
number = {6},
pages = {583--598},
title = {{Equilibrium Theory Revisited: Mutual Gaze and Personal Space in Virtual Environments}},
volume = {10},
year = {2001}
}
@article{Tian2001,
abstract = {An incremental method for learning Bayesian networks based on evolutionary computing, IEMA, is put forward. IEMA introduces the evolutionary algorithm and EM algorithm into the process of incremental learning; it can avoid getting into local maxima, and also incrementally learn Bayesian networks with high accuracy in the presence of missing values and hidden variables. In addition, we improved the incremental learning process by N. Friedman and M. Goldschmidt (1997). The experimental results verified the validity of IEMA. In terms of storage cost, IEMA is comparable with the incremental learning method of Friedman et al, while it is more accurate.},
author = {Tian, Fengzhan Tian Fengzhan and Zhang, Hongwei Zhang Hongwei and Lu, Yuchang Lu Yuchang and Shi, Chunyi Shi Chunyi},
doi = {10.1109/ICDM.2001.989594},
isbn = {0-7695-1119-8},
journal = {Proceedings 2001 IEEE International Conference on Data Mining},
keywords = {Bayesian Network,em,evolutionary algorithm,learning},
pages = {0--1},
title = {{Incremental learning of Bayesian networks with hidden variables}},
year = {2001}
}
@article{Lerner2003,
abstract = {This report extends earlier context-free treatments of turn-taking for con- versation by describing the context-sensitive operation of the principal forms of addressing employed by current speakers to select next speakers. It first describes the context-specific limitations of gaze-directional addressing, and the selective deployment and more-than-addressing action regularly accom- plished by address terms (most centrally, names). In addition to these ex- plicit methods of addressing, this report introduces tacit forms of addressing that call on the innumerable context-specific particulars of circumstance, content, and composition to select a next speaker.},
author = {Lerner, Gene H.},
doi = {10.1017/S004740450332202X},
isbn = {0047-4045 U6 - ctx{\{}{\_}{\}}ver=Z39.88-2004{\{}{\&}{\}}ctx{\{}{\_}{\}}enc=info{\{}{\%}{\}}3Aofi{\{}{\%}{\}}2Fenc{\{}{\%}{\}}3AUTF-8{\{}{\&}{\}}rfr{\{}{\_}{\}}id=info:sid/summon.serialssolutions.com{\{}{\&}{\}}rft{\{}{\_}{\}}val{\{}{\_}{\}}fmt=info:ofi/fmt:kev:mtx:journal{\{}{\&}{\}}rft.genre=article{\{}{\&}{\}}rft.atitle=Selecting+next+speaker{\{}{\%}{\}}3A+The+context-sensitive+operation+of+a+context-free+organization{\{}{\&}{\}}rft.jtitle=Language+in+Society{\{}{\&}{\}}rft.au=LERNER{\{}{\%}{\}}2C+GENE+H{\{}{\&}{\}}rft.date=2003-04-01{\{}{\&}{\}}rft.pub=Cambridge+University+Press{\{}{\&}{\}}rft.issn=0047-4045{\{}{\&}{\}}rft.volume=32{\{}{\&}{\}}rft.issue=2{\{}{\&}{\}}rft.spage=177{\{}{\&}{\}}rft.epage=20},
issn = {0047-4045},
journal = {Language in Society},
keywords = {Context,Conversation,Turn allocation,Turn-taking},
month = {4},
number = {02},
pages = {177--201},
title = {{Selecting next speaker: The context-sensitive operation of a context-free organization}},
url = {http://www.journals.cambridge.org/abstract{\_}S004740450332202X},
volume = {32},
year = {2003}
}
@inproceedings{Andrist2014,
abstract = {Gaze aversion-the intentional redirection away from the face of an interlocutor-is an important nonverbal cue that serves a number of conversational functions, including signaling cognitive effort, regulating a conversation's intimacy level, and managing the conversational floor. In prior work, we developed a model of how gaze aversions are employed in conversation to perform these functions. In this paper, we extend the model to apply to conversational robots, enabling them to achieve some of these functions in conversations with people. We present a system that addresses the challenges of adapting human gaze aversion movements to a robot with very different affordances, such as a lack of articulated eyes. This system, implemented on the NAO platform, autonomously generates and combines three distinct types of robot head movements with different purposes: face-tracking movements to engage in mutual gaze, idle head motion to increase lifelikeness, and purposeful gaze aversions to achieve conversational functions. The results of a human-robot interaction study with 30 participants show that gaze aversions implemented with our approach are perceived as intentional, and robots can use gaze aversions to appear more thoughtful and effectively manage the conversational floor.},
address = {New York, New York, USA},
annote = {look into [3]},
author = {Andrist, Sean and Tan, Xiang Zhi and Gleicher, Michael and Mutlu, Bilge},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1145/2559636.2559666},
isbn = {9781450326582},
keywords = {gaze,turn-taking},
pages = {25--32},
publisher = {ACM Press},
title = {{Conversational Gaze Aversion for Humanlike Robots}},
url = {http://dl.acm.org/citation.cfm?doid=2559636.2559666},
year = {2014}
}
@inproceedings{Schillingmann2015,
abstract = {The recognition of gaze as for example mutual gaze plays an important role in social interaction. Previous research shows that already infants are capable of detecting mutual gaze. Such abilities are relevant for robots to learn from interaction, for example detecting when the robot is being addressed. Although various gaze tracking methods have been proposed, few seem to be openly available for robotic platforms such as iCub. In this paper we will describe a gaze tracking system for humanoid robots that is completely based on freely available libraries and data sets. Our system is able to estimate horizontal and vertical gaze directions using low resolution VGA images from robot embodied vision at 30 ? frames per second. For this purpose we developed a pupil detection algorithm combining existing approaches to increase ? noise robustness. Our method combines positions of the face and ? eye features as well as context features such as eyelid correlates and thus does not rely on fixed head orientations. An evaluation ? on the iCub robot shows that our method is able to estimate mutual gaze with 96{\%} accuracy at 8◦ tolerance and one meter ? distance to the robot. The results further support that mutual gaze detection yields higher accuracy in an embodied setup ? compared to other configurations},
author = {Schillingmann, Lars and Nagai, Yukie},
booktitle = {International Conference on Humanoid Robots (Humanoids)},
doi = {10.1109/HUMANOIDS.2015.7363515},
isbn = {978-1-4799-6885-5},
keywords = {gaze detection,icub},
month = {11},
pages = {8--13},
publisher = {IEEE-RAS},
title = {{Yet Another Gaze Detector: An Embodied Calibration Free System for the iCub Robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7363515 http://ieeexplore.ieee.org/document/7363515/},
year = {2015}
}
@article{Pfeffer2016,
abstract = {Reasoning on large and complex real-world models is a computationally difficult task, yet one that is required for effective use of many AI applications. A plethora of inference algorithms have been developed that work well on specific models or only on parts of general models. Consequently, a system that can intelligently apply these inference algorithms to different parts of a model for fast reasoning is highly desirable. We introduce a new framework called structured factored inference (SFI) that provides the foundation for such a system. Using models encoded in a probabilistic programming language, SFI provides a sound means to decompose a model into sub-models, apply an inference algorithm to each sub-model, and combine the resulting information to answer a query. Our results show that SFI is nearly as accurate as exact inference yet retains the benefits of approximate inference methods.},
archivePrefix = {arXiv},
author = {Pfeffer, Avi and Ruttenberg, Brian and Kretschmer, William},
eprint = {1606.03298},
keywords = {probabilistic programming,reasoning},
month = {6},
title = {{Structured Factored Inference: A Framework for Automated Reasoning in Probabilistic Programming Languages}},
url = {http://arxiv.org/abs/1606.03298},
year = {2016}
}
@article{Vogeley2010,
abstract = {"Artificial humans", so-called "Embodied Conversational Agents" and humanoid robots, are assumed to facilitate human-technology interaction referring to the unique human capacities of interpersonal communication and social information processing. While early research and development in artificial intelligence (AI) focused on processing and production of natural language, the "new AI" has also taken into account the emotional and relational aspects of communication with an emphasis both on understanding and production of nonverbal behavior. This shift in attention in computer science and engineering is reflected in recent developments in psychology and social cognitive neuroscience. This article addresses key challenges which emerge from the goal to equip machines with socio-emotional intelligence and to enable them to interpret subtle nonverbal cues and to respond to social affordances with naturally appearing behavior from both perspectives. In particular, we propose that the creation of credible artificial humans not only defines the ultimate test for our understanding of human communication and social cognition but also provides a unique research tool to improve our knowledge about the underlying psychological processes and neural mechanisms. {\textcopyright} 2010.},
archivePrefix = {arXiv},
author = {Vogeley, Kai and Bente, Gary},
doi = {10.1016/j.neunet.2010.06.003},
eprint = {arXiv:1011.1669v3},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Artificial humans,Nonverbal communication,Social cognitive neuroscience,Social psychology,embodiment,hai,nonverbal},
month = {10},
number = {8-9},
pages = {1077--1090},
publisher = {Elsevier Ltd},
title = {{“Artificial humans”: Psychology and neuroscience perspectives on embodiment and nonverbal communication}},
url = {http://dx.doi.org/10.1016/j.neunet.2010.06.003 http://linkinghub.elsevier.com/retrieve/pii/S0893608010001164},
volume = {23},
year = {2010}
}
@book{Freeman2011,
author = {Freeman, Eric and Robson, Elisabeth},
keywords = {head-first,programming,web},
pages = {610},
title = {{Head First: HTML5 Programming}},
year = {2011}
}
@article{elanpaper,
author = {Brugman, Hennie and Russel, Albert},
journal = {Proceedings of the 4th International Conference on Language Resources and Language Evaluation (LREC 2004)},
number = {October},
pages = {2065--2068},
title = {{Annotating multi-media / multi-modal resources with ELAN}},
year = {2009}
}
@article{Endsley2015,
abstract = {Situation awareness (SA) has become a widely used construct within the human factors community, the focus of considerable research over the past 25 years. This research has been used to drive the development of advanced information displays, the design of automated systems, information fusion algorithms, and new training approaches for improving SA in individuals and teams. In recent years, a number of papers criticized the Endsley model of SA on various grounds. I review those criticisms here and show them to be based on misunderstandings of the model. I also review several new models of SA, including situated SA, distributed SA, and sensemaking, in light of this discussion and show how they compare to existing models of SA in individuals and teams.},
author = {Endsley, M. R.},
doi = {10.1177/1555343415572631},
isbn = {1555343415572},
issn = {1555-3434},
journal = {Journal of Cognitive Engineering and Decision Making},
keywords = {a substan-,and measurement have formed,design,ing,memory,sa,sensemaking,situation assessment,situation awareness,team situation awareness,theory,train-,working},
number = {1},
pages = {4--32},
title = {{Situation Awareness Misconceptions and Misunderstandings}},
url = {http://edm.sagepub.com/lookup/doi/10.1177/1555343415572631},
volume = {9},
year = {2015}
}
@article{Monga1990,
abstract = {This paper deals with edge detection in 3d images such as scanner, magnetic resonance (NMR), or spatio-temporal data. We propose an unified formalism form 3d edge detection using optimal, recursive and separable filters recently introduced for 2d edge detection. Then we obtain some efficient 3d edge detection algorithms having a low comptutational cost. We also show that 3d edge closing enable to extract many edges not provided by the filtering stage without introducing noisy edges. Experimental results obtained on NMR images are shown.},
author = {Monga, Olivier and Deriche, Rachid and Malandain, Gr'egoire and Cocquerez, Jean Pierre},
journal = {Computer Vision — ECCV 90},
pages = {56--65},
title = {{Recursive filtering and edge closing: Two primary tools for 3d edge detection}},
year = {1990}
}
@inproceedings{Pizzagalli2018,
abstract = {{\textcopyright} 2018 IEEE. This paper presents the Home Interactive Interface (HIC), a ubiquitous projected GUI providing the dwellers of a Smart Home with an instrument for controlling appliances, comfort metrics and systems. This paper describes the architecture of the HIC and its underlying system, focusing on its appearance, modules and functions. The services provided by HIC are contextualized in two different use cases taking place inside a domestic environment. The first use case illustrates HIC's services and functionalities dedicated to support the dwellers during meal preparations; the use case describes how HIC is able to provide step-by-step tailored instructions and to monitor the whole process. In the second use case, HIC's features for easy comfort metrics management are depicted. The main advantages deriving from the adoption of this home controller system are summarized in the last section, together with the future directions of this work and the usability tests considered to validate the home controller.},
author = {Pizzagalli, Simone and Spoladore, Daniele and Arlati, Sara and Sacco, Marco and Greci, Luca},
booktitle = {International Conference on Serious Games and Applications for Health (SeGAH)},
doi = {10.1109/SeGAH.2018.8401374},
isbn = {978-1-5386-6298-4},
keywords = {Ambient Assisted Living,Home Controller,Interactive Projection,Smart Home},
month = {5},
pages = {1--6},
publisher = {IEEE},
title = {{HIC: An Interactive and Ubiquitous Home Controller System for the Smart Home}},
url = {https://ieeexplore.ieee.org/document/8401374/},
year = {2018}
}
@article{2012_Brock_IEEERob.Autom.Magazine,
annote = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/2012{\_}Brock{\_}IEEERob.Autom.Magazine{\_}06213232.pdf},
author = {{Oliver Brock Dov Katz} and Srinivasa, Siddhartha S},
doi = {10.1109/MRA.2012.2192818},
journal = {IEEE Robotics {\&} Automation Magazine},
month = {6},
number = {2},
pages = {18--19},
title = {{Mobile Manipulation (From the Guest Editors)}},
url = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/2012{\_}Brock{\_}IEEERob.Autom.Magazine{\_}06213232.pdf},
volume = {19},
year = {2012}
}
@article{Rusu2010,
abstract = {We present the Viewpoint Feature Histogram (VFH), a descriptor for 3D point cloud data that encodes geometry and viewpoint. We demonstrate experimentally on a set of 60 objects captured with stereo cameras that VFH can be used as a distinctive signature, allowing simultaneous recognition of the object and its pose. The pose is accurate enough for robot manipulation, and the computational cost is low enough for real time operation. VFH was designed to be robust to large surface noise and missing depth information in order to work reliably on stereo data.},
author = {Rusu, R B and Bradski, G and Thibaux, R and Hsu, J},
doi = {10.1109/IROS.2010.5651280},
isbn = {978-1-4244-6674-0},
journal = {2010 IEEE/RSJ International Conference on Intelligent Robots and Systems},
month = {10},
pages = {2155--2162},
publisher = {Ieee},
title = {{Fast 3D recognition and pose using the Viewpoint Feature Histogram}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5651280 http://www.willowgarage.com/sites/default/files/Rusu10IROS.pdf},
year = {2010}
}
@inproceedings{Kim2011,
abstract = {Activity recognition (AR) research promises to enable a multitude of human-centric applications in smart environments. Nevertheless, application developers will require assurance mechanisms before they can confidently use and apply AR in real-world pervasive systems. In this work we propose an extension of an existing AR approach in which richer recognition semantics that address confidence and assurance are provided. Our approach differentiates between an activity and its effect and subsequently relies on verifying an activity by recognizing its effect. We present our approach along with a comparative experimental evaluation.},
address = {New York, New York, USA},
author = {Kim, Eunju and Helal, Sumi and Nugent, Chris and Lee, Jae Woong},
booktitle = {Proceedings of the 2011 international workshop on Situation activity {\&} goal awareness - SAGAware '11},
doi = {10.1145/2030045.2030051},
isbn = {9781450309264},
keywords = {activity recognition,smart home},
pages = {15},
publisher = {ACM Press},
title = {{Assurance-oriented activity recognition}},
url = {http://dl.acm.org/citation.cfm?doid=2030045.2030051},
year = {2011}
}
@inproceedings{Yu2015,
abstract = {Inspired by studies of human-human con- versations, we present methods for incre- mentally coordinating speech production with listeners' visual foci of attention. We introduce a model that considers the de- mands and availability of listeners' atten- tion at the onset and throughout the pro- duction of system utterances, and that in- crementally coordinates speech synthesis with the listener's gaze. We present an im- plementation and deployment of the model in a physically situated dialog sys- tem and discuss lessons learned.},
author = {Yu, Zhou and Bohus, Dan and Horvitz, Eric},
booktitle = {Annual Meeting of the Special Interest Group on Discourse and Dialogue},
keywords = {attention},
pages = {402--406},
title = {{Incremental Coordination: Attention-Centric Speech Production in a Physically Situated Conversational Agent}},
year = {2015}
}
@inproceedings{Lutkebohle2009,
abstract = {If robots are to succeed in novel tasks, they must be able to learn from humans. To improve such human-robot interaction, a system is presented that provides dialog structure and engages the human in an exploratory teaching scenario. Thereby, we specifically target untrained users, who are supported by mixed-initiative interaction using verbal and non-verbal modalities. We present the principles of dialog structuring based on an object learning and manipulation scenario. System development is following an interactive evaluation approach and we will present both an extensible, event-based interaction architecture to realize mixed-initiative and evaluation results based on a video-study of the system. We show that users benefit from the provided dialog structure to result in predictable and successful human-robot interaction.},
author = {Lutkebohle, I. and Peltason, J. and Schillingmann, L. and Wrede, B. and Wachsmuth, S. and Elbrechter, C. and Haschke, Robert},
booktitle = {2009 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2009.5152521},
isbn = {978-1-4244-2788-8},
issn = {1050-4729},
month = {5},
pages = {4156--4162},
publisher = {IEEE},
title = {{The curious robot - Structuring interactive robot learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152521},
year = {2009}
}
@article{Horridge2011,
abstract = {We present the OWL API, a high level Application Programming Interface (API) for working with OWL ontologies. The OWL API is closely aligned with the OWL 2 structural specification. It supports parsing and rendering in the syntaxes defined in the W3C specification (Functional Syntax, RDF/XML, OWL/XML and the Manchester OWL Syntax); manipulation of ontological structures; and the use of reasoning engines. The reference implementation of the OWL API, written in Java, includes validators for the various OWL 2 profiles - OWL 2 QL, OWL 2 EL and OWL 2 RL. The OWL API has widespread usage in a variety of tools and applications. Keywords:},
author = {Horridge, Matthew and Bechhofer, Sean},
doi = {10.3233/SW-2011-0025},
journal = {Semantic Web},
keywords = {api,application development,framework,java,owl,reasoning},
number = {1},
pages = {11--21},
title = {{The OWLAPI: A Java API for OWL ontologies}},
volume = {2},
year = {2011}
}
@inproceedings{Tang2011,
abstract = {We present an object recognition system which leverages the additional sensing and calibration information available in a robotics setting together with large amounts of training data to build high fidelity object models for a dataset of textured household objects. We then demonstrate how these models can be used for highly accurate detection and pose estimation in an end-to-end robotic perception system incorporating simultaneous segmentation, object classification, and pose fitting. The system can handle occlusions, illumination changes, multiple objects, and multiple instances of the same object. The system placed first in the ICRA 2011 Solutions in Perception instance recognition challenge. We believe the presented paradigm of building rich 3D models at training time and including depth information at test time is a promising direction for practical robotic perception systems.},
author = {Tang, Jie and Miller, Stephen and Singh, Arjun and Abbeel, Pieter},
booktitle = {ICRA},
pages = {3467--3474},
title = {{A Textured Object Recognition Pipeline for Color and Depth Image Data}},
year = {2011}
}
@inproceedings{Reschke2011,
abstract = {We study the feasibility of utilising the RF transceiver of a mobile device to establish some kind of situation awareness. Our results show that the analysis of channel charac- teristics can provide additional and sufficiently accurate context information. This situation awareness is cheap in the sense that it can be obtained from already ongoing communication in a network of nodes. In this paper we present results from a case study with several USRP software radios. We show how the presence, position and activity of persons in a room can be obtained exclusively from channel measurements.},
annote = {Situation recogninition using only a RF-Transceivers (USB devices).

Decide between: 
* presence {\&} room condition
** door state
** person exists in room
* location of person: distinguishing betw. 3 positions
* activity recognition
** standind at p1
** walking betw. p1 {\&} p2
** sitting at p1
** empty room},
author = {Reschke, Markus and Schwarzl, Sebastian and Starosta, Johannes and Sigg, Stephan},
booktitle = {2011 IEEE 73rd Vehicular Technology Conference (VTC Spring)},
doi = {10.1109/VETECS.2011.5956453},
isbn = {978-1-4244-8332-7},
keywords = {rf-sensor,single-sensor,situation recognition},
month = {5},
pages = {1--5},
publisher = {IEEE},
title = {{Situation Awareness Based on Channel Measurements}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5956453},
year = {2011}
}
@inproceedings{Ravenet2016,
abstract = {Virtual Reality and immersive experiences, which allow players to share the same virtual environment as the characters of a virtual world, have gained more and more interest recently. In order to conceive these immersive virtual worlds, one of the challenges is to give to the characters that populate them the ability to express be- haviors that can support the immersion. In this work, we propose a model capable of controlling and simulating a conversational group of social agents in an immersive environment. We describe this model which has been previously validated using a regular screen setting and we present a study for measuring whether users rec- ognized the attitudes expressed by virtual agents through the real- time generated animations of nonverbal behavior in an immersive setting. Results mirrored those of the regular screen setting thus providing further insights for improving players experiences by in- tegrating them into immersive simulated group conversations with characters that express different interpersonal attitudes.},
address = {New York, New York, USA},
author = {Ravenet, Brian and Bevacqua, Elisabetta and Cafaro, Angelo and Ochs, Magalie and Pelachaud, Catherine},
booktitle = {Proceedings of the 9th International Conference on Motion in Games - MIG '16},
doi = {10.1145/2994258.2994280},
isbn = {9781450345927},
keywords = {conversational,group,group simulation,nonverbal,nonverbal behavior,roles,turn-taking,virtual agent},
pages = {175--180},
publisher = {ACM Press},
title = {{Perceiving attitudes expressed through nonverbal behaviors in immersive virtual environments}},
url = {http://dl.acm.org/citation.cfm?doid=2994258.2994280},
year = {2016}
}
@article{Ohshima2015,
abstract = {This paper proposes a novel utterance generation mechanism for a Talking-Ally robot through the utilization of the concepts of hearership and addressivity. The approach incorporates an addressee's eye-gaze behaviors (stateof hear- ership) in order to produce the utterances (addressivity) nec- essary for achieving smooth communication (synchronized with bodily interactions), which are perceived as being per- suasive by the addressee. The results of the study show that the resources of the hearer were significant in generating or adjusting to the structure of utterances in order to persuade the addressee. Additionally, an analysis of dynamic interac- tions revealed that both the human and robot influenced each other's behaviors—e.g., the robot influenced the addressee's attention and the human influenced the robot in changing its utterances.The results of a subjective rating indicated that the robot recognized the participants as hearers, and it was also capable of utterance generation and behaved autonomously (robotic life-likeness),which proved to be crucial in enhanc- ing the persuasiveness of the robot's communication.},
annote = {Experiment: Gemeinsames Tv schauen. Roboter kommentiert

Variablen: 

* attention-coordination (-+)
* turn-initials/entrust behaviours (-r+)

={\textgreater} koodinierte turn-initials/entrust behaviours erh{\"{o}}hen attention beim Nutzer},
author = {Ohshima, Naoki and Ohyama, Yusuke and Odahara, Yuki and {De Silva}, P. Ravindra S and Okada, Michio},
doi = {10.1007/s12369-014-0273-8},
isbn = {1236901402738},
issn = {18754805},
journal = {International Journal of Social Robotics},
keywords = {Hearership and addressivity,Mutual influences,Persuasiveness,Utterance generation,attention},
number = {1},
pages = {51--62},
title = {{Talking-Ally: The Influence of Robot Utterance Generation Mechanism on Hearer Behaviors}},
volume = {7},
year = {2015}
}
@article{Baum2012,
author = {Baum, Marcus and Faion, Florian and Hanebeck, Uwe D.},
doi = {10.1109/MFI.2012.6343003},
isbn = {978-1-4673-2512-7},
journal = {2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)},
month = {9},
pages = {186--191},
publisher = {Ieee},
title = {{Tracking ground moving extended objects using RGBD data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6343003},
year = {2012}
}
@article{Li2013,
abstract = {In pervasive computing environment the low-level context data provided by the sensors are usually meaningless, and thus higher-level context needs to be extracted. Situation is the semantic interpretation of low-level context, permitting a higher-level specification of human behavior in the scene and the corresponding system service. Context modeling and reasoning are the two key parts in the situation awareness. In this paper we present a multiple level architecture for context modeling, and a reasoning approach based on the Dempster- Shafer Theory (DST) and semantic similarity. The Dempster- Shafer theory is employed to analyze low-level context and eliminate the conflict among different sensors. Semantic similarity is used to reason out the higher-level context information based on the ontology. Computer simulation reveals that the proposed approach allows more efficient and accurate reasoning of higher-level context information compared to the existing approach.},
annote = {"Situation is the semantic interpretation of low-level context, permitting a higher-level specification of human behavior in the scene and the corresponding system service."


"In context-aware applications, situation is external semantic interpretation of low-level context [2], permitting a higher-level specification of human behavior in the scene and the corresponding system services."


Pyramid System:
* low level 'primitive context' = sensordata in human conceptual states
* inference engine prodces: inferred context
* aggregator reasonss: high level context},
author = {Li, Zhong Yuan and Park, Jong Chang and Lee, Byungjun and Youn, Hee Yong},
doi = {10.1109/CSE.2013.87},
isbn = {978-0-7695-5096-1},
journal = {2013 IEEE 16th International Conference on Computational Science and Engineering},
keywords = {context modeling,context reasoning,dempster-shafer theory,ontology,semantic similarity,situation,situation awareness,situation awarenesss,ubiquitous computing},
month = {12},
pages = {545--552},
publisher = {Ieee},
title = {{Situation Awareness Based on Dempster-Shafer Theory and Semantic Similarity}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6755267},
year = {2013}
}
@techreport{Bohus2010a,
abstract = {We describe a computational framework for modeling and managing turn-taking in open- world spoken dialog systems. We present a rep- resentation and methodology for tracking the conversational dynamics in multiparty interac- tions, making floor control decisions, and ren- dering these decisions into appropriate behav- iors. We show how the approach enables an em- bodied conversational agent to participate in multiparty interactions, and to handle a diversity of natural turn-taking phenomena, including multiparty floor management, barge-ins, restarts, and continuations. Finally, we discuss results and lessons learned from experiments.},
annote = {for each speech-segment:
multinomial variable speaker(segm) : next to sound localisation
addressees(segm) : tracked by engagement component
side participants : conversation-addressees-speaker
overhearers : all - conversation},
author = {Bohus, Dan and Horvitz, Eric},
booktitle = {Microsoft Research Technical Report MSR-TR 2010-115},
keywords = {Addressee,conversational floor,multiparty,turn-taking},
pages = {10},
title = {{Computational Models for Multiparty Turn Taking}},
url = {http://research.microsoft.com/en-us/um/people/dbohus/docs/turntaking{\_}msr{\_}tr.pdf},
year = {2010}
}
@article{Duffy2003,
abstract = {This paper discusses the issues pertinent to the development of a meaningful social interaction between robots and people through employing degrees of anthropomorphism in a robot's physical design and behaviour. As robots enter our social space, we will inherently project/impose our interpretation on their actions similar to the techniques we employ in rationalising, for example, a pet's behaviour. This propensity to anthropomorphise is not seen as a hindrance to social robot development, but rather a useful mechanism that requires judicious examination and employment in social robot research. {\textcopyright} 2003 Elsevier Science B.V. All rights reserved.},
author = {Duffy, Brian R.},
doi = {10.1016/S0921-8890(02)00374-3},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Anthropomorphism,Artificial emotion,Artificial intelligence,Humanoid,Social robots},
month = {3},
number = {3-4},
pages = {177--190},
title = {{Anthropomorphism and the social robot}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0921889002003743},
volume = {42},
year = {2003}
}
@article{Niebles2008,
abstract = {We present a novel unsupervised learning method for human action categories. A video sequence is repre- sented as a collection of spatial-temporal words by extract- ing space-time interest points. The algorithm automatically learns the probability distributions of the spatial-temporal words and the intermediate topics corresponding to human action categories. This is achieved by using latent topic models such as the probabilistic Latent Semantic Analysis (pLSA) model and Latent Dirichlet Allocation (LDA). Our approach can handle noisy feature points arisen from dy- namic background and moving cameras due to the appli- cation of the probabilistic models. Given a novel video se- quence, the algorithm can categorize and localize the human action(s) contained in the video. We test our algorithm on three challenging datasets: the KTH human motion dataset, theWeizmann human action dataset, and a recent dataset of figure skating actions.Our results reflect the promise of such a simple approach. In addition, our algorithm can recognize and localize multiple actions in long and complex video se- quences containing multiple motions.},
author = {Niebles, Juan Carlos and Wang, Hongcheng and Fei-Fei, Li},
doi = {10.1007/s11263-007-0122-4},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {action categorization,bag of words,learning,spatio-temporal interest points,topic models,unsupervised learning},
month = {3},
number = {3},
pages = {299--318},
title = {{Unsupervised Learning of Human Action Categories Using Spatial-Temporal Words}},
url = {http://link.springer.com/10.1007/s11263-007-0122-4},
volume = {79},
year = {2008}
}
@inproceedings{Rios-Martinez2011,
abstract = {With the growing demand of personal assistance to mobility and mobile service robotics, robot navigation systems must be ??aware?? of the social conventions followed by people. They must respect proximity constraints but also respect people interacting. For example, they may not break interaction between people talking, unless the occupants want to take part in the conversation. In this case, they must be able to join the group using a socially adapted behavior. This paper proposes a risk-based navigation method including both the traditional notion of risk of collision and the notion of risk of disturbance. Results exhibit new emerging behavior showing how a robot takes into account social conventions in its navigation strategy.},
author = {Rios-Martinez, Jorge and Spalanzani, Anne and Laugier, Christian},
booktitle = {International Conference on Intelligent Robots and Systems (IROS)},
doi = {10.1109/IROS.2011.6048137},
isbn = {978-1-61284-454-1},
keywords = {Human aware navigation,Proxemics,risk assessment},
month = {9},
pages = {2014--2019},
publisher = {IEEE/RSJ},
title = {{Understanding Human Interaction for Probabilistic Autonomous Navigation using Risk-RRT Approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6048137},
year = {2011}
}
@inproceedings{ISRR,
address = {Lucerne, Switzerland},
annote = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/holisticKatz-Brock2010.pdf},
author = {{Dov Katz} and Brock, Oliver},
booktitle = {International Symposium of Robotics Research},
month = {8},
pages = {1--16},
publisher = {Springer Verlag},
title = {{A Factorization Approach to Manipulation in Unstructured Environments}},
url = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/holisticKatz-Brock2010.pdf},
year = {2009}
}
@incollection{Choi,
author = {Choi, Wongun and Chao, Yu-wei and Pantofaru, Caroline and Savarese, Silvio},
doi = {10.1007/978-3-319-10593-2_28},
keywords = {activity,activity recognition,group discovery,groups,iteraction,recognition,social interaction},
pages = {417--433},
title = {{Discovering Groups of People in Images}},
url = {http://research.google.com/pubs/pub43104.html http://link.springer.com/10.1007/978-3-319-10593-2{\_}28},
year = {2014}
}
@inproceedings{Fischer2013,
abstract = {At the core of every object recognition system lies the development and integration of distinct feature descriptors to create object representations robust against varying perspec- tives or lightning conditions. Recent work has primarily focused on the development of distinct point features. While these features achieve impressive recognition results, point features fail to capture the shape and appearance of an object with less or even without texture. This paper proposes a novel method for the rapid and dense computation of 2D and 3D image cues from RGB-D data to target the recognition of objects without rich texture and a global histogram-based descriptor for the distinct description of object models.},
author = {Fischer, Jan and Bormann, Richard and Arbeiter, Georg and Verl, Alexander},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
isbn = {9781467356428},
pages = {2104--2109},
title = {{A feature descriptor for texture-less object representation using 2D and 3D cues from RGB-D data}},
year = {2013}
}
@article{Hochreiter1997,
author = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
doi = {10.1162/neco.1997.9.8.1735},
issn = {0899-7667},
journal = {Neural Computation},
month = {11},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
volume = {9},
year = {1997}
}
@article{Bay2008,
abstract = {This article presents a novel scale- and rotation-invariant detector and descriptor, coined SURF (Speeded-Up Robust Features). SURF approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (specifically, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by sim- plifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper encompasses a detailed description of the detector and descriptor and then explores the effects of the most important param- eters.Weconclude the article with SURF's application to two challenging, yet converse goals: camera calibration as a special case of image registration, and object recognition. Our experiments underline SURF's usefulness in a broad range of topics in computer vision. ?},
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and {Van Gool}, Luc},
doi = {10.1016/j.cviu.2007.09.014},
issn = {10773142},
journal = {Computer Vision and Image Understanding},
keywords = {camera calibration,feature description,interest points,local features,object recognition},
month = {6},
number = {3},
pages = {346--359},
title = {{Speeded-Up Robust Features (SURF)}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@article{Koenderink1979,
abstract = {It is argued that the internal model of any object must take the form of a function, such that for any intended action the resulting reafference is predict- able. This function can be derived explicitly for the case of visual perception of rigid bodies by ambulant observers. The function depends on physical causation, not physiology; consequently, one can make a priori statements about possible internal models. A pos- teriori it seems likely that the orientation sensitive units described by Hubel and Wiesel constitute a physiological substrate subserving the extraction of the invariants of this function. The function is used to define a measure for the visual complexity of solid shape. Relations with Gestalt theories of perception are discussed.},
author = {Koenderink, J. J. and van {Doorn, A. J.}},
journal = {Biological Cybernetics},
number = {4},
pages = {211--216},
title = {{The Internal Representation of Solid Shape with Respect to Vision}},
volume = {32},
year = {1979}
}
@inproceedings{Seifried2009,
abstract = {While most homes are inherently social places, existing devices designed to control consumer electronics typically only support single user interaction. Further, as the number of consumer electronics in modern homes increases, people are often forced to switch between many controllers to interact with these devices. To simplify interaction with these devices and to enable more collaborative forms of device control, we propose an integrated remote control system, called CRISTAL (Control of Remotely Interfaced Systems using Touch-based Actions in Living spaces). CRISTAL enables people to control a wide variety of digital devices from a centralized, interactive tabletop system that provides an intuitive, gesture-based interface that enables multiple users to control home media devices through a virtually augmented video image of the surrounding environment. A preliminary user study of the CRISTAL system is presented, along with a discussion of future research directions.},
address = {New York, New York, USA},
author = {Seifried, Thomas and Haller, Michael and Scott, Stacey D. and Perteneder, Florian and Rendl, Christian and Sakamoto, Daisuke and Inami, Masahiko},
booktitle = {ACM International Conference on Interactive Tabletops and Surfaces (ITS)},
doi = {10.1145/1731903.1731911},
isbn = {9781605587332},
keywords = {collaborative interface,multi-touch,remote controller},
pages = {33},
publisher = {ACM Press},
title = {{CRISTAL: A Collaborative Home Media and Device Controller Based on a Multi-touch Display}},
url = {http://dl.acm.org/citation.cfm?id=1731903.1731911 http://portal.acm.org/citation.cfm?doid=1731903.1731911},
year = {2009}
}
@incollection{Ravenet2015,
abstract = {In this paper we propose a computational model for the real time generation of nonverbal behaviors supporting the expression of in-terpersonal attitudes for turn-taking strategies and group formation in multi-party conversations among embodied conversational agents. Start-ing from the desired attitudes that an agent aims to express towards every other participant, our model produces the nonverbal behavior that should be exhibited in real time to convey such attitudes while manag-ing the group formation and attempting to accomplish the agent's own turn-taking strategy. We also propose an evaluation protocol for similar multi-agent configurations. We conducted a study following this protocol to evaluate our model. Results showed that subjects properly recognized the attitudes expressed by the agents through their nonverbal behavior and turn taking strategies generated by our system.},
author = {Ravenet, Brian and Cafaro, Angelo and Biancardi, Beatrice and Ochs, Magalie and Pelachaud, Catherine},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-21996-7_41},
isbn = {9783319219950},
issn = {16113349},
pages = {375--388},
title = {{Conversational Behavior Reflecting Interpersonal Attitudes in Small Group Interactions}},
url = {http://link.springer.com/10.1007/978-3-319-21996-7{\_}41},
volume = {9238},
year = {2015}
}
@article{gco3,
author = {Boykov, Y. and Kolmogorov, V.},
doi = {10.1109/TPAMI.2004.60},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {9},
number = {9},
pages = {1124--1137},
title = {{An Experimental Comparison of Min-Cut/Max-Flow Algorithms for Energy Minimization in Vision}},
url = {http://ieeexplore.ieee.org/document/1316848/},
volume = {26},
year = {2004}
}
@inproceedings{Attard2013,
abstract = {Today's personal devices provide a stream of information which, if processed adequately, can provide a better insight into their owner's current activities, environment, location, etc. In treating these de- vices as part of a personal sensor network, we exploit rawand inter- preted context information in order to enable the automatic recog- nition of personal recurring situations. An ontology-based graph matching technique continuously compares a person's ‘live con- text', with all previously-stored situations, both of which are rep- resented as an instance of the DCON Context Ontology. Whereas each situation corresponds to an adaptive DCON instance, initially marked by a person and gradually characterised over time, the live context representation is constantly updated with mashed-up con- text information streaming in from various personal sensors. In this paper we present the matching technique employed to enable automatic situation recognition, and an experiment to evaluate its performance based on real users and their perceived context data.},
address = {New York, New York, USA},
annote = {"Live Context refers to their transient context, which comprises of information about the documents be- ing accessed, people who are in their vicinity, tasks and projects they are working on, goals they have in mind, environmental data (including weather conditions), and the device(s) they are using [15]. "


"In contrast, a personal Situation refers to a recurring ‘fuzzy' context, which combines physical (i.e., existence of an entity in some particular region of space and time [14]) and virtual (i.e., in- terpreted existence of an entity through online activities and device and application use) user circumstances that are independent of a particular point in time."


Situation can be characterized through a series of context observations.


Devices and services are nodes in a personal sensor network.


Siuation learning:
* First annotation by a user
* identification of recurring situation and recognition enhancement through user feedback},
author = {Attard, Judie and Scerri, Simon and Rivera, Ismael and Handschuh, Siegfried},
booktitle = {Proceedings of the 9th International Conference on Semantic Systems - I-SEMANTICS '13},
doi = {10.1145/2506182.2506197},
isbn = {9781450319720},
keywords = {dcon,mobile,ontology,situation recognition},
pages = {113},
publisher = {ACM Press},
title = {{Ontology-based situation recognition for context-aware systems}},
url = {http://dl.acm.org/citation.cfm?doid=2506182.2506197},
year = {2013}
}
@article{Heider1950,
annote = {video: https://www.youtube.com/watch?v=VTNmLt7QX8E},
author = {Heider, Fritz and Simmel, Marianne},
doi = {10.2307/1416950},
issn = {00029556},
journal = {The American Journal of Psychology},
keywords = {antropomorphism},
month = {4},
number = {2},
pages = {243},
title = {{An Experimental Study of Apparent Behavior}},
url = {http://joi.jlc.jst.go.jp/JST.Journalarchive/jjpsy1926/20.2{\_}67?from=CrossRef http://www.jstor.org/stable/1416950?origin=crossref https://www.jstor.org/stable/1416950?origin=crossref},
volume = {57},
year = {1944}
}
@inproceedings{Jayagopi2013,
abstract = {In this paper, we investigate the task of addressee estimation in multi-party interactions. For every utterance from a human participant, the robot should know if it was being addressed or not, so as to respond and behave accordingly. To accomplish this various cues could be made use of: the most important being gaze cues of the speaker. Apart from this several other cues can act as contextual variables to improve the estimation accuracy of this task. For example, the gaze cue of other participants, and the long-term or short-term dialog context. In this paper we investigate the possibility to combine such information from diverse sources to improve the addressee estimation task. For this study, we use 11 interactions with a humanoid robot NAO1 giving quiz to two human participants.},
annote = {* addressee recognition not widely examided in hri but in hci/avatar/hhi
* use gaze all participants},
author = {Jayagopi, Dinesh Babu and Odobez, Jean-Marc},
booktitle = {2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
doi = {10.1109/HRI.2013.6483544},
isbn = {978-1-4673-3101-2},
issn = {21672148},
keywords = {Addressee estimation,HRI,Social robots,assignee recognition,dialog,gaze,hri},
month = {3},
pages = {147--148},
publisher = {IEEE},
title = {{Given that, should i respond? Contextual addressee estimation in multi-party human-robot interactions}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6483544},
year = {2013}
}
@inproceedings{Imai2001,
abstract = {This paper proposes a speech generation system named Linta-III,$\backslash$nwhich generates an utterance dependent on a real world situation. To$\backslash$ngenerate the situated utterance, Linta-III has a joint attention$\backslash$nmechanism, which develops joint attention between a person and a robot.$\backslash$nThe joint attention mechanism employs eye-contact and an attention$\backslash$nexpression. The eye-contact promotes the relationship between the person$\backslash$nand the robot. The attention expression manifests relevant sensor$\backslash$ninformation with a physical expression. With the eye-contact and$\backslash$nattention expression, the joint attention mechanism is able to draw the$\backslash$nperson's attention to the same sensor information as the robot. As a$\backslash$nresult of the joint attention, Linta-III is able to omit obvious words$\backslash$nin the situation from an utterance description. We also conducted a$\backslash$npsychological experiment on the development of joint attention. The$\backslash$nresults indicated that the eye-contact and attention expression are$\backslash$nsignificant factors in the development of joint attention},
annote = {Attention im Dialog: Eher multimodale Referenzierung von Objekten und situiertheit.

* attention expression -{\textgreater} deiktische Geste
* eye contact zur Unterst{\"{u}}tzung der Geste 
{\textgreater} Ziel anschauen und zeigen, dann Augenkontakt

Test: Effekt von Augenkontakt auf erreichen von joint attention

* roboter f{\"{a}}hrt vor
* zeigt auf Poster, sagt 'look at this'
* schaut abwechselnd auf poster/person oder nicht (Kondition)

={\textgreater} Leute schauen eher auf referenziertes Objekt},
author = {Imai, Michita and Ono, Tetsuo and Ishiguro, Hiroshi},
booktitle = {Proceedings 10th IEEE International Workshop on Robot and Human Interactive Communication. ROMAN 2001 (Cat. No.01TH8591)},
doi = {10.1109/ROMAN.2001.981955},
isbn = {0-7803-7222-0},
issn = {02780046},
keywords = {attention},
number = {4},
pages = {512--517},
publisher = {IEEE},
title = {{Physical relation and expression: joint attention for human-robot interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=981955},
volume = {50},
year = {2001}
}
@phdthesis{Holthaus,
author = {Holthaus, Patrick},
keywords = {1proxemics,hri},
school = {Bielefeld University},
title = {{Approaching Human-Like Spatial Awareness in Social Robotics - An Investigation of Spatial Interaction Strategies with a Receptionist Robot}},
url = {urn:nbn:de:hbz:361-27330383},
year = {2014}
}
@inproceedings{Lala2018,
abstract = {The task of identifying when to take a conversational turn is an important function of spoken dialogue systems. The turn-taking system should also ideally be able to handle many types of dialogue, from structured conversation to spontaneous and unstructured discourse. Our goal is to determine how much a generalized model trained on many types of dialogue scenarios would improve on a model trained only for a specific scenario. To achieve this goal we created a large corpus of Wizard-of-Oz conversation data which consisted of several different types of dialogue sessions, and then compared a generalized model with scenario-specific models. For our evaluation we go further than simply reporting conventional metrics, which we show are not informative enough to evaluate turn-taking in a real-time system. Instead, we process results using a performance curve of latency and false cut-in rate, and further improve our model's real-time performance using a finite-state turn-taking machine. Our results show that the generalized model greatly outperformed the individual model for attentive listening scenarios but was worse in job interview scenarios. This implies that a model based on a large corpus is better suited to conversation which is more user-initiated and unstructured. We also propose that our method of evaluation leads to more informative performance metrics in a real-time system.},
address = {New York, New York, USA},
author = {Lala, Divesh and Inoue, Koji and Kawahara, Tatsuya},
booktitle = {International Conference on Multimodal Interaction (ICMI)},
doi = {10.1145/3242969.3242994},
isbn = {9781450356923},
keywords = {deep learning,dialogue systems,evaluation methods,neural networks,turn-taking},
pages = {78--86},
publisher = {ACM Press},
title = {{Evaluation of Real-time Deep Learning Turn-Taking Models for Multiple Dialogue Scenarios}},
url = {http://dl.acm.org/citation.cfm?doid=3242969.3242994},
year = {2018}
}
@article{Bohus2011,
abstract = {We report on an empirical study of a multiparty turn-taking model for physically situated spoken dialog systems. We present subjective and objective performance measures that show how the model, supported with a basic set of sensory competencies and turn-taking policies, can enable interactions with multiple participants in a collaborative task setting. The analysis brings to the fore several phenomena and frames challenges for managing multiparty turn taking in physically situated interaction.},
author = {Bohus, Dan and Horvitz, Eric},
journal = {Special Interest Group on Discourse and Dialogue (SIGDIAL)},
keywords = {dialog,multiparty,turn-taking},
number = {1974},
pages = {98--109},
title = {{Multiparty Turn Taking in Situated Dialog: Study, Lessons, and Directions}},
url = {http://dl.acm.org/citation.cfm?id=2132903},
year = {2011}
}
@incollection{Jan2007,
abstract = {This paper presents a model for simulating cultural differences in the conversational behavior of virtual agents. The model provides parameters for differences in proxemics, gaze and overlap in turn taking. We present a review of literature on these factors and show results of a study where native speakers of North American English, Mexican Spanish and Arabic were asked to rate the realism of the simulations generated based on different cultural parameters with respect to their culture.},
address = {Berlin, Heidelberg},
author = {Jan, Du{\v{s}}an and Herrera, David and Martinovski, Bilyana and Novick, David and Traum, David},
booktitle = {Intelligent Virtual Agents},
doi = {10.1007/978-3-540-74997-4_5},
isbn = {9783540749967},
issn = {03029743},
keywords = {conversational agents,cul-,gaze,proxemics,turn taking},
pages = {45--56},
publisher = {Springer Berlin Heidelberg},
title = {{A Computational Model of Culture-Specific Conversational Behavior}},
url = {http://link.springer.com/10.1007/978-3-540-74997-4{\_}5},
year = {2007}
}
@inproceedings{Shi2011,
abstract = {Analysing conversation initiation. To participate in a conversation, when a particular spatial formation occurs, she feel that she is participation. Once she participate, she try to maintain such formations. Authors build a model and implement it in a humanoid robot. Then test it on a shopkeeper scenario.},
author = {Shi, Chao and Shimada, Michihiro and Kanda, Takayuki and Ishiguro, Hiroshi and Hagita, Norihiro},
booktitle = {Robotics: Science and Systems VII},
doi = {10.15607/RSS.2011.VII.039},
isbn = {9780262517799},
month = {6},
number = {May 2014},
publisher = {Robotics: Science and Systems Foundation},
title = {{Spatial Formation Model for Initiating Conversation}},
url = {http://www.roboticsproceedings.org/rss07/p39.pdf},
year = {2011}
}
@article{Ni2015,
abstract = {Human activity detection within smart homes is one of the basis of unobtrusive wellness monitoring of a rapidly aging population in developed countries. Most works in this area use the concept of “activity” as the building block with which to construct applications such as healthcare monitoring or ambient assisted living. The process of identifying a specific activity encompasses the selection of the appropriate set of sensors, the correct preprocessing of their provided raw data and the learning/reasoning using this information. If the selection of the sensors and the data processing methods are wrongly performed, the whole activity detection process may fail, leading to the consequent failure of the whole application. Related to this, the main contributions of this review are the following: first, we propose a classification of the main activities considered in smart home scenarios which are targeted to older people's independent living, as well as their characterization and formalized context representation; second, we perform a classification of sensors and data processing methods that are suitable for the detection of the aforementioned activities. Our aim is to help researchers and developers in these lower-level technical aspects that are nevertheless fundamental for the success of the complete application.},
author = {Ni, Qin and {Garc{\'{i}}a Hernando}, Ana and de la Cruz, Iv{\'{a}}n},
doi = {10.3390/s150511312},
isbn = {3463414929},
issn = {1424-8220},
journal = {Sensors},
keywords = {activity characterization,activity monitoring,data processing,e-health services development,independent living,sensors,smart,smart home},
number = {5},
pages = {11312--11362},
title = {{The Elderly's Independent Living in Smart Homes: A Characterization of Activities and Sensing Infrastructure Survey to Facilitate Services Development}},
url = {http://www.mdpi.com/1424-8220/15/5/11312/},
volume = {15},
year = {2015}
}
@article{Rime1985,
abstract = {Three experiments were designed to assess whether the perception of inter- personal emotions induced by animated geometric figures, such as those used by Heider and Simmel (1944) and Michotte (1950), vary consensuatly among observers according to the specific kinetic stimuli presented. Using Michotte's original disk method, the first experiment showed eight kinetic structures to 60 subjects who were asked to describe their perceptions on rating scales. The perceptions were shown to vary according to the dynamic features of the stimulus, and a high degree of interobserver consensus was generally recorded. In the second experiment, subjects were shown human silhouettes animated according to the same kinetic structures as those of experiment 1. The profiles of their answers were generally similar to those recorded for animated geometric figures, but the consensus tended to be weaker. The third experiment compared the rating scale descriptions of five kinetic structures made by European, American, and African subjects tested in their own countries. The perceptions of European and American subjects were very close to each other for the larger part of the material The data of the Africans differed from the Americans in one way and from the Europeans in another way.},
author = {Rime, Bernard and Boulanger, Bernadette and Laubin, Philippe and Richir, Marc and Stroobants, Kathleen},
doi = {10.1007/BF00991830},
isbn = {0146-7239},
issn = {0146-7239},
journal = {Motivation and Emotion},
month = {9},
number = {3},
pages = {241--260},
title = {{The perception of interpersonal emotions originated by patterns of movement}},
url = {http://link.springer.com/10.1007/BF00991830},
volume = {9},
year = {1985}
}
@inproceedings{Hayashi2015,
abstract = {We propose a new method of recognizing daily human activities based on a Deep Neural Network (DNN), using multi- modal signals such as environmental sound and subject acceleration. We conduct recognition experiments to compare the proposed method to other methods such as a Support Vec- tor Machine (SVM), using real-world data recorded continuously over 72 hours. Our proposed method achieved a frame accuracy rate of 85.5{\%} and a sample accuracy rate of 91.7{\%} when identifying nine different types of daily activities. Furthermore, the proposed method outperformed the SVM-based method when an additional “Other” activity category was in- cluded. Therefore, we demonstrate that DNNs are a robust method of daily activity recognition.},
author = {Hayashi, Tomoki and Nishida, Masafumi and Kitaoka, Norihide and Takeda, Kazuya},
booktitle = {European Signal Processing Conference (EUSIPCO) DAILY},
isbn = {9780992862633},
keywords = {DNN,Daily activity recognition,acceleration signal,activities of daily living,dnn,environmental sound signal,multi-modal,multimodal},
pages = {2351--2355},
title = {{Daily Activity Recognition Based on DNN using Environmental Sound and Acceleration Signals}},
year = {2015}
}
@inproceedings{Sandnes2017,
author = {Sandnes, Frode Eika and Herstad, Jo and Stangeland, Andrea Marie and Medola, Fausto Orsi},
booktitle = {SmartWorld, Ubiquitous Intelligence {\&} Computing, Advanced {\&} Trusted Computed, Scalable Computing {\&} Communications, Cloud {\&} Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)},
doi = {10.1109/UIC-ATC.2017.8397460},
isbn = {978-1-5386-0435-9},
keywords = {6,although such remotes are,context-awareness,of the usability of,often certain small unique,particular,remote controls in,similar,smart home,target group are skeptical,they have,traits that greatly affect,universal remote,usability},
month = {8},
number = {c},
pages = {1--6},
publisher = {IEEE},
title = {{UbiWheel: A Simple Context-Aware Universal Control Concept for Smart Home Appliances that Encourages Active Living}},
url = {https://ieeexplore.ieee.org/document/8397460/},
year = {2017}
}
@book{Strunk2009,
author = {Strunk, William and White, Elwyn Brooks},
isbn = {9780205309023},
publisher = {Longman},
title = {{The Elements of Style}}
}
@article{Ogden2000,
abstract = {Visual nonverbal behaviour plays a significant role in allowing humans to structure their interactions meaningfully, thus facilitating communication between individuals. Such behaviour may be exploited in the design of socially intelligent robots. While much visual behaviour$\backslash$n(e.g. facial expression) is hard to detect using a machine vision system, especially in unconstrained situations such as natural interaction, simple gestures and the positions of interactants are relatively easy to detect. This paper begins with a description of some related work, followed by an overview of some systems that track and/or interpret human movement. This is followed by a discussion of what is known about the structure of human interactions and a detailed example of a particular kind of structured interaction (the greeting). Ways in which interactional structure can be exploited in the development of socially intelligent robots are considered: finally, a specific application involving the use of robots in the rehabilitation of autistic children is described. We refer to this approach, using knowledge of interactional structure both to generate social behaviour for a robot and to interpret interactions for a machine vision system, as interactive vision. The work described in this paper is still in its early stages and there are no experimental results to report as yet: the purpose of this paper is rather to outline ways in which sociological and psychological work on human interactional behaviour can be exploited in socially intelligent agents and machine vision systems.},
author = {Ogden, B. and Dautenhahn, K.},
journal = {Proc. SIRS2000, Symposium on Intelligent Robotic Systems, Reading, UK},
number = {1998},
pages = {353--361},
title = {{Robotic etiquette: Structured interaction in humans and robots}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.3170{\&}rep=rep1{\&}type=pdf http://hdl.handle.net/2299/1945},
year = {2000}
}
@inproceedings{Singh2012,
abstract = {With the growth in social media, internet of things, and planetary- scale sensing there is an unprecedented need to assimilate spatio- temporally distributed multimedia streams into actionable information. Consequently the concepts like objects, scenes, and events, need to be extended to recognize situations (e.g. epidemics, traffic jams, seasons, flash mobs). This paper motivates and computationally grounds the problem of situation recognition. It describes a systematic approach for combining multimodal real-time big data into actionable situations. Specifically it presents a generic approach for modeling and recognizing situations. A set of generic building blocks and guidelines help the domain experts model their situations of interest. The created models can be tested, refined, and deployed into practice using a developed system (EventShop). Results of applying this approach to create multiple situation-aware applications by combining heterogeneous streams (e.g. Twitter, Google Insights, Satellite imagery, Census) are presented.},
address = {New York, New York, USA},
annote = {Situations in big scale
* hurricanes
* wildfires
* traffic quality
* economic recessions
* seasons
* demonstrations
* happiness-index








Def. Situation: "An actionable abstraction of observed spatio-temporal descriptors"








Approach: Filter and combine.
Example: Get Flu Tags from twitter, combine with average calculate growth rate for USA-Heatmap},
author = {Singh, Vivek K and Gao, Mingyan and Jain, Ramesh},
booktitle = {Proceedings of the 20th ACM international conference on Multimedia - MM '12},
doi = {10.1145/2393347.2396421},
isbn = {9781450310895},
keywords = {awareness,e-mage,event-condition-action,events,framework,modeling,sensor networks,situation,situation detection,situation recognition,social networks,spatio-temporal},
pages = {1209},
publisher = {ACM Press},
title = {{Situation recognition: An Evolving Problem for Heterogeneous Dynamic Big Multimedia Data}},
url = {http://dl.acm.org/citation.cfm?doid=2393347.2396421},
year = {2012}
}
@inproceedings{Hegel2009,
abstract = {Research on social robots is mainly comprised of research into algorithmic problems in order to expand a robot{\{}$\backslash$textasciiacute{\}}s capabilities to improve communication with human beings. Also, a large body of research concentrates on the appearance, i.e. aesthetic form of social robots. However, only little reference to their definition is made. In this paper we argue that form, function, and context have to be taken systematically into account in order to develop a model to help us understand social robots. Therefore, we address the questions: What is a social robot, what are the interdisciplinary research aspects of social robotics, and how are these different aspects interlinked? In order to present a comprehensive and concise overview of the various aspects we present a framework for a definition towards social robots.},
author = {Hegel, Frank and Muhl, Claudia and Wrede, Britta and Hielscher-Fastabend, Martina and Sagerer, Gerhard},
booktitle = {The Second International Conferences on Advances in Computer-Human Interactions (ACHI)},
keywords = {social robotics},
number = {Section II},
pages = {169----174},
publisher = {IEEE},
title = {{Understanding Social Robots}},
year = {2009}
}
@phdthesis{Meissner2014,
abstract = {With the advent of the Internet-of-Things way more sensor-generated data streams came available that researchers want to exploit context from. Many researchers worked on context recognition for rather unimodal data in pervasive systems, but recent works about object virtualisation in the Internet-of-Things domain enable context-exploitation based on processing multi-modal information collected from pervasive systems. Additionally to the sensed data there is formalised knowledge about the real world objects emitted by IoT services as contributed by the author in [1], [2] and [3]. In this work an approach for context recognition is proposed that takes knowledge about virtual objects and its relationships into account in order to improve context recognition. The approach will only recognise context that has been predefined manually beforehand, no new context information can be exploited with the work proposed here. This work's scope is about recognising the activity that a user is most likely involved in by observing the evolving context of a user of a pervasive system. As an assumption for this work the activities have to be modelled as graphs in which the nodes are situations observable by a pervasive system. The pervasive system to be utilised has to be built compliant to the Architectural Reference Model for the IoT (ARM) to which the author has contributed to in [4] and [5]. The hybrid context model proposed in this thesis is made of an ontology-based part as well as a probability-based part. Ontologies assist in adapting the probability distributions for the Hidden Markov Model-based recognition technique according to the current context. It could be demonstrated in this work that the context-aware adaptation of the recognition model increased the detection rate of the activity recognition system. Key},
author = {Meissner, Stefan},
number = {October},
title = {{Activity Recognition in Event Driven IoT- Service architectures}},
year = {2014}
}
@article{Manoel2017,
abstract = {In statistical learning for real-world large-scale data problems, one must often resort to "streaming" algorithms which operate sequentially on small batches of data. In this work, we present an analysis of the information-theoretic limits of mini-batch inference in the context of generalized linear models and low-rank matrix factorization. In a controlled Bayes-optimal setting, we characterize the optimal performance and phase transitions as a function of mini-batch size. We base part of our results on a detailed analysis of a mini-batch version of the approximate message-passing algorithm (Mini-AMP), which we introduce. Additionally, we show that this theoretical optimality carries over into real-data problems by illustrating that Mini-AMP is competitive with standard streaming algorithms for clustering.},
archivePrefix = {arXiv},
author = {Manoel, Andre and Krzakala, Florent and Tramel, Eric W. and Zdeborov{\'{a}}, Lenka},
eprint = {1706.00705},
keywords = {bayes,inference,streaming},
month = {6},
number = {i},
pages = {1--19},
title = {{Streaming Bayesian inference: theoretical limits and mini-batch approximate message-passing}},
url = {https://arxiv.org/abs/1706.00705 http://arxiv.org/abs/1706.00705},
year = {2017}
}
@inproceedings{Buschmeier2011,
abstract = {Successful dialogue is based on collaborative efforts of the interactants to ensure mutual understanding. This paper presents work towards making conversational agents 'attentive speakers' that continuously attend to the communicative feedback given by their interlocutors and adapt their ongoing and subsequent communicative behaviour to their needs. A comprehensive conceptual and architectural model for this is proposed and first steps of its realisation are described. Results from a prototype implementation are presented. {\textcopyright} 2011 Springer-Verlag.},
address = {Reykjavik, Iceland},
author = {Buschmeier, Hendrik and Kopp, Stefan},
booktitle = {Proceedings of the 11th International Conference on Intelligent Virtual Agents},
doi = {10.1007/978-3-642-23974-8_19},
isbn = {9783642239731},
issn = {03029743},
keywords = {Communicative feedback,adaptation,attentive speaker agents,attributed listener state,feedback elicitation,feedback interpretation},
pages = {169--182},
title = {{Towards conversational agents that attend to and adapt to communicative user feedback}},
volume = {11},
year = {2011}
}
@inproceedings{Baeg2007,
abstract = {This paper is concerned with constructing a prototype smart home environment which has been built in the research building of Korea Institute of Industrial Technology (KITECH) to demonstrate the practicability of a robot-assisted future home environment. Key functionalities that a home service robot must provide are localization, navigation, object recognition and object handling. A considerable amount of research has been conducted to make the service robot perform these operations with its own sensors, actuators and a knowledge database. With all heavy sensors, actuators and a database, the robot could have performed the given tasks in a limited environment or showed the limited capabilities in a natural environment. We initiated a smart home environment project for light-weight service robots to provide reliable services by interacting with the environment through the wireless sensor networks. This environment consists of the following three main components: smart objects with an radio frequency identification (RFID) tag and smart appliances with sensor network functionality; the home server that connects smart devices as well as maintains information for reliable services; and the service robots that perform tasks in collaboration with the environment. In this paper, we introduce various types of smart devices which are developed for assisting the robot by providing sensing and actuation, and present our approach on the integration of these devices to construct the smart home environment. Finally, we discuss the future directions of our project.},
author = {{Seung-Ho Baeg} and {Jae-Han Park} and {Jaehan Koh} and {Kyung-Wook Park} and {Moon-Hong Baeg}},
booktitle = {International Conference on Control, Automation and Systems},
doi = {10.1109/ICCAS.2007.4407059},
isbn = {978-89-950038-6-2},
keywords = {RFID tags,Sensor Networks,Service Robotics,Smart Environment},
pages = {1078--1082},
publisher = {IEEE},
title = {{Building a Smart Home Environment for Service Robots Based on RFID and Sensor Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4407059},
year = {2007}
}
@inproceedings{Hung2011,
abstract = {The first step towards analysing social interactive behaviour in crowded$\backslash$nenvironments is to identify who is interacting with whom. This paper$\backslash$npresents a new method for detecting focused encounters or F-formations$\backslash$nin a crowded, real-life social environment. An F-formation is a specific$\backslash$ninstance of a group of people who are congregated together with the$\backslash$nintent of conversing and exchanging information with each other.$\backslash$nWe propose a new method of estimating F-formations using a graph$\backslash$nclustering algorithm by formulating the problem in terms of identifying$\backslash$ndominant sets. A dominant set is a form of maximal clique which occurs$\backslash$nin edge weighted graphs. As well as using the proximity between people,$\backslash$nbody orientation information is used; we propose a socially motivated$\backslash$nestimate of focus orientation (SMEFO), which is calculated with location$\backslash$ninformation only. Our experiments show significant improvements in$\backslash$nperformance over the existing modularity cut algorithm and indicates$\backslash$nthe effectiveness of using a local social context for detecting F-formations.},
address = {New York, New York, USA},
author = {Hung, Hayley and Kr{\"{o}}se, Ben},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/2070481.2070525},
isbn = {9781450306416},
keywords = {F-formations,dominant sets,f-formations,graph,human behavior,modularity cut,poster},
pages = {231},
publisher = {ACM Press},
title = {{Detecting F-Formations as Dominant Sets}},
url = {http://doi.acm.org/10.1145/2070481.2070525 http://dl.acm.org/citation.cfm?doid=2070481.2070525},
year = {2011}
}
@article{Pohling2018,
author = {Pohling, Marian and Leichsenring, Christian and Hermann, Thomas},
doi = {10.3233/AIS-190533},
issn = {18761372},
journal = {Journal of Ambient Intelligence and Smart Environments},
keywords = {architecture,base cube one,bco,cognitive service robotics apartment,csra,discovery,hardware abstraction layer,location-based,service,service oriented architecture,smart environment,smart home,soa},
month = {9},
number = {5},
pages = {373--401},
title = {{Base Cube One: A Location-Addressable Service-Oriented Smart Environment Framework}},
url = {https://www.medra.org/servlet/aliasResolver?alias=iospress{\&}doi=10.3233/AIS-190533},
volume = {11},
year = {2019}
}
@incollection{Alameda-Pineda2018,
abstract = {“Free-standing conversational groups” are what we call the elementary building blocks of social interactions formed in settings when people are standing and congregate in groups. The automatic detection, analysis, and tracking of such structural conversational units captured on camera poses many interesting chal- lenges for the research community. First, although delineating these formations is strongly linked to other behavioral cues such asheadandbody poses, finding meth- ods that successfully describe and exploit these links is not obvious. Second, the use of visual data is crucial, but when analyzing crowded scenes, one must account for occlusions and low-resolution images. In this regard, the use of other sensing technologies such as wearable devices can facilitate the analysis of social interac- tions by complementing the visual information. Yet the exploitation of multiple modalities poses other challenges in terms of data synchronization, calibration, and fusion. In this chapter, we discuss recent advances in multimodal social scene analysis, in particular for the detection of conversational groups or F-formations [Kendon 1990]. More precisely, a multimodal joint head and body pose estimator is described and compared to other recent approaches for head and body pose esti- mation and F-formation detection. Experimental results on the recently published SALSA dataset are reported, they evidence the long road toward a fully automated high-precision social scene analysis framework.},
address = {New York, NY, USA},
author = {Alameda-Pineda, Xavier and Ricci, Elisa and Sebe, Nicu},
booktitle = {Frontiers of Multimedia Research},
doi = {10.1145/3122865.3122869},
editor = {Chang, Shih-Fu},
isbn = {978-1-97000-107-5},
keywords = {conversation,f-formations,groups,interaction},
pages = {51--74},
title = {{Multimodal Analysis of Free-Standing Conversational Groups}},
url = {https://doi.org/10.1145/3122865.3122869},
year = {2018}
}
@article{Lepetit2005,
abstract = {Many applications require tracking of complex 3D objects. These include visual servoing of robotic arms on specific target objects, Augmented Reality systems that require real-time registration of the object to be augmented, and head tracking systems that sophisticated interfaces can use. Computer Vision offers solutions that are cheap, practical and non-invasive. This survey reviews the different techniques and approaches that have been developed by industry and research. First, important mathematical tools are introduced: Camera representation, robust estimation and uncertainty estimation. Then a comprehensive study is given of the numerous approaches developed by the Augmented Reality and Robotics communities, beginning with those that are based on point or planar fiducial marks and moving on to those that avoid the need to engineer the environment by relying on natural features such as edges, texture or interest. Recent advances that avoid manual initialization and failures due to fast motion are also presented. The survery concludes with the different possible choices that should be made when implementing a 3D tracking system and a discussion of the future of vision-based 3D tracking. Because it encompasses many computer vision techniques from lowlevel vision to 3D geometry and includes a comprehensive study of the massive literature on the subject, this survey should be the handbook of the student, the researcher, or the engineer who wants to implement a 3D tracking system. 1},
author = {Lepetit, Vincent and Fua, Pascal},
number = {1},
pages = {1--89},
title = {{Monocular Model-Based 3D Tracking of Rigid Objects : A Survey}},
volume = {1},
year = {2005}
}
@article{Cheng2007,
abstract = {Road situation analysis in Interactive Intelligent Driver-Assistance and SafetyWarning (I2DASW) systems involves estimation and prediction of the position and size of various on-road obstacles. Real-time processing, given incomplete and uncertain information, is a challenge for current object detec- tion and tracking technologies. This paper proposed a develop- ment framework and novel algorithms for road situation analysis based on driving action behavior, where the safety situation is analyzed by simulating real driving action behaviors. First, we review recent development and trends in road situation analy- sis to provide perspective for the related research. Second, we introduce a road situation analysis framework, where onboard sensors provide information about drivers, traffic environment, and vehicles. Finally, on the basis of the previous frameworks, we proposed multiple-obstacle detection and tracking algorithms using multiple sensors including radar, lidar, and a camera, where a decentralized track-to-track fusion approach is introduced to fuse these sensors. In order to reduce the effect of obstacle shape and appearance, we cluster lidar data and then classify obstacles into two categories: static and moving objects. Future collisions are assessed by computation of local tracks of moving obstacles using extended Kalman filter, maximum likelihood estimation to fuse distributed local tracks into global tracks, and finally, compu- tation of future collision distribution from the global tracks. Our experimental results show that our approach is efficient for road situation evaluation and prediction.},
author = {Cheng, Hong and Zheng, Nanning and Zhang, Xuetao and Qin, Junjie and van de Wetering, Huub},
doi = {10.1109/TITS.2006.890073},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
month = {3},
number = {1},
pages = {157--167},
title = {{Interactive Road Situation Analysis for Driver Assistance and Safety Warning Systems: Framework and Algorithms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4114334},
volume = {8},
year = {2007}
}
@article{Aldoma2011,
abstract = {This paper focuses on developing afast and accurate 3D feature for use in object recognition and pose estimation for rigid objects. More specifically, given a set of CAD mod­ els of different objects representing our knoweledge of the world - obtained using high-precission scanners that de­ liver accurate and noiseless data - our goal is to identify and estimate their pose in a real scene obtained by a depth sensor like the Microsoft Kinect. Borrowing ideas from the Viewpoint Feature Histogram (VFH) due to its computa­ tional efficiency and recognition performance, we describe the Clustered Viewpoint Feature Histogram (CVFH) and the cameras roll histogram together with our recognition framework to show that it can be effectively used to recog­ nize objects and 6DOF pose in real environments dealing with partial occlusion, noise and different sensors atributes for training and recognition data. We show that CVFH out­ peiforms VFH and present recognition results using the Mi­ crosoft Kinect Sensor on an object set of 44 objects.},
author = {Aldoma, Aitor and Vincze, Markus and Blodow, Nico and Gossow, David and Gedikli, Suat and Rusu, Radu Bogdan and Bradski, Gary and Garage, Willow},
journal = {International Conference on Computer Vision Workshops (ICCV)},
pages = {585--592},
title = {{CAD-Model Recognition and 6DOF Pose Estimation Using 3D Cues}},
year = {2011}
}
@inproceedings{Gaschler2012a,
address = {Boston, MA},
author = {Gaschler, Andre and Huth, Kerstin and Giuliani, M},
booktitle = {International Conference on Human-Robot Interaction (HRI) Gaze in Human-Robot Interaction Workshop},
keywords = {head pose estimation,social human-,state of interaction},
publisher = {ACM/IEEE},
title = {{Modelling State of Interaction from Head Poses for Social Human-Robot Interaction}},
url = {http://www6.in.tum.de/Main/Publications/Gaschler2012a.pdf},
year = {2012}
}
@inproceedings{Strauß2008,
abstract = {The PIT corpus is a German multi-media corpus of multi-party dialogues recorded in aWizard-of-Oz environment at the University of Ulm. The scenario involves two human dialogue partners interacting with a multi-modal dialogue system in the domain of restaurant selection. In this paper we present the characteristics of the data which was recorded in three sessions resulting in a total of 75 dialogues and about 14 hours of audio and video data. The corpus is available at http://www.uni-ulm.de/in/pit.},
author = {Strau{\ss}, Petra-Maria and Hoffmann, Holger and Minker, Wolfgang and Neumann, Heiko and Palm, G{\"{u}}nther and Scherer, Stefan and Traue, Harald C and Weidenbacher, Ulrich},
booktitle = {Sixth International Conference on Language Resources and Evaluation (LREC'08)},
isbn = {2-9517408-4-0},
keywords = {agent,interaction,multi person,wizard of oz},
pages = {2442--2445},
title = {{The PIT Corpus Of German Multi-Party Dialogues}},
year = {2008}
}
@article{Devitt2006,
abstract = {Bayesian Networks are probabilistic structured representations of domains which have been applied to monitoring and manipulating cause and effects for modelled systems as disparate as the weather, disease and mobile telecommunications networks. Although useful, Bayesian Networks are notoriously difficult to build accurately and efficiently which has somewhat limited their application to real world problems. Ontologies are also a structured representation of knowledge, encoding facts and rules about a given domain. This paper outlines an approach to harness the knowledge and inference capabilities inherent in an ontology model to automate the construction of Bayesian Networks to accurately represent a domain of interest. The approach was implemented in the context of an adaptive, self-configuring networkmanagement systemin the telecommunications domain. In this system, the ontology model has the dual function of knowledge repository and facilitator of automated workflows and the generated BN serves to monitor effects of management activity, forming part of a feedback look for self-configuration decisions and tasks.},
author = {Devitt, Ann and Danev, Boris and Matusikova, Katarina},
journal = {Applied Ontology},
keywords = {bayes networks,bayesian network,generate,inference,knowledge representation,ontology,semi-automatic},
number = {1},
title = {{Constructing Bayesian Networks Automatically using Ontologies}},
url = {http://hdl.handle.net/2262/5322{\%}5Cnhttp://www.tara.tcd.ie/handle/2262/5322},
volume = {1},
year = {2006}
}
@article{dautenhahn2007socially,
author = {Dautenhahn, Kerstin},
doi = {10.1098/rstb.2006.2004},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
month = {4},
number = {1480},
pages = {679--704},
title = {{Socially Intelligent Robots: Dimensions of Human–Robot Interaction}},
url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2006.2004 https://royalsocietypublishing.org/doi/10.1098/rstb.2006.2004},
volume = {362},
year = {2007}
}
@article{Dautenhahn1998,
abstract = {In this article, socially intelligent agents (SIAs) are understood to be agents that do not only from an observer point of view behave socially but that are able to recognize and identify other agents and establish and maintain relationships to other agents. The process of building SIAs is influenced by what the human as the designer considers "social," and conversely, agent tools that are behaving socially can influence human conceptions of sociality. A cognitive technology (CT) approach toward designing SIAs affords an opportunity to study the process of (1) how new forms of interactions and functionalities and use of technology can emerge at the human-tool interface, (2) how social agents can constrain their cognitive and social potential, and (3) how social agent technology and human (social) cognition can co-evolve and co-adapt and result in new forms of sociality. Agent-human interaction requires a cognitive fit between SIA technology and the human-in-the-loop as designer of, user of, and participant in social interactions. Aspects of human social psychology, e.g., storytelling, empathy, embodiment, and historical and ecological grounding, can contribute to a believable and cognitively well-balanced design of SIA technology in order to further the relationship between humans and agent tools. It is hoped that approaches to believability based on these concepts can avoid the "shallowness" chat merely takes advantage of the anthropomorphizing tendency in humans. This approach is put into the general framework of Embodied Artificial Life (EAL) research. The article concludes with a terminology and list of guidelines useful for SIA design.},
author = {Dautenhahn, Kerstin},
doi = {10.1080/088395198117550},
issn = {0883-9514},
journal = {Applied Artificial Intelligence},
month = {10},
number = {7-8},
pages = {573--617},
title = {{The Art of Designing Socially Intelligent Agents: Science, Fiction, and the Human in the Loop}},
url = {http://www.tandfonline.com/doi/abs/10.1080/088395198117550},
volume = {12},
year = {1998}
}
@article{Potamitis2003,
abstract = {We present an integrated system that uses speech as a natural input modality to provide user-friendly access to information and entertainment devices installed in a real home environment. The system is based on a combination of beamforming techniques and speech recognition. The general problem addressed in this work is that of hands-free speech recognition in a reverberant room where users walk while engaged in conversation in the presence of different types of house-specific noisy conditions (e.g. TV/radio broadcast, interfering speakers, ventilator/air-condition noise, etc). The paper focuses on implementation details and practical considerations concerning the integration of diverse technologies into a working system.},
author = {Potamitis, Ilyas and Georgila, K. and Fakotakis, Nikos and Kokkinakis, George},
journal = {European Conference on Speech Communication and Technology},
keywords = {multiparty,smart home,verbal interaction},
title = {{An Integrated System for Smart-Home Control of Appliances Based on Remote Speech Interaction}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.78.2851{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@article{Champion2017,
abstract = {This paper addresses the challenge of viewing and navigating Bayesian networks as their structural size and complexity grow. Starting with a review of the state of the art of visualizing Bayesian networks, an area which has largely been passed over, we improve upon existing visualizations in three ways. First, we apply a disciplined approach to the graphic design of the basic elements of the Bayesian network. Second, we propose a technique for direct, visual comparison of posterior distributions resulting from alternative evidence sets. Third, we leverage a central mathematical tool in information theory, to assist the user in finding variables of interest in the network, and to reduce visual complexity where unimportant. We present our methods applied to two modestly large Bayesian networks constructed from real-world data sets. Results suggest the new techniques can be a useful tool for discovering information flow phenomena, and also for qualitative comparisons of different evidence configurations, especially in large probabilistic networks.},
archivePrefix = {arXiv},
author = {Champion, Clifford and Elkan, Charles},
eprint = {1707.00791},
month = {7},
title = {{Visualizing the Consequences of Evidence in Bayesian Networks}},
url = {http://arxiv.org/abs/1707.00791},
year = {2017}
}
@article{Mavridis2015,
abstract = {In this paper, an overview of human-robot interactive communication is presented, covering verbal as well as non-verbal aspects. Following a historical introduction, and motivation towards fluid human-robot communication, ten desiderata are proposed, which provide an organizational axis both of recent as well as of future research on human-robot communication. Then, the ten desiderata are examined in detail, culminating in a unifying discussion, and a forward-looking conclusion.},
archivePrefix = {arXiv},
author = {Mavridis, Nikolaos},
doi = {10.1016/j.robot.2014.09.031},
eprint = {1401.4994},
isbn = {0921-8890},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Human-robot communication,Human-robot interaction,Non-verbal,Survey,Verbal,communication,interaction,multi person,multi-modal},
month = {1},
number = {P1},
pages = {22--35},
publisher = {Elsevier B.V.},
title = {{A review of verbal and non-verbal human–robot interactive communication}},
url = {http://dx.doi.org/10.1016/j.robot.2014.09.031 http://linkinghub.elsevier.com/retrieve/pii/S0921889014002164},
volume = {63},
year = {2015}
}
@inproceedings{Akhtiamov2017,
abstract = {The necessity of addressee detection arises in multiparty spoken dialogue systems which deal with human-human-computer interaction. In order to cope with this kind of interaction, such a system is supposed to determine whether the user is addressing the system or another human. The present study is focused on multimodal addressee detection and describes three levels of speech and text analysis: acoustical, syntactical, and lexical. We define the connection between different levels of analysis and the classification performance for different categories of speech and determine the dependence of addressee detection performance on speech recognition accuracy. We also compare the obtained results with the results of the original research performed by the authors of the Smart Video Corpus which we use in our computations. Our most effective meta-classifier working with acoustical, syntactical, and lexical features reaches an unweighted average recall equal to 0.917 showing almost a nine percent advantage over the best baseline model, though this baseline classifier additionally uses head orientation data. We also propose a universal meta-model based on acoustical and syntactical analysis, which may theoretically be applied in different domains.},
author = {Akhtiamov, Oleg and Sidorov, Maxim and Karpov, Alexey A. and Minker, Wolfgang},
booktitle = {Conference of the International Speech Communication Association (Interspeech)},
doi = {10.21437/Interspeech.2017-501},
keywords = {[Electronic Manuscript],acoustic,addressee,multi person},
month = {8},
pages = {2521--2525},
publisher = {ISCA},
title = {{Speech and Text Analysis for Multimodal Addressee Detection in Human-Human-Computer Interaction}},
url = {http://www.isca-speech.org/archive/Interspeech{\_}2017/abstracts/0501.html},
year = {2017}
}
@article{Dim2013,
author = {Dim, Eyal and Kuflik, Tsvi},
journal = {International Workshop on Location Awareness for Mixed and Dual Reality},
keywords = {dim},
pages = {25--28},
title = {{Social F-Formation in Blended Reality}},
year = {2013}
}
@phdthesis{Jin2015,
author = {Jin, Huiliang},
keywords = {Communication interdispositif,Ecran public,Inter-device communication,Interaction proxmique,Proxemic interaction,Public display,Smart city,Ville intelligente,proxemics,smart environments,user interfaces},
school = {Ecole Centrale de Lyon},
title = {{Proxemic interaction and migratable user interface: applied to smart city}},
type = {Theses},
url = {https://tel.archives-ouvertes.fr/tel-01127427},
year = {2014}
}
@article{powers2008,
author = {Powers, David},
journal = {Journal of Machine Learning Technologies},
number = {1},
pages = {37--63},
title = {{Evaluation: From Precision, Recall and F-Factor to ROC, Informedness, Markedness {\&} Correlation}},
volume = {2},
year = {2011}
}
@inproceedings{Ricci2015,
abstract = {We present a novel approach for jointly estimating targets' head, body orientations and conversational groups called F-formations from a distant social scene (e.g., a cocktail party captured by surveillance cameras). Differing from related works that have (i) coupled head and body pose learning by exploiting the limited range of orientations that the two can jointly take, or (ii) determined F-formations based on the mutual head (but not body) orientations of interactors, we present a unified framework to jointly infer both (i) and (ii). Apart from exploiting spatial and orientation relationships, we also integrate cues pertaining to temporal consistency and occlusions, which are beneficial while handling low-resolution data under surveillance settings. Efficacy of the joint inference framework reflects via increased head, body pose and F-formation estimation accuracy over the state-of-the-art, as confirmed by extensive experiments on two social datasets.},
author = {Ricci, Elisa and Varadarajan, Jagannadan and Subramanian, Ramanathan and Bulo, Samuel Rota and Ahuja, Narendra and Lanz, Oswald},
booktitle = {International Conference on Computer Vision (ICCV)},
doi = {10.1109/ICCV.2015.529},
isbn = {978-1-4673-8391-2},
keywords = {F-Formatino,groups,orientation,pose},
month = {12},
pages = {4660--4668},
publisher = {IEEE},
title = {{Uncovering Interactions and Interactors: Joint Estimation of Head, Body Orientation and F-Formations from Surveillance Videos}},
url = {http://ieeexplore.ieee.org/document/7410886/},
volume = {2015 Inter},
year = {2015}
}
@inproceedings{Yanco2004,
abstract = {This paper extends a tmonomy af human- robot interaction (HN) introduced in 2002 [I] to include additional categories as well as updates to the categories from the original taxouoniy. New classifications include measures of the social nature of the task (huntan interaction roles and humon-robot pl7.vsical proximi@), task {\&}pel and robot morpholop.},
author = {Yanco, H.A. and Drury, J.},
booktitle = {2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)},
doi = {10.1109/ICSMC.2004.1400763},
isbn = {0-7803-8567-5},
keywords = {hc,review,taxonomy},
pages = {2841--2846},
publisher = {IEEE},
title = {{Classifying human-robot interaction: an updated taxonomy}},
url = {http://www.researchgate.net/publication/220755236{\_}Classifying{\_}human-robot{\_}interaction{\_}an{\_}updated{\_}taxonomy/file/9c9605229dde7b60e0.pdf http://ieeexplore.ieee.org/document/1400763/},
volume = {3},
year = {2004}
}
@article{Prankl2011,
abstract = {Man-made environments are abundant with pla- nar surfaces which have attractive properties for robotics manipulation tasks and are a prerequisite for a variety of vision tasks. This work presents automatic on-line 3D object model ac- quisition assuming a robot to manipulate the object. Objects are represented with piecewise planar surfaces in a spatio-temporal graph. Planes once detected as homographies are tracked and serve as priors in subsequent images. After reconstruction of the planes the 3D motion is analyzed and initial object hypotheses are created. In case planes start moving independently a split event is triggered, the spatio-temporal object graph is traced back and visible planes as well as occluded planes are assigned to the most probable split object. The novelty of this framework is to formalize Multi-body Structure-and-Motion (MSaM), that is, to segment interest point tracks into different rigid objects and compute the multiple-view geometry of each object, with Minimal Description Length (MDL) based on model selection of planes in an incremental manner. Thus, object models are built from planes, which directly can be used for robotic manipulation.},
author = {Prankl, Johann and Zillich, Michael and Vincze, Markus},
doi = {10.1109/ICRA.2011.5979935},
isbn = {978-1-61284-386-5},
journal = {2011 IEEE International Conference on Robotics and Automation},
month = {5},
pages = {1784--1790},
publisher = {Ieee},
title = {{3D piecewise planar object model for robotics manipulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5979935},
year = {2011}
}
@inproceedings{Kasteren2008,
abstract = {A sensor system capable of automatically recognizing activ- ities would allow many potential ubiquitous applications. In this paper, we present an easy to install sensor network and an accurate but inexpensive annotation method. A recorded dataset consisting of 28 days of sensor data and its anno- tation is described and made available to the community. Through a number of experiments we show how the hid- den Markov model and conditional random fields perform in recognizing activities. We achieve a timeslice accuracy of 95.6{\%} and a class accuracy of 79.4{\%}.},
author = {Kasteren, Tim Van and Noulas, Athanasios and Englebienne, Gwenn and Kr, Ben},
booktitle = {UbiComp '08 Proceedings of the 10th international conference on Ubiquitous computing},
isbn = {9781605581361},
keywords = {activity recognition},
pages = {1--9},
title = {{Accurate Activity Recognition in a Home Setting}},
year = {2008}
}
@book{Nugent2011,
abstract = {This book consists of a number of chapters addressing different aspects of activity recognition, roughly in three main categories of topics. The first topic will be focused on activity modeling, representation and reasoning using mathematical models, knowledge representation formalisms and AI techniques. The second topic will concentrate on activity recognition methods and algorithms. Apart from traditional methods based on data mining and machine learning, we are particularly interested in novel approaches, such as the ontology-based approach, that facilitate data integration, sharing and automatic/automated processing. In the third topic we intend to cover novel architectures and frameworks for activity recognition, which are scalable and applicable to large scale distributed dynamic environments. In addition, this topic will also include the underpinning technological infrastructure, i.e. tools and APIs, that supports function/capability sharing and reuse, and rapid development and deployment of technological solutions. The fourth category of topic will be dedicated to representative applications of activity recognition in intelligent environments, which address the life cycle of activity recognition and their use for novel functions of the end-user systems with comprehensive implementation, prototyping and evaluation. This will include a wide range of application scenarios, such as smart homes, intelligent conference venues and cars.},
address = {Paris},
author = {Nugent, Chris D. and Biswas, Jit and Hoey, Jesse},
doi = {10.2991/978-94-91216-05-3},
editor = {Chen, Liming and Nugent, Chris D. and Biswas, Jit and Hoey, Jesse},
isbn = {978-90-78677-42-0},
keywords = {activity recognition,baseline,bayes,conditional random fields,hiden markov model,probabilistic,semi-markov model},
number = {May},
pages = {344},
publisher = {Atlantis Press},
series = {Atlantis Ambient and Pervasive Intelligence},
title = {{Activity Recognition in Pervasive Intelligent Environments}},
url = {http://books.google.com/books?id=KKKdz7Ou8iUC{\&}pgis=1 http://link.springer.com/10.2991/978-94-91216-05-3},
volume = {4},
year = {2011}
}
@article{Truong2018,
abstract = {We propose a unified framework for approaching pose prediction, and socially aware robot navigation, which enables a mobile service robot to safely and socially approach a dynamic human or human group in a social environment. The proposed framework is composed of four major functional blocks: 1) human detection and human features extraction to estimate the human states, and the social interaction information from the socio-spatio-temporal characteristics of a human and a group of humans; 2) a dynamic social zone (DSZ) consisting of an extended personal space and a social interaction space is modeled by the human states and social interaction information to represent space around the human and human group; 3) the approaching pose of the robot to a human or a human group is predicted using the DSZ and the environmental surroundings; and 4) the DSZ and the estimated approaching pose are incorporated into a motion planning system, comprising a local path planner and dynamic window approach technique, to generate the motion control commands for the mobile robot. We evaluate the developed framework through both simulation and real-world experiments under the newly proposed human safety and comfort indices, including the social individual index, social group index, and social direction index. The results show that the unified framework is fully capable of driving a mobile robot to approach both stationary and moving humans and human groups in a socially acceptable manner while guaranteeing human safety and comfort. {\textcopyright} 2016 IEEE.},
author = {Truong, Xuan-Tung and Ngo, Trung-Dung},
doi = {10.1109/TCDS.2017.2751963},
issn = {2379-8920},
journal = {IEEE Transactions on Cognitive and Developmental Systems},
keywords = {Approaching humans,dynamic social zone (DSZ),human safety and comfort indices (HSCIs),social robot,socially aware robot navigation},
month = {9},
number = {3},
pages = {557--572},
publisher = {IEEE},
title = {{“To Approach Humans?”: A Unified Framework for Approaching Pose Prediction and Socially Aware Robot Navigation}},
url = {https://ieeexplore.ieee.org/document/8036225/},
volume = {10},
year = {2018}
}
@inproceedings{Eyssel2011,
abstract = {Recent research has shown that anthropomorphism represents a means to facilitate HRI. Under which conditions do people anthropomorphize robots and other nonhuman agents? This research question was investigated in an experiment that manipulated participants' anticipation of a prospective human-robot interaction (HRI) with a robot whose behavior was characterized by either low or high predictability. We examined effects of these factors on perceptions of anthropomorphism and acceptance of the robot. Innovatively, the present research demonstrates that anticipation of HRI with an unpredictable agent increased anthropomorphic inferences and acceptance of the robot. Implications for future research on psychological determinants of anthropomorphism are discussed.},
address = {New York, New York, USA},
author = {Eyssel, Friederike and Kuchenbrandt, Dieta and Bobinger, Simon},
booktitle = {Proceedings of the 6th international conference on Human-robot interaction - HRI '11},
doi = {10.1145/1957656.1957673},
isbn = {9781450305617},
issn = {2167-2121},
keywords = {anthropomorphism,human-robot interaction,motivation,predictability of robot behavior},
pages = {61},
publisher = {ACM Press},
title = {{Effects of anticipated human-robot interaction and predictability of robot behavior on perceptions of anthropomorphism}},
url = {http://portal.acm.org/citation.cfm?doid=1957656.1957673},
year = {2011}
}
@inproceedings{Michaud2005,
abstract = {Designing robots that are capable of interacting with humans in real life settings is a challenging task. One key issue is the integration of multiple modalities (e.g., mo- bility, physical structure, navigation, vision, audition, dialogue, reasoning) into a coherent framework. Taking the AAAI Mobile Robot Challenge (making a robot attend the National confer- ence on Artificial Intelligence) as the experimental context, we are currently addressing hardware, software and computation integration issues involved in designing a robot capable of sophisticated interaction with humans. This paper reports on our design solutions and the current status of the work, along with the potential impacts this design will have on human-robot interaction research.},
author = {Michaud, F. and Brosseau, Y. and Cote, C. and Letourneau, D. and Moisan, P. and Ponchon, A. and Raievsky, C. and Valin, J.-M. and Beaudryy, E. and Kabanza, F.},
booktitle = {ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.},
doi = {10.1109/ROMAN.2005.1513775},
isbn = {0-7803-9274-4},
keywords = {planning},
pages = {172--177},
publisher = {IEEE},
title = {{Modularity and integration in the design of a socially interactive robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1513775},
year = {2005}
}
@inproceedings{Sebastian2017,
abstract = {In this paper, we revisit a real-time socially aware navigation planner which helps a mobile robot to navigate alongside humans in a socially acceptable manner. This navigation planner is a modification of nav{\_}core package of Robot Operating System (ROS), based upon earlier work and further modified to use only egocentric sensors. The planner can be utilized to provide safe as well as socially appropriate robot navigation. Primitive features including interpersonal distance between the robot and an interaction partner and features of the environment (such as hallways detected in real-time) are used to reason about the current state of an interaction. Gaussian Mixture Models (GMM) are trained over these features from human-human interaction demonstrations of various interaction scenarios. This model is both used to discriminate different human actions related to their navigation behavior and to help in the trajectory selection process to provide a social-appropriateness score for a potential trajectory. This paper presents an evaluation done in simulation while utilizing data from real human interactions.},
author = {Sebastian, Meera and Banisetty, Santosh Balajee and Feil-Seifer, David},
booktitle = {2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)},
doi = {10.1109/ROMAN.2017.8172334},
isbn = {978-1-5386-3518-6},
keywords = {Robotic Etiquette,Social Intelligence for Robots,Social Learning and Skill Acquisition Via Teaching},
month = {8},
pages = {405--410},
publisher = {IEEE},
title = {{Socially-aware navigation planner using models of human-human interaction}},
url = {http://ieeexplore.ieee.org/document/8172334/},
volume = {2017-Janua},
year = {2017}
}
@incollection{Lindner2011,
archivePrefix = {arXiv},
author = {Lindner, Felix and Eschenbach, Carola},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-642-23196-4_16},
editor = {Egenhofer, Max and Giudice, Nicholas and Moratz, Reinhard and Worboys, Michael},
eprint = {9780201398298},
isbn = {9783642242632},
pages = {283--303},
title = {{Towards a Formalization of Social Spaces for Socially Aware Robots}},
url = {http://www.ulb.tu-darmstadt.de/tocs/59142804.pdf http://link.springer.com/10.1007/3-540-68339-9{\_}34 http://link.springer.com/10.1007/978-3-642-23196-4{\_}16},
year = {2011}
}
@article{Mormul2016,
author = {Mormul, Mathias and Hirmer, Pascal and Wieland, Matthias and Mitschang, Bernhard},
doi = {10.1007/s00450-016-0335-2},
issn = {1865-2034},
journal = {Computer Science - Research and Development},
keywords = {Context,Context-awareness,Data management,Internet of things,Situation,Situation-awareness,context,context-awareness,data manage-,internet of things,ment,model,situation,situation-awareness},
month = {11},
publisher = {Springer Berlin Heidelberg},
title = {{Situation model as interface between situation recognition and situation-aware applications}},
url = {http://link.springer.com/10.1007/s00450-016-0335-2},
year = {2016}
}
@inproceedings{Li2012a,
abstract = {— Localization of multiple active speakers in natural environments with only two microphones is a challenging problem. Reverberation degrades performance of speaker local-ization based exclusively on directional cues. The audio modal-ity alone has problems with localization accuracy while the video modality alone has problems with false speaker activity detections. This paper presents an approach based on audio-visual fusion in two stages. In the first stage, speaker activity is detected based on the audio-visual fusion which can handle false lip movements. In the second stage, a Gaussian fusion method is proposed to integrate the estimates of both modalities. As a consequence, the localization accuracy and robustness compared to the audio/video modality alone is significantly increased. Experimental results in various scenarios confirmed the improved performance of the proposed system.},
author = {Li, Zhao and Herfet, Thorsten and Grochulla, Martin and Thormahlen, Thorsten},
booktitle = {2012 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)},
doi = {10.1109/MFI.2012.6343015},
isbn = {978-1-4673-2512-7},
keywords = {audio visual,speaker localisation},
month = {9},
pages = {1--7},
publisher = {IEEE},
title = {{Multiple active speaker localization based on audio-visual fusion in two stages}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6343015},
year = {2012}
}
@article{Pan2009,
abstract = {Off-line model reconstruction relies on an image collection phase and a slow recon- struction phase, requiring a long time to verify a model obtained from an image sequence is acceptable. We propose a new model acquisition system, called ProFORMA, which generates a 3D model on-line as the input sequence is being collected. As the user rotates the object in front of a stationary camera, a partial model is reconstructed and displayed to the user to assist view planning. The model is also used by the system to robustly track the pose of the object. Models are rapidly produced through a Delaunay tetrahe- dralisation of points obtained from on-line structure from motion estimation, followed by a probabilistic tetrahedron carving step to obtain a textured surface mesh of the object. 1},
author = {Pan, Qi and Reitmayr, Gerhard and Drummond, Tom},
doi = {10.5244/C.23.112},
isbn = {1-901725-39-1},
journal = {Procedings of the British Machine Vision Conference 2009},
number = {c},
pages = {112.1--112.11},
publisher = {British Machine Vision Association},
title = {{ProFORMA: Probabilistic Feature-based On-line Rapid Model Acquisition}},
url = {http://www.bmva.org/bmvc/2009/Papers/Paper297/Paper297.html},
year = {2009}
}
@article{Riley2007,
abstract = {We conducted a theoretical investigation of a complex command and control (C2) operation--the manoeuvres planning processes in Army land-battle situations, to improve understanding of how technology can best be designed to support planning and course of action development. We drew upon results from cognitive task analyses and interviews with subject matter experts and insights gleaned from observations of Army training exercises and experiments to make inferences on the C2 activities carried out in preparation for tactical manoeuvres. In this paper, we summarize several critical human factors issues associated with planning in a rapidly evolving environment, as identified in our investigation, and describe system design concepts aimed at addressing these challenges to distributed collaborative planning of C2 activities. We conclude with implications for the application of these findings to other C2 domains.},
author = {Riley, Jennifer M and Endsley, Mica R and Bolstad, Cheryl a and Cuevas, Haydee M},
doi = {10.1080/00140130600612614},
issn = {0014-0139},
journal = {Ergonomics},
keywords = {Awareness,Cognition,Cooperative Behavior,Human Engineering,Humans,Interviews as Topic,Military Medicine,Military Medicine: organization {\&} administration,Military Personnel,Military Personnel: psychology,Planning Techniques,Software,Systems Analysis,Task Performance and Analysis,United States,User-Computer Interface,War},
number = {12-13},
pages = {1139--53},
title = {{Collaborative planning and situation awareness in Army command and control.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17008250},
volume = {49},
year = {2007}
}
@article{Hayashi1988a,
abstract = {This paper reports on a comparative study of the use of simultaneous talk or 'sync talk' by Japanese and English speakers. The analysis is based on a concept of floor derived in part from Edelsky [Language in Society, 10, 383-421 (1981)], but adapted and developed for the purposes of the present study. Analysis of tape-recorded and videotaped conversations among Japanese speakers (speaking Japanese) and among American speakers (speaking English) shows that, while both groups use sync talk, it is used with much greater frequency by the Japanese speakers. This can be shown by quantitative analysis. Furthermore, in addition to verbal simultaneous talk, Japanese speakers were found to engage in more nonverbal interactive behavior during conversation than their American counterparts. The final section of this paper discusses the implications of these findings for cross-cultural communication and language learning.},
author = {Hayashi, Reiko},
doi = {10.1111/j.1467-971X.1988.tb00237.x},
issn = {0883-2919},
journal = {World Englishes},
month = {11},
number = {3},
pages = {269--288},
title = {{Simultaneous Talk? From the Perspective of Floor Management of English and Japanese Speakers}},
url = {http://doi.wiley.com/10.1111/j.1467-971X.1988.tb00237.x},
volume = {7},
year = {1988}
}
@inproceedings{Asfour2006,
abstract = {In this paper, we present a new humanoid robot currently being developed for applications in human-centered environments. In order for humanoid robots to enter human- centered environments, it is indispensable to equip them with manipulative, perceptive and communicative skills necessary for real-time interaction with the environment and humans. The goal of our work is to provide reliable and highly integrated humanoid platforms which on the one hand allow the implementation and tests of various research activities and on the other hand the realization of service tasks in a household scenario. We introduce the different subsystems of the robot. We present the kinematics, sensors, and the hardware and software architecture. We propose a hierarchically organized architecture and introduce the mapping of the functional features in this architecture into hardware and software modules. We also describe different skills related to real-time object localization and motor control, which have been realized and integrated into the entire control architecture.},
author = {Asfour, T and Regenstein, K and Azad, P and Schr, J and Bierbaum, A and Vahrenkamp, N and Dillmann, R},
booktitle = {Humanoid Robots, 2006 6th IEEE-RAS International Conference on},
isbn = {142440200X},
pages = {169--175},
title = {{ARMAR-III: An Integrated Humanoid Platform for Sensory-Motor Control}},
year = {2006}
}
@inproceedings{Lang2003,
abstract = {In order to enable the widespread use of robots in home and office environments, systems with natural interaction capabilities have to be developed. A prerequisite for natural interaction is the robot's ability to automatically recognize when and how long a person's attention is directed towards it for communication. As in open environments several persons can be present simultaneously, the detection of the communication partner is of particular importance. In this paper we present an attention system for a mobile robot which enables the robot to shift its attention to the person of interest and to maintain attention during interaction. Our approach is based on a method for multi-modal person tracking which uses a pan-tilt camera for face recognition, two microphones for sound source localization, and a laser range finder for leg detection. Shifting of attention is realized by turning the camera into the direction of the person which is currently speaking. From the orientation of the head it is decided whether the speaker addresses the robot. The performance of the proposed approach is demonstrated with an evaluation. In addition, qualitative results from the performance of the robot at the exhibition part of the ICVS'03 are provided.},
address = {New York, New York, USA},
author = {Lang, Sebastian and Kleinehagenbrock, Marcus and Hohenner, Sascha and Fritsch, Jannik and Fink, Gernot a and Sagerer, Gerhard},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/958432.958441},
isbn = {1581136218},
keywords = {attention,human robot interaction,multi modal person tracking},
pages = {28},
publisher = {ACM Press},
title = {{Providing the Basis for Human-Robot-Interaction: A Multi-Modal Attention System for a Mobile Robot}},
url = {http://portal.acm.org/citation.cfm?id=958441 http://portal.acm.org/citation.cfm?doid=958432.958441},
year = {2003}
}
@article{Zarkowski2019,
author = {{\.{Z}}arkowski, Mateusz},
doi = {10.1007/s12369-019-00603-1},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {human,multi-party interaction,robot control,robot interaction,social robot,user assessment},
month = {11},
title = {{Multi-party Turn-Taking in Repeated Human–Robot Interactions: An Interdisciplinary Evaluation}},
url = {http://link.springer.com/10.1007/s12369-019-00603-1},
year = {2019}
}
@incollection{Vascon2019,
author = {Vascon, Sebastiano and Pelillo, Marcello},
booktitle = {Multimodal Behavior Analysis in the Wild},
doi = {10.1016/B978-0-12-814601-9.00024-9},
pages = {247--267},
publisher = {Elsevier},
title = {{Detecting conversational groups in images using clustering games}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780128146019000249},
year = {2019}
}
@techreport{Roure1999,
annote = {Definition: "A computer program is said to learn from experiance E with respectto some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experiance E." [25]


* inference learning as process of inferential search through knowledge space
* learning Bayesian networks may be classified as sort of inductive, unsupervised and data driven learning},
author = {Roure, Josep and Sang{\"{u}}esa, Ram{\'{o}}n},
keywords = {bayesian networks,incremental learning,learning,machine learning},
title = {{Incremental Methods for Bayesian Network Learning}},
year = {1999}
}
@inproceedings{Thangarajah2006,
address = {New York, New York, USA},
author = {Thangarajah, John and Padgham, Lin and Sardina, Sebastian},
booktitle = {Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems - AAMAS '06},
doi = {10.1145/1160633.1160819},
isbn = {1595933034},
keywords = {agent and multi-agent architectures,agent systems,frameworks,infras-,tructures and environments for},
pages = {1049},
publisher = {ACM Press},
title = {{Modelling situations in intelligent agents}},
url = {http://portal.acm.org/citation.cfm?doid=1160633.1160819},
year = {2006}
}
@misc{lippe1995,
author = {von der Lippe, J{\"{u}}rgen},
keywords = {music},
title = {{Man Wei{\ss} Es Nicht}},
year = {1995}
}
@book{Goodwin1981,
author = {Goodwin, Charles},
keywords = {HHI,conversation,gaze,interaction},
publisher = {Academic Press},
title = {{Conversational Organization Interaction Between Speakers And Hearers}},
year = {1981}
}
@inproceedings{Kafer2010,
abstract = {The recognition and prediction of intersection situations and an accompanying threat assessment are an in- dispensable skill of future driver assistance systems. This study focuses on the recognition of situations involving two vehicles at intersections. For each vehicle, a set of possible future motion trajectories is estimated and rated based on a motion database for a time interval of 2-4 s ahead. Possible situations involving two vehicles are generated by a pairwise combination of these individual motion trajectories. An interaction model based on the mutual visibility of the vehicles and the assumption that a driver will attempt to avoid a collision is used to rate possible situations. The correspondingly favoured situations are classified with a probabilistic framework. The proposed method is evaluated on a real-world differential GPS data set acquired during a test drive of about 10 km, including three road intersections. Our method is typically able to recognise the situation correctly about 1.5-3 s before the last vehicle has passed its minimum distance to the centre of the intersection.},
author = {K{\"{a}}fer, Eugen and Hermes, Christoph and W{\"{o}}hler, Christian and Kummert, Franz and Ritter, Helge},
booktitle = {2010 20th International Conference on Pattern Recognition},
doi = {10.1109/ICPR.2010.1029},
isbn = {978-1-4244-7542-1},
keywords = {-situation recognition,as ordered tuples combining,constraint,interaction model,motion prediction,position,states,trajectories,vehicles are represented by,visibility,which are defined,yaw angle},
month = {8},
pages = {4234--4237},
publisher = {IEEE},
title = {{Recognition and Prediction of Situations in Urban Traffic Scenarios}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5597735},
year = {2010}
}
@inproceedings{Bernardin2007,
abstract = {This paper presents a novel system for the automatic and unobtrusive tracking and identification of multiple persons in an indoor environment. Information from several fixed cameras is fused in a particle filter framework to simulta- neously track multiple occupants. A set of steerable fuzzy- controlled pan-tilt-zoom cameras serves to smoothly track persons of interest and opportunistically capture facial close- ups for face identification. In parallel, speech segmentation, sound source localization and speaker identification are per- formed using several far-field microphones and arrays. The information coming asynchronously and sporadically from several sources, such as track updates and spatio-temporally localized visual and acoustic identification cues, is fused at higher level to gradually refine the global scene model and increase the system's confidence in the set of recognized identities. The system has been trained on a small set of users' faces and/or voices and showed good performance in natural meeting scenarios at quickly acquiring their identi- ties and complementing the ID information missing in single modalities.},
address = {New York, New York, USA},
author = {Bernardin, Keni and Stiefelhagen, Rainer},
booktitle = {Proceedings of the 15th international conference on Multimedia - MULTIMEDIA '07},
doi = {10.1145/1291233.1291388},
isbn = {9781595937025},
keywords = {person tracking},
pages = {661},
publisher = {ACM Press},
title = {{Audio-visual multi-person tracking and identification for smart environments}},
url = {http://portal.acm.org/citation.cfm?doid=1291233.1291388},
year = {2007}
}
@article{clopper1934,
abstract = {(1) General Discussion. In facing the problem of statistical estimation it may often be desirable to obtain from a random sample a single estimate, say a, of the value of an unknown parameter, a, in the population sampled. It has always, however, been realised ... $\backslash$n},
author = {Clopper, C. J. and Pearson, E. S.},
doi = {10.2307/2331986},
issn = {00063444},
journal = {Biometrika},
month = {12},
number = {4},
pages = {404},
title = {{The Use of Confidence or Fiducial Limits Illustrated in the Case of the Binomial}},
url = {https://www.jstor.org/stable/2331986?origin=crossref},
volume = {26},
year = {1934}
}
@incollection{Gordon2006,
abstract = {Many applications of 3D object recognition, such as aug- mented reality or robotic manipulation, require an accurate solution for the 3D pose of the recognized objects. This is best accomplished by building a metrically accurate 3D model of the object and all its fea- ture locations, and then fitting this model to features detected in new images. In this chapter, we describe a system for constructing 3D met- ric models from multiple images taken with an uncalibrated handheld camera, recognizing these models in new images, and precisely solving for object pose. This is demonstrated in an augmented reality applica- tion where objects must be recognized, tracked, and superimposed on new images taken from arbitrary viewpoints without perceptible jitter. This approach not only provides for accurate pose, but also allows for integration of features from multiple training images into a single model that provides for more reliable recognition.},
author = {Gordon, Iryna and Lowe, David G},
booktitle = {Toward Category-Level Object Recognition},
pages = {67--82},
title = {{What and Where : 3D Object Recognition with Accurate Pose}},
year = {2006}
}
@article{Ciolek1980,
author = {Ciolek, T. Matthew and Kendon, Adam},
doi = {10.1111/j.1475-682X.1980.tb00022.x},
issn = {0038-0245},
journal = {Sociological Inquiry},
month = {7},
number = {3-4},
pages = {237--271},
title = {{Environment and the Spatial Arrangement of Conversational Encounters}},
url = {http://doi.wiley.com/10.1111/j.1475-682X.1980.tb00022.x},
volume = {50},
year = {1980}
}
@phdthesis{Rusu2009,
abstract = {ENVIRONMENT models serve as important resources for an autonomous robot by provid- ing it with the necessary task-relevant information about its habitat. Their use enables robots to perform their tasks more reliably, flexibly, and efficiently. As autonomous robotic platforms get more sophisticated manipulation capabilities, they also need more expressive and comprehensive environment models: for manipulation purposes their models have to in- clude the objects present in the world, together with their position, form, and other aspects, as well as an interpretation of these objects with respect to the robot tasks. This thesis proposes Semantic 3D Object Models as a novel representation of the robot's operating environment that satisfies these requirements and shows how these models can be automatically acquired from dense 3D range data. The thesis contributes in two important ways to the research area acquisition of environment models. The first contribution is a novel framework for Semantic 3D Object Model acquisition from Point Cloud Data. The functionality of this framework includes robust alignment and integra- tion mechanisms for partial data views, fast segmentation into regions based on local surface characteristics, and reliable object detection, categorization, and reconstruction. The computed models are semantic in that they infer structures in the data that are meaningful with respect to the robot task. Examples of such objects are doors and handles, supporting planes, cupboards, walls, or movable smaller objects. The second key contribution is point cloud representations based on 3D point feature his- tograms (3D-PFHs), which model the local surface geometry for each point. 3D-PFHs dis- tinguish themselves from alternative 3D feature representations in that they are very fast to compute, robust against variations in pose and sampling density, and cope well with noisy sensor data. Their use substantially improves the quality of the Semantic 3D Object Models acquired, as well as the speed with which they are computed. 3D-PFHs come with specific software tools that allow for the learning of surface characteristics based on their underlying geometry, the assembly of most distinctive 3D points from a given cloud, as well as limited view-invariant correspondence search for 3D registration. III The contributions presented in this thesis have been fully implemented and empirically evaluated on different robots performing different tasks in different environments. The first demonstration relates to the problem of cleaning tables by disposing the objects on them into a garbage bin with a personal robotic assistant in the presence of humans in its working space. The framework for Semantic 3D Object Model acquisition is demonstrated and used to con- struct dynamic 3D collision maps, annotate the surrounding world with semantic labels, and extract object clusters supported by tables in real-time performance. The second demonstra- tion presents an on-the-fly model acquisition system for door and handle identification from noisy 3D point cloud maps. Experimental results show good robustness in the presence of large variations in the data, without suffering from the classical under or over-fitting problems usually associated with similar initiatives based on machine learning classifiers. The third ap- plication example tackles the problem of real-time semantic mapping of indoor environments with different kinds of terrain classes, such as walkways and stairs, for the navigation of a six-legged robot with terrain-specific walking modes.},
author = {Rusu, Radu Bogdan},
booktitle = {KI - K{\"{u}}nstliche Intelligenz},
issn = {0933-1875},
month = {8},
number = {4},
school = {Technischen Universit{\"{a}}t M{\"{u}}nchen},
title = {{Semantic 3D Object Maps for Everyday Manipulation in Human Living Environments}},
url = {http://files.rbrusu.com/publications/RusuPhDThesis.pdf},
volume = {24},
year = {2009}
}
@article{Vinciarelli2009,
abstract = {The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence - the ability to recognize human social signals and social behaviours like turn taking, politeness, and disagreement - in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for social signal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially aware computing. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
annote = {13,42,59,218},
author = {Vinciarelli, Alessandro and Pantic, Maja and Bourlard, Herv{\'{e}}},
doi = {10.1016/j.imavis.2008.11.007},
isbn = {9781605583037},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Computer vision,Human behaviour analysis,Social interactions,Social signals,Speech processing,dialog,interaction,social signal processing},
month = {11},
number = {12},
pages = {1743--1759},
publisher = {Elsevier B.V.},
title = {{Social signal processing: Survey of an emerging domain}},
url = {http://dx.doi.org/10.1016/j.imavis.2008.11.007 http://linkinghub.elsevier.com/retrieve/pii/S0262885608002485},
volume = {27},
year = {2009}
}
@inproceedings{Kim2006,
abstract = {This paper proposes the control of smart home environments such as lights and curtains using body gestures. We use a forward spotting scheme that executes gesture segmentation and recognition simultaneously. The start and end points of gestures are determined by zero crossing from negative to positive (or from positive to negative) of a competitive differential observation probability that is defined by the difference of observation probability between the maximal gesture and the non-gesture. We also use the sliding window and accumulative HMMs. We apply the proposed simultaneous gesture segmentation and recognition method to recognize the upperbody gestures for controlling the curtains and lights in a smart home environment. Experimental results show that the proposed method has a good recognition rate of 95.42{\%} for continuously changing gestures.},
author = {Kim, Daehwan and Kim, Daijin},
booktitle = {International Conference on Hybrid Information Technology},
doi = {10.1109/ICHIT.2006.253644},
isbn = {0-7695-2674-8},
keywords = {Gesture recognition,Gesture segmentation,Hidden Markov model,Smart home control},
month = {11},
pages = {439--446},
publisher = {IEEE},
title = {{An Intelligent Smart Home Control Using Body Gestures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4021249 http://ieeexplore.ieee.org/document/4021249/},
volume = {2},
year = {2006}
}
@article{Dautenhahn2005,
abstract = {The study presented in this paper explored people's perceptions and attitudes towards the idea of a future robot companion for the home. A human-centred approach was adopted using questionnaires and human-robot interaction trials to derive data from 28 adults. Results indicated that a large proportion of participants were in favour of a robot companion and saw the potential role as being an assistant, machine or servant. Few wanted a robot companion to be a friend. Household tasks were preferred to child/animal care tasks. Humanlike communication was desirable for a robot companion, whereas humanlike behaviour and appearance were less essential. Results are discussed in relation to future research directions for the development of robot companions.},
author = {Dautenhahn, Kerstin and Woods, Sarah and Kaouri, Christina and Walters, Michael L. and Koay, Kheng Lee and Werry, Iain},
doi = {10.1109/IROS.2005.1545189},
journal = {International Conference on Intelligent Robots and Systems (IROS)},
keywords = {Human perception and attitudes,Robot appearance,Robot companion,Robot-human interaction,Social robotics,hri,study},
pages = {1488--1493},
title = {{What is a Robot Companion - Friend, Assistant or Butler?}},
year = {2005}
}
@book{Burger2009,
author = {Burger, W and Burge, M J},
isbn = {9781846289682},
publisher = {Springer London, Limited},
series = {Texts in Computer Science},
title = {{Digital Image Processing: An Algorithmic Introduction Using Java}},
url = {http://books.google.de/books?id=4gBUz{\_}IkkSsC},
year = {2009}
}
@incollection{Teichman2013,
abstract = {We consider the problem of segmenting and tracking deformable objects in color video with depth (RGBD) data available from commodity sensors such as the Kinect. We frame this problem with very few assumptions - no prior object model, no stationary sensor, no prior 3D map - thus making a solution potentially useful for a large number of applications, including semi-supervised learning, 3D model capture, and object recognition.Our approach makes use of a rich feature set, including local image appearance, depth discontinuities, optical flow, and surface normals to inform the segmentation decision in a conditional random field model. In contrast to previous work, the proposed method learns how to best make use of these features from ground-truth segmented sequences.We provide qualitative and quantitative analyses which demonstrate substantial improvement over the state of the art.},
address = {Berlin, Heidelberg},
author = {Teichman, Alex and Thrun, Sebastian},
booktitle = {Algorithmic Foundations of Robotics X},
doi = {10.1007/978-3-642-36279-8},
editor = {Frazzoli, Emilio and Lozano-Perez, Tomas and Roy, Nicholas and Rus, Daniela},
isbn = {978-3-642-36278-1},
pages = {575--590},
publisher = {Springer Berlin Heidelberg},
series = {Springer Tracts in Advanced Robotics},
title = {{Learning to Segment and Track in RGBD}},
url = {http://link.springer.com/10.1007/978-3-642-36279-8},
volume = {86},
year = {2013}
}
@incollection{Horton2017,
author = {Horton, William S},
booktitle = {The Routledge Handbook of Discourse Processes, 2nd Edition},
number = {January},
title = {{Theories and approaches to the study of conversation and interactive discourse William S. Horton Northwestern University}},
year = {2017}
}
@article{Bennewitz2005,
abstract = { An essential capability for a robot designed to interact with humans is to show attention to the people in its surroundings. To enable a robot to involve multiple persons into interaction requires the maintenance of an accurate belief about the people in the environment. In this paper, we use a probabilistic technique to update the knowledge of the robot based on sensory input. In this way, the robot is able to reason about the uncertainty in its belief about people in the vicinity and is able to shift its attention between different persons. Even people who are not the primary conversational partners are included into the interaction. In practical experiments with a humanoid robot, we demonstrate the effectiveness of our approach.},
author = {Bennewitz, Maren and Faber, Felix and Joho, Dominik and Schreiber, Michael and Behnke, Sven},
doi = {10.1109/IROS.2005.1545158},
isbn = {0780389123},
journal = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS},
pages = {1295--1300},
title = {{Integrating vision and speech for conversations with multiple persons}},
year = {2005}
}
@article{Dautenhahn2003,
abstract = {This paper discusses robots that are operational within a human-inhabited environment. Specifically, we identify different roles that such robots can adopt, reflecting different human-robot relationships. We discuss three different roles of robots in a project where we develop a robot as a therapeutic tool for children with autism: the robot as a therapeutic playmate, the robot as a social mediator, and the robot as a model social agent. Implications of these roles that go beyond this particular project are discussed.},
author = {Dautenhahn, Kerstin},
doi = {10.1017/S0263574703004922},
isbn = {0263-5747},
issn = {02635747},
journal = {Robotica},
keywords = {Autism therapy,Human society,Robots},
month = {8},
number = {4},
pages = {S0263574703004922},
title = {{Roles and functions of robots in human society: implications from research in autism therapy}},
url = {http://www.journals.cambridge.org/abstract{\_}S0263574703004922},
volume = {21},
year = {2003}
}
@techreport{Matsuyama2010,
abstract = {We propose a framework for a robot to participate in and activate multiparty conversation. In multiparty conversation, the robot should select its behavior based on both linguistic information and participation structure. In this paper, we focus on multiparty conversation game “Nandoku,” which is often played in elderly care facilities. The robot acts as one of the participants, and tries to promote the communication activeness. The framework handles the dialogue situation from three aspects: multiparty conversation, game progress and communication activation, and selects the most effective robot's behavior according to these three aspects.},
address = {Arlington, VA},
author = {Matsuyama, Yoichi and Taniyama, Hikaru and Fujie, Shinya and Kobayashi, Tetsunori},
booktitle = {AAAI Fall Symposium - Technical Report},
isbn = {9781577354871},
keywords = {AAAI Technical Report FS-10-05},
number = {FS-10-05},
pages = {68--73},
title = {{Framework of Communication Activation Robot Participating in Multiparty Conversation}},
url = {http://www.aaai.org/ocs/index.php/FSS/FSS10/paper/viewFile/2286/2759},
year = {2010}
}
@inproceedings{Sun2013,
abstract = {Over the last years, the robotics community has made substantial progress in detection and 3D pose estimation of known and unknown objects. However, the question of how to identify objects based on language descriptions has not been investigated in detail. While the computer vision community recently started to investigate the use of attributes for object recognition, these approaches do not consider the task settings typically observed in robotics, where a combination of appearance attributes and object names might be used in referral language to identify specific objects in a scene. In this paper, we introduce an approach for identifying objects based on natural language containing appearance and name attributes. To learn rich RGB-D features needed for attribute classification, we extend recently introduced sparse coding techniques so as to automatically learn attribute-dependent features.We introduce a large data set of attribute descriptions of objects in the RGB-D object dataset. Experiments on this data set demonstrate the strong performance of our approach to language based object identification. We also show that our attribute-dependent features provide significantly better generalization to previously unseen attribute values, thereby enabling more rapid learning of new attribute values.},
author = {Sun, Yuyin and Bo, Liefeng and Fox, Dieter},
booktitle = {2013 IEEE International Conference on Robotics and Automation},
isbn = {9781467356428},
pages = {2088--2095},
title = {{Attribute Based Object Identification}},
year = {2013}
}
@article{Nguyen1988,
author = {Nguyen, V.-D.},
doi = {10.1177/027836498800700301},
issn = {0278-3649},
journal = {The International Journal of Robotics Research},
month = {6},
number = {3},
pages = {3--16},
title = {{Constructing Force- Closure Grasps}},
url = {http://ijr.sagepub.com/cgi/doi/10.1177/027836498800700301},
volume = {7},
year = {1988}
}
@article{Yamaoka2010,
abstract = {In this paper, we report a model that allows a robot to appropriately control its position as it presents information to a user. This capability is indispensable, since in the future, many robots will function in daily situations such as shopkeepers presenting products to customers or museum guides presenting information to visitors. Psychology research suggests that people adjust their positions to establish a joint view toward a target object. Similarly, when a robot presents an object, it should stand at an appropriate position that considers the positions of both the listener and the object to optimize the listener's field of view and establish a joint view. We observed human-human interaction situations, where people presented objects, and developed a model for an information-presenting robot to appropriately adjust its position. Our model consists of four constraints to establish O-space: 1) proximity to listener; 2) proximity to object; 3) listener's field of view; and 4) presenter's field of view. We also experimentally evaluate the effectiveness of our model.},
author = {Yamaoka, Fumitaka and Kanda, Takayuki and Ishiguro, Hiroshi and Hagita, Norihiro},
doi = {10.1109/TRO.2009.2035747},
issn = {1552-3098},
journal = {IEEE Transactions on Robotics},
keywords = {Human-robot interaction,Joint attention,Proximity},
month = {2},
number = {1},
pages = {187--195},
title = {{A Model of Proximity Control for Information-Presenting Robots}},
url = {http://ieeexplore.ieee.org/document/5361334/},
volume = {26},
year = {2010}
}
@misc{Hirmer2015,
author = {Hirmer, Pascal and Wieland, Matthias and Schwarz, Holger and Mitschang, Bernhard and Breitenb{\"{u}}cher, Uwe and Leymann, Frank},
keywords = {recognition,situation,templates},
title = {{SitRS – A Situation Recognition Service based on Modeling and Executing Situation Templates}},
year = {2015}
}
@inproceedings{Carrino2011,
abstract = {In this paper, we describe a multimodal approach for human-smart environment interaction. The input interaction is based on three modalities: deictic gestures, symbolic gestures and isolated-words. The deictic gesture is interpreted using the PTAMM (Parallel Tracking and Multiple Mapping) method exploiting a camera handheld or worn on the user arm. The PTAMM algorithm tracks in real-time the position and orientation of the hand in the environment. This information is used to point real or virtual objects, previously added to the environment, using the optical camera axis. Symbolic hand-gestures and isolated voice commands are recognized and used to interact with the pointed target. Haptic and acoustic feedbacks are provided to the user in order to improve the quality of the interaction. A complete prototype has been realized and a first usability evaluation, assessed with the help of 10 users has shown positive results.},
address = {New York, New York, USA},
author = {Carrino, Stefano and P{\'{e}}clat, Alexandre and Mugellini, Elena and {Abou Khaled}, Omar and Ingold, Rolf},
booktitle = {International Conference on Multimodal Interfaces (ICMI)},
doi = {10.1145/2070481.2070501},
isbn = {9781450306416},
keywords = {Ambient intelligence,addressee,gesture recognition,interaction,multi-modal,multimodal interaction,pointing,smart environments,wearable and pervasive computing.},
pages = {105},
publisher = {ACM Press},
title = {{Humans and Smart Environments: A Novel Multimodal Interaction Approach}},
url = {http://dl.acm.org/citation.cfm?id=2070501 http://dl.acm.org/citation.cfm?doid=2070481.2070501},
year = {2011}
}
@inproceedings{Richter2018,
abstract = {Intelligent agents need to perceive and correctly interpret the social signals of their interaction partners. In order to support the development of these skills, we establish a process of long-term data acquisition, annotation and continuous model evaluation. We facilitate automatic recording and annotation of unconstrained, multicentric interactions in a smart environment. Finally, we simplify manual ground truth annotation and allow continuous evaluation of our recognition models on a growing set of interactions.},
address = {New York, New York, USA},
author = {Richter, Viktor and Kummert, Franz},
booktitle = {Companion of ACM/IEEE International Conference on Human-Robot Interaction (HRI)},
doi = {10.1145/3173386.3177005},
isbn = {9781450356152},
keywords = {continuous evaluation,conversational cues,multi-modal interaction,smart environments},
pages = {217--218},
publisher = {ACM Press},
title = {{Continuous Interaction Data Acquisition and Evaluation}},
url = {http://dl.acm.org/citation.cfm?doid=3173386.3177005},
volume = {Part F1351},
year = {2018}
}
@inproceedings{Bruno2015,
abstract = {Robots for the elderly are a particular category of home assistive robots, helping people in the execution of daily life tasks to extend their independent life. Such robots should be able to determine the level of independence of the user and track its evolution over time, to adapt the assistance to the person capabilities and needs. Human Activity Recognition systems employ various sensing strategies, relying on environmental or wearable sensors, to recognize the daily life activities which provide insights on the health status of a person. The main contribution of the article is the design of an heterogeneous information management framework, allowing for the description of a wide variety of human activities in terms of multi-modal environmental and wearable sensing data and providing accurate knowledge about the user activity to any assistive robot.},
author = {Bruno, Barbara and Grosinger, Jasmin and Mastrogiovanni, Fulvio and Pecora, Federico and Saffiotti, Alessandro and Sathyakeerthy, Subhash and Sgorbissa, Antonio},
booktitle = {RO-MAN},
isbn = {9781467367035},
keywords = {activity recognition,assisted living,multi-modal,wearable},
number = {iii},
pages = {594--600},
title = {{Multi-modal Sensing for Human Activity Recognition}},
year = {2015}
}
@article{Guo,
abstract = {Wireless smart home system is to facilitate people's lives and it trends to adopt a more intelligent way to provide services. It is beneficial to design an intelligent Smart Home (SH) to precisely recognize users' behaviors and automatically response the corresponding activities to satisfy users' actual demands. However, activity models in the existing approaches are usually constructed separately through statistic probability. These models cannot recognize the user's dynamical intentions accurately. To address the problem, we propose a new SH architecture with smart device enabled sensor networks and develop the prototype system. Moreover, we propose the hybrid semantic model based on the statistic probability model and the semantic association model, and an assistance algorithm is presented. In our prototype system, the smart devices are described by semantic models. When the user needs assistances, smart gateway can provide appropriate services according to the inference results of the algorithm. The algorithm has been implemented and the results show that the accuracy of the algorithm based on the hybrid model is higher than the statistic probability model.},
author = {Guo, Kun and Li, Yonghua and Lu, Yueming and Sun, Xiang and Wang, S. and Cao, R.},
doi = {10.1177/155014772396012},
issn = {1550-1477},
journal = {International Journal of Distributed Sensor Networks},
month = {8},
number = {8},
pages = {2396012--2396012},
title = {{An Activity Recognition-Assistance Algorithm Based on Hybrid Semantic Model in Smart Home}},
url = {http://dsn.sagepub.com/lookup/doi/10.1177/155014772396012},
volume = {12},
year = {2016}
}
@inproceedings{Lutkebohle2010,
abstract = {A robot's head is important both for directional sensors and, in human-directed robotics, as the single most visible interaction interface. However, designing a robot's head faces contradicting requirements when integrating powerful sensing with social expression. Furher, reactions of the general public show that current head designs often cause negative user reactions and distract from the functional capabilities. Therefore, this contribution presents a novel anthropomorphic robot head called "Flobi", which combines state-of-the-art sensing functionality with an exterior that elicits a sympathetic emotional response. It can display primary and secondary emotions in a human-like way, to enable intuitive human-robot-interaction. To facilitate further research on facial appearance, the exterior is fully modular and replaceable. While current state-of-the-art still requires trade-offs when integrating sensing and social expression, Flobi has been designed to enable service robotic applications, with high-resolution, wide-angle stereo vision, gyroscope motion compensation and stereo audio. For ease of integration, the head is self-contained, including 18 actuators, sensors and control boards, all in a human-head sized package.},
author = {Lütkebohle, Ingo and Hegel, Frank and Schulz, Simon and Hackel, Matthias and Wrede, Britta and Wachsmuth, Sven and Sagerer, Gerhard},
booktitle = {International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ROBOT.2010.5509173},
isbn = {978-1-4244-5038-1},
month = {5},
pages = {3384--3391},
publisher = {IEEE},
title = {{The Bielefeld Anthropomorphic Robot Head Flobi}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5509173 http://ieeexplore.ieee.org/document/5509173/},
year = {2010}
}
@inproceedings{Groh2010,
abstract = {A novel method for quantitatively measuring social interactions on small temporal and spatial scales on the basis of interaction geometry (reduced to the parameters interpersonal distance and relative body orientation) with the help of infrared (IR) tracking is introduced. The method is intended to be used to establish a probabilistic classifier to identify existing social situations on the basis of measuring the two parameters for pairs of persons through a series of experiments. The classifier can then be used for characterizing the social context (as an evidence for or against established social situations) of users using sensors in mobile devices in view of useful future Mobile Social Networking services. A first experiment is conducted with the method, a number of standard classifiers including a Gaussian Mixture Model are trained and evaluated and the results are discussed.},
author = {Groh, Georg and Lehmann, Alexander and Reimers, Jonas and Friess, Marc Rene and Schwarz, Loren},
booktitle = {2010 IEEE Second International Conference on Social Computing},
doi = {10.1109/SocialCom.2010.11},
isbn = {978-1-4244-8439-3},
keywords = {social robotics},
month = {8},
pages = {1--8},
publisher = {IEEE},
title = {{Detecting Social Situations from Interaction Geometry}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5590934},
year = {2010}
}
@article{Vacher2010,
author = {Vacher, Michel and Fleury, Anthony and Portet, Fran{\c{c}}ois and Serignat, Jean-fran{\c{c}}ois and Noury, Norbert},
journal = {New Developments in Biomedical Engineering},
pages = {645----673},
title = {{Complete Sound and Speech Recognition System for Health Smart Homes : Application to the Recognition of Activities of Daily Living}},
year = {2010}
}
@article{Fischer2012a,
abstract = {In surveillance systems, the situation awareness of decision makers is often a crucial point in making appropriate decisions. For supporting the situation assessment process, modules performing an automatic interpretation of the observed environment can be used. However, there is still a need for an optimal solution for the definition of such modules. In this article we describe how situations of interest can be modeled in a human-understandable way and how their existence can be inferred from sensor observations by the use of dynamic Bayesian networks. A crucial point of modeling such networks is the definition of the parameters, namely the conditional probabilities. We present a method for an automatic definition of the parameters that can be easily used by a human operator when designing a new network. By using this approach, we define two example networks that are able to recognize situations of interest in the VIRAT dataset. Finally, the two networks are applied to the VIRAT dataset and we present an evaluation of the performance of the automatic situation assessment.},
author = {Fischer, Y and Beyerer, J},
isbn = {VO -},
journal = {Information Fusion (FUSION), 2012 15th International Conference on},
keywords = {Bayesian methods,Hidden Markov models,Loading,Object oriented modeling,Probabilistic logic,Surveillance,VIRAT dataset,Vehicles,automatic observed environmental interpretation,automatic parameter definition,automatic situation assessment performance evaluat,bayes networks,belief networks,conditional probabilistic situation assessment,decision maker situation awareness,decision making,dynamic Bayesian networks,human operator,image sensors,optimal solution,probability,sensor observations,situation,situation assessment,video surveillance,video surveillance systems},
pages = {888--895},
title = {{Defining dynamic Bayesian networks for probabilistic situation assessment}},
year = {2012}
}
@inproceedings{Aehnelt2015,
abstract = {Information assistance helps in many application domains to structure, guide and control hu- man work processes. However, it lacks a formalisation and automated processing of background knowledge which vice versa is required to provide ad-hoc assistance. In this paper, we describe our conceptual and technical work to include contextual background knowledge in raising aware- ness, guiding, and monitoring the assembly worker. We present cognitive architectures as missing link between highly sophisticated manufacturing data systems and implicitly available contextual knowledge on work procedures and concepts of the work domain. Our work is illustrated with examples in SWI-Prolog and the Soar cognitive architecture. 1},
author = {Aehnelt, Mario and Bader, Sebastian},
keywords = {cognitive architectures,information assistance,smart manufacturing},
number = {JANUARY},
title = {{Information Assistance for Smart Assembly Stations}},
year = {2015}
}
@inproceedings{Gatica-Perez2006a,
author = {Gatica-Perez, Daniel},
booktitle = {2006 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems},
doi = {10.1109/MFI.2006.265658},
isbn = {1-4244-0567-X},
keywords = {Acoustic,Probabilistic Methods,Vision},
month = {9},
pages = {41--46},
publisher = {IEEE},
title = {{Analyzing Group Interactions in Conversations: a Review}},
url = {http://ieeexplore.ieee.org/document/4042075/},
year = {2006}
}
@phdthesis{Afshar2016,
author = {Afshar, Hadi Mohasel},
booktitle = {Probabilistic Inference in Piecewise Graphical Models},
number = {August},
school = {The Australian National University},
title = {{Probabilistic inference in graphical models}},
url = {http://mlg.eng.cam.ac.uk/zoubin/course03/hbtnn2e-I.pdf},
year = {2016}
}
@inproceedings{Sato2014,
abstract = {In this study, we suggest a method to coordinate turn-taking and talking in multi-party conversations by the gaze of a robot that participates on the side. Also, we use the experimental paradigm named 'Cooperative Turn-taking Game in Non-verbal Situation', which is a simplified multi-party conversation environment. We investigated whether designing eye-gazes for such robots can coordinate turn-taking and talking in multi-party conversations, and we found the robot's gaze could coordinate turn-taking and talking in multi-party conversations. Our study is expected to effectively encourage desirable talking in such multi-party conversations as collaborative learning scenes.},
author = {Sato, Ryo and Takeuchi, Yugo},
booktitle = {The 23rd IEEE International Symposium on Robot and Human Interactive Communication},
doi = {10.1109/ROMAN.2014.6926266},
isbn = {978-1-4799-6765-0},
issn = {1944-9445},
month = {8},
number = {October},
pages = {280--285},
publisher = {IEEE},
title = {{Coordinating turn-taking and talking in multi-party conversations by controlling robot's eye-gaze}},
url = {http://ieeexplore.ieee.org/document/6926266/},
volume = {2014-Octob},
year = {2014}
}
@article{Bicocchi2014,
abstract = {Massive networks of wearable devices have recently become a key scenario for pattern recognition technologies. Applications range from implicit human-machine interactions, to autonomous monitoring of user habits and activities. This paper presents a framework providing developers with tools to orchestrate the continuous process of collecting and classifying data streams in aware-systems. It supports service oriented, reconfigurable components and provides a solid background to put at joint work specification- and data-driven approaches. It also provides an innovative meta-classification scheme allowing to implement applications by editing a simple state automata. Experimental results suggest that the approach could be integrated in a number of applications for: (i) improving energy efficiency, (ii) improving classification accuracy and (iii) improving software engineering of aware systems.},
author = {Bicocchi, Nicola and Ingegneria, Dipartimento},
isbn = {9781450330473},
pages = {1057--1062},
title = {{Human Aware Superorganisms}},
year = {2014}
}
@inproceedings{Gross2012,
abstract = {This paper presents results of the development of a socially assistive home robot companion for older people suffering from mild cognitive impairment (MCI) and living (alone) at home. This work was part of the European FP7 project "CompanionAble" (2008-2012) 1 which aimed at developing assistive technologies that can support these elderly and help them to remain in their familiar living environment for as long as possible. To overcome current market entry barriers, from the start we consistently adopted a user- and application-centered development process of the companion robot and focused on three main aspects: (i) the realization of a set of mandatory functionalities to support care recipients and caregivers, (ii) a strict design and usability driven realization to increase the acceptance of the robot by the different end-user groups (the elderly, their relatives, and caregivers), and (iii) the development and component selection considering production and operational costs. In continuation of the work presented in 2, this paper describes the final implementation of the companion robot and presents latest results of functional tests and early findings of user studies recently conducted in the smart house of the Dutch project partner Smart Homes in Eindhoven, The Netherlands. 2012 IEEE.},
author = {Gross, H.-M. and Schroeter, Ch and Mueller, S. and Volkhardt, M. and Einhorn, E. and Bley, A. and Langner, T. and Merten, M. and Huijnen, C. and van den Heuvel, H. and van Berlo, A.},
booktitle = {International Conference on Systems, Man, and Cybernetics (SMC)},
doi = {10.1109/ICSMC.2012.6377798},
isbn = {978-1-4673-1714-6},
keywords = {Companion Robots,Human-Robot-Interaction,Smart Homes,Socially assistive Robotics,User studies,hri},
month = {10},
pages = {637--644},
publisher = {IEEE},
title = {{Further Progress Towards a Home Robot Companion for People with Mild Cognitive Impairment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6377798},
year = {2012}
}
@inproceedings{Ishiguro1999,
abstract = {This paper proposes a robot architecture that enables us to progressively develop a robot. The architecture consisting of situated modules has merits of both the traditional function-based and behavior-based architectures in addition to the merit in the development. We have developed a robot based on the architecture. By reporting the development process, this paper discusses advantages of the proposed architecture.},
author = {Ishiguro, Hiroshi and Kanda, Toshiyuki and Kimoto, Katumi and Ishida, Toru},
booktitle = {Intelligent Robots and Systems, 1999. IROS '99},
pages = {1617 -- 1624 vol.3},
title = {{A Robot Architecture Based on Situated Modules}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=811710{\&}tag=1},
year = {1999}
}
@inproceedings{Takemae2006,
abstract = {We propose a method that uses the participants' head ori- entation and utterances for automatically identifying the ad- dressee of each utterance in face-to-face multiparty conversa- tions, such as meetings. First, each participant's head orien- tation is determined through vision-based detection and the presence/absence of utterances is extracted using the power of voices captured by microphones. Second, gaze direction (whom each participant is looking at) is estimated from just detected head orientation using the Support Vector Machine. Third, several related features such as amount and frequency of gaze and eye contact are calculated in each utterance in- terval. Finally, a Bayesian Network is used to classify each utterance into one of two types of utterances: (a) the speaker is addressing a single participant and (b) the speaker is ad- dressing all participants. Experiments on addressee estima- tionwith 3-person conversations confirm the usefulness of our method.},
author = {Takemae, Yoshinao and Ozawa, Shinji},
booktitle = {International Conference on Multimedia and Expo},
doi = {10.1109/ICME.2006.262773},
isbn = {1-4244-0366-7},
month = {7},
pages = {1285--1288},
publisher = {IEEE},
title = {{Automatic Addressee Identification Based on Participants' Head Orientation and Utterances for Multiparty Conversations}},
url = {http://ieeexplore.ieee.org/document/4036842/},
volume = {2006},
year = {2006}
}
@inproceedings{Takayama2009,
abstract = {Agentic objects are those entities that are perceived and responded to in-the-moment as if they were agentic despite the likely reflective perception that they are not agentic at all. They include autonomous robots, but also simpler systems like automatic doors, trashcans, and staplers - anything that seems to possess agency. It is well known that low-level spatiotemporal information elicits in-the-moment responses that are interpreted as perceiving mentalism [8, 17], but people reflectively believe that there is a distinction between human and non-human agents. How are we to make sense of these agentic objects?},
address = {New York, New York, USA},
author = {Takayama, Leila},
booktitle = {Proceedings of the 4th ACM/IEEE international conference on Human robot interaction - HRI '09},
doi = {10.1145/1514095.1514155},
isbn = {9781605584041},
issn = {2167-2121},
pages = {239},
publisher = {ACM Press},
title = {{Making sense of agentic objects and teleoperation}},
url = {http://portal.acm.org/citation.cfm?doid=1514095.1514155},
year = {2009}
}
@article{Strachan2012,
abstract = {SIFT features have known limitations for matching features under changes in 3D pose of the object. We investigate the use of multimodal information from the range and intensity domains as an approach to mitigating the effects of changes in view point. In this investigation we outline an experimental proce- dure applied to range and intensity images of real-world objects captured using a stereo camera pair and calibrated turntable setup offering reliable ground truth for the 3D motion of surface locations. From our investigation we propose a novel surface descriptor, EC-SIFT, which shows a significant improvement on state of the art performance in the experiments we present.},
author = {Strachan, E. and Siebert, J. P.},
doi = {10.1109/SITIS.2012.69},
isbn = {978-1-4673-5152-2},
journal = {2012 Eighth International Conference on Signal Image Technology and Internet Based Systems},
month = {11},
pages = {426--432},
publisher = {Ieee},
title = {{Local Multi-Modal SIFT Features in Co-registered Range and Intensity Images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6395126},
year = {2012}
}
@inproceedings{Cristani2011,
abstract = {We present a novel approach for detecting social interactions in a crowded scene by employing solely visual cues. The detection of social interactions in unconstrained scenarios is a valuable and important task, especially for surveillance purposes. Our proposal is inspired by the social signaling literature, and in particular it considers the sociological notion of F-formation. An F-formation is a set of possible configurations in space that people may assume while participating in a social interaction. Our system takes as input the positions of the people in a scene and their (head) orientations; then, employing a voting strategy based on the Hough transform, it recognizes F-formations and the individuals associated with them. Experiments on simulations and real data promote our idea.},
author = {Cristani, Marco and Bazzani, Loris and Paggetti, Giulia and Fossati, Andrea and Tosato, Diego and Bue, Alessio Del and Menegaz, Gloria and Murino, Vittorio},
booktitle = {Procedings of the British Machine Vision Conference},
doi = {10.5244/C.25.23},
isbn = {1-901725-43-X},
keywords = {f-formations,social robotics},
pages = {23.1--23.12},
publisher = {British Machine Vision Association},
title = {{Social Interaction Discovery by Statistical Analysis of F-Formations}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper23/index.html},
year = {2011}
}
@misc{Abadi2015,
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://download.tensorflow.org/paper/whitepaper2015.pdf},
year = {2015}
}
@article{Besl1992,
abstract = {The authors describe a general-purpose, representation-independent method for the accurate and computationally efficient registration of 3-D shapes including free-form curves and surfaces. The method handles the full six degrees of freedom and is based on the iterative closest point (ICP) algorithm, which requires only a procedure to find the closest point on a geometric entity to a given point. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric, and the rate of convergence is rapid during the first few iterations. Therefore, given an adequate set of initial rotations and translations for a particular class of objects with a certain level of `shape complexity', one can globally minimize the mean-square distance metric over all six degrees of freedom by testing each initial registration. One important application of this method is to register sensed data from unfixtured rigid objects with an ideal geometric model, prior to shape inspection. Experimental results show the capabilities of the registration algorithm on point sets, curves, and surfaces},
author = {Besl, P.J. and McKay, Neil D.},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {2},
pages = {239 -- 256},
title = {{A method for registration of 3-D shapes}},
volume = {14},
year = {1992}
}
@inproceedings{Blum2012,
abstract = {In this work we address the problem of feature extraction for object recognition in the context of cameras providing RGB and depth information (RGB-D data). We consider this problem in a bag of features like setting and propose a new, learned, local feature descriptor for RGB-D images, the convolutional k-means descriptor. The descriptor is based on recent results from the machine learning community. It automatically learns feature responses in the neighborhood of detected interest points and is able to combine all avail- able information, such as color and depth into one, concise representation. To demonstrate the strength of this approach we show its applicability to different recognition problems. We evaluate the quality of the descriptor on the RGB-D Object Dataset where it is competitive with previously published results and propose an embedding into an image processing pipeline for object recognition and pose estimation.},
author = {Blum, Manuel and Wulfing, Jan and Riedmiller, Martin},
booktitle = {2012 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2012.6225188},
isbn = {978-1-4673-1405-3},
month = {5},
pages = {1298--1303},
publisher = {Ieee},
title = {{A learned feature descriptor for object recognition in RGB-D data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6225188 http://ml.informatik.uni-freiburg.de/{\_}media/publications/blumicra2011.pdf},
year = {2012}
}
@inproceedings{Storf2009a,
abstract = {Among the central challenges of Ambient Assisted Living systems are the autonomous and reliable recognition of the assisted person's current situation and the proactive offering and rendering of adequate assistance services. In the context of emergency support, such situations may be acute emergency situations or long-term deviations from typical behavior that will result in emergency situations in the future. To optimize the treatment of the former and the prevention of the latter, reliable recognition of characteristic activities of daily living is necessary. In this paper, we present our multi-agent-based activity recognition framework as well as experiences made with it. Besides a detailed discussion of our hybrid recognition approach, we also elaborate on the tailoring of the underlying reasoning models to the individual environments and users in an initial learning phase. Finally, we present experiences made with the recognition framework in our Ambient Assisted Living Laboratory.},
author = {Storf, Holger and Becker, Martin and Riedl, Martin},
booktitle = {Proceedings of the 3d International ICST Conference on Pervasive Computing Technologies for Healthcare},
doi = {10.4108/ICST.PERVASIVEHEALTH2009.6108},
publisher = {ICST},
title = {{Rule-based activity recognition framework: Challenges, technique and learning}},
url = {http://eudl.eu/doi/10.4108/ICST.PERVASIVEHEALTH2009.6108},
year = {2009}
}
@article{Sabanovic2006,
abstract = {This paper discusses the use of observational studies of human-robot social interaction in open human-inhabited environments as a method for improving on the design and evaluating the interactive capabilities of social robots. First, we discuss issues that have surfaced in attempts to evaluate social interactions between humans and robots. Next, we review two observational studies involving robots interacting socially with humans and discuss how the results can be applied to improving robot design. The first is an analysis of a mobile conference-attending robot that performed a search task by augmenting its perception through social interaction with human attendees. The second is an analysis of a stationary robotic receptionist that provides information to visitors and enhances interaction through story-telling. Through these examples, we show how observational studies can be applied to human-robot social interactions in varying contexts and with differing tasks to quantitatively and qualitatively evaluate (and discover unanticipated aspects of) the social interaction. Finally, we discuss design recommendations suggested by insights gained through these analyses},
author = {Sabanovic, Selma and Michalowski, Marek P. and Simmons, Reid},
doi = {10.1109/AMC.2006.1631758},
isbn = {0-7803-9511-1},
journal = {International Workshop on Advanced Motion Control, AMC},
pages = {576--581},
title = {{Robots in the wild: Observing human-robot social interaction outside the lab}},
volume = {2006},
year = {2006}
}
@inproceedings{Stocker2014,
abstract = {Situation assessment, i.e. the process of achieving situation awareness, is common in environ- mental monitoring, where assessment occurs predominantly on sensor data and awareness is for the state of environmental phenomena. For a particular location, an environmental monitoring system may measure and compute mean hourly PM2.5 concentration to acquire knowledge for situations of unhealthy exposure by humans to ambient air; it may measure aerosol particle size distribution to acquire knowledge for situations of atmospheric new particle formation; it may measure road-pavement vibration to acquire knowledge for traffic. The process can be divided in four generic sub processes, namely data acquisition, data processing, knowl- edge acquisition and extraction, and knowledge representation and reasoning. We outline an ontology for the process. It aligns and specializes the generic concepts of several upper ontologies. The ontology could form a building block in the discovery and query of situational knowledge acquired and represented by dis- tributed environmental monitoring systems, from heterogeneous sensor data and for diverse environmental phenomena, in time and space.},
address = {San Diego},
author = {Stocker, Markus and R{\"{o}}nkk{\"{o}}, Mauno and Kolehmainen, Mikko},
booktitle = {International Congress on Environment Modelling and Software},
keywords = {ation awareness,environmental monitoring,knowledge,knowledge acquisition,knowledge representation,ontology,reasoning,sensor data,situ-,situation,wavellite},
publisher = {International Environmental Modelling and Software Society (iEMSs)},
title = {{Towards an Ontology for Situation Assessment in Environmental Monitoring}},
year = {2014}
}
@inproceedings{Baum2011,
abstract = {This paper is about tracking an extended object or a group target, which gives rise to a varying number of measurements from different measurement sources. For this purpose, the shape of the target is tracked in addition to its kinematics. The target extent is modeled with a new approach called Random Hypersurface Model (RHM) that assumes varying measurement sources to lie on scaled versions of the shape boundaries. In this paper, a star-convex RHM is introduced for tracking star-convex shape approximations of targets. Bayesian inference for star-convex RHM is performed by means of a Gaussian-assumed state estimator allowing for an efficient recursive closed-form measurement update. Simulations demonstrate the performance of this approach for typical extended object and group tracking scenarios.},
author = {Baum, Marcus and Hanebeck, Uwe D},
booktitle = {Proceedings of the 14th International Conference on Information Fusion (Fusion 2011)},
isbn = {9780982443828},
keywords = {extended ob-,group targets,jects,shape tracking,target tracking},
title = {{Shape Tracking of Extended Objects and Group Targets with Star-Convex RHMs}},
url = {http://digbib.ubka.uni-karlsruhe.de/volltexte/1000035108},
year = {2011}
}
@article{Torta2015,
abstract = {One of the most common tasks of a robot com- panion in the home is communication. In order to initiate an information exchange with its human partner, the robot needs to attract the attention of the human. This paper presents results of two user studies (N = 12) to evaluate the effec- tiveness of unimodal and multimodal communication cues for attracting attention. Results showed that unimodal com- munication cues which involve sound generate the fastest reaction times. Contrary to expectations, multimodal com- munication cues resulted inlonger reaction times with respect to the unimodal communication cue that produced the short- est reaction time.},
annote = {Roboter versucht Aufmerksamkeit von Personen zu bekommen

Variation:
* anschauen
* blinken (led)
* winken
* 'hallo' sagen

sp{\"{a}}ter Kombinationen davon

={\textgreater} Reaktionszeiten

* Sprache am schnellsten
* dann Winken, dann Blinken
* Sprache + Winken -{\textgreater} Reaktionszeit dazwischen

Annahme: Roboter nicht im visuellen Fokus -{\textgreater} Audio salienter},
author = {Torta, Elena and van Heumen, Jim and Piunti, Francesco and Romeo, Luca and Cuijpers, Raymond},
doi = {10.1007/s12369-014-0271-x},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Attention,Human???robot interaction,Psychometric study,Socially-assistive robots,attention},
month = {2},
number = {1},
pages = {89--96},
title = {{Evaluation of Unimodal and Multimodal Communication Cues for Attracting Attention in Human–Robot Interaction}},
url = {http://link.springer.com/10.1007/s12369-014-0271-x},
volume = {7},
year = {2015}
}
@article{Cabrera-Quiros2018,
author = {Cabrera-Quiros, Laura and Demetriou, Andrew and Gedik, Ekin and van der Meij, Leander and Hung, Hayley},
doi = {10.1109/TAFFC.2018.2848914},
issn = {1949-3045},
journal = {IEEE Transactions on Affective Computing},
keywords = {dataset,f-formations,group},
number = {c},
pages = {1--1},
title = {{The MatchNMingle Dataset: a Novel Multi-Sensor Resource for the Analysis of Social Interactions and Group Dynamics In-The-Wild during Free-Standing Conversations and Speed Dates}},
url = {https://ieeexplore.ieee.org/document/8395003/},
volume = {PP},
year = {2018}
}
@inproceedings{Sagonas2013,
abstract = {Automatic facial point detection plays arguably the most important role in face analysis. Several methods have been proposed which reported their results on databases of both constrained and unconstrained conditions. Most of these databases provide annotations with different mark-ups and in some cases the are problems related to the accuracy of the fiducial points. The aforementioned issues as well as the lack of a evaluation protocol makes it difficult to compare performance between different systems. In this paper, we present the 300 Faces in-the-Wild Challenge: The first facial landmark localization Challenge which is held in conjunction with the International Conference on Computer Vision 2013, Sydney, Australia. The main goal of this challenge is to compare the performance of different methods on a new-collected dataset using the same evaluation protocol and the same mark-up and hence to develop the first standardized benchmark for facial landmark localization.},
author = {Sagonas, Christos and Tzimiropoulos, Georgios and Zafeiriou, Stefanos and Pantic, Maja},
booktitle = {International Conference on Computer Vision Workshops (ICCV)},
doi = {10.1109/ICCVW.2013.59},
isbn = {978-1-4799-3022-7},
month = {12},
pages = {397--403},
publisher = {IEEE},
title = {{300 Faces in-the-Wild Challenge: The First Facial Landmark Localization Challenge}},
url = {http://ieeexplore.ieee.org/document/6755925/},
year = {2013}
}
@inproceedings{Faria2015,
abstract = {In this work, we present a human-centered robot application in the scope of daily activity recognition towards robot-assisted living. Our approach consists of a probabilistic ensemble of classifiers as a dynamic mixture model considering the Bayesian probability, where each base classifier contributes to the inference in proportion to its posterior belief. The classification model relies on the confidence obtained from an uncertainty measure that assigns a weight for each base clas- sifier to counterbalance the joint posterior probability. Spatio- temporal 3D skeleton-based features extracted from RGB-D sensor data are modeled in order to characterize daily activities, including risk situations (e.g.: falling down, running or jumping in a room). To assess our proposed approach, challenging public datasets such as MSR-Action3D and MSR-Activity3D [1] [2] were used to compare the results with other recent methods. Reported results show that our proposed approach outperforms state-of-the-art methods in terms of overall accuracy. Moreover, we implemented our approach using Robot Operating System (ROS) environment to validate the DBMM running on-the-fly in a mobile robot with an RGB-D sensor onboard to identify daily activities for a robot-assisted living application.},
author = {Faria, Diego R and Vieira, Mario and Premebida, Cristiano and Nunes, Urbano},
booktitle = {RO-MAN},
isbn = {9781467367035},
keywords = {activity recognition,bayes,kinect,rgbd,skeleton},
pages = {582--587},
title = {{Probabilistic Human Daily Activity Recognition towards Robot-assisted Living}},
year = {2015}
}
@inproceedings{Rios-Martinez2012,
abstract = {The objective of this paper is to present a strategy to safely move a robot in an unknown and complex environment where people are moving and interacting. The robot, by using only its sensor data, must navigate respecting humans' comfort. To obtain good results in such a dynamic environment, a prediction on humans' movement is also crucial. To solve all the aforementioned problems we introduce a suitable cost function. Its optimization is obtained by using a new stochastic and adaptive optimization algorithm (CAO). This method is very useful in particular when the analytical expression of the optimization function is unknown but numerical values are available for any state configuration. Additionally, the proposed method can easily incorporate any dynamical and environmental constraints. To validate the performance of the proposed solution, several simulation results are provided.},
author = {Rios-Martinez, Jorge and Renzaglia, Alessandro and Spalanzani, Anne and Martinelli, Agostino and Laugier, Christian},
booktitle = {International Conference on Robotics and Automation (ICRA)},
doi = {10.1109/ICRA.2012.6224934},
isbn = {978-1-4673-1405-3},
month = {5},
pages = {2880--2885},
publisher = {IEEE},
title = {{Navigating Between People: A Atochastic Optimization Approach}},
url = {http://ieeexplore.ieee.org/document/6224934/},
volume = {231855},
year = {2012}
}
@article{Culmone2015,
abstract = {In the last years, the extensive use of smart objects embedded in the physical world, in order to monitor and record physical or environmental conditions, has increased rapidly. In this scenario, heterogeneous devices are connected together into a network. Data generated from such system are usually stored in a database, which often shows a lack of semantic information and relationship among devices. Moreover, this set can be incomplete, unreliable, incorrect and noisy. So, it turns out to be important both the integration of information and the interoperability of applications. For this reason, ontologies are becoming widely used to describe the domain and achieve efficient interoperability of information system. An example of the described situation could be represented by Ambient Assisted Living context, which intends to enable older or disabled people to remain living independently longer in their own house. In this contest, human activity recognition plays a main role because it could be considered as starting point to facilitate assistance and care for elderly. Due to the nature of human behavior, it is necessary to manage the time and spatial restrictions. So, we propose a framework that implements a novel methodology based on the integration of an ontology for representing contextual knowledge and a Complex Event Processing engine for supporting timed reasoning. Moreover, it is an infrastructure where knowledge, organized in conceptual spaces (based on its meaning) can be semantically queried, discovered, and shared across applications. In our framework, benefits deriving from the implementation of a domain ontology are exploited into different levels of abstrac-tion. Thereafter, reasoning techniques represent a preprocessing method to prepare data for the final temporal analysis. The results, presented in this paper, have been obtained applying the methodology into AALISABETH, an Ambient Assisted Living project aimed to monitor the lifestyle of old people, not suffering from major chronic diseases or severe disabilities.},
author = {Culmone, Rosario and Giuliodori, Paolo and Quadrini, Michela},
journal = {International Journal on Advances in Intelligent Systems},
keywords = {Complex Event Processing (CEP),OntoAALISABETH Domain Ontology,Semantic Reasoning,–Pattern Recognition},
number = {2},
title = {{Human Activity Recognition using a Semantic Ontology-Based Framework}},
url = {http://www.iariajournals.org/intelligent{\_}systems/www.iaria.org},
year = {2015}
}
@inproceedings{Borodulkin2002,
abstract = {In contrast to the rapid development of home automation equipment and 'smart home' capabilities, comparatively less attention has been paid to the development of comprehensive, comfortable and self-explaining user interfaces. This is acknowledged to be an important obstacle for the broad success of smart home ideas and products. In order to make home automation desirable and economically feasible, it is vital to improve the attractivity, intuitiveness and adaptivity of the user-home interaction taking advantage from modern information technologies such as Virtual Reality technology and downloadable hypertext documents. The user interface is augmented by the use of a virtual environment in conjunction with the physical environment adding information from different systems and sensors in the home, based on virtual environment activities. In this paper we describe a powerful graphical interface created with standard 3D programming tools to control and supervise the household. A number of features implemented in a prototype and implementation aspects will be described.},
annote = {Interaction via a rich 3D UI},
author = {Borodulkin, Leonid and Ruser, Heinrich and Trankler, Hans-Rolf},
booktitle = {International Symposium on Virtual and Intelligent Measurement Systems (2002)},
doi = {10.1109/VIMS.2002.1009367},
isbn = {0-7803-7344-8},
keywords = {interaction,interface,smart home},
number = {May},
pages = {111--115},
publisher = {IEEE},
title = {{3D virtual "smart home" user interface}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1009367 http://ieeexplore.ieee.org/document/1009367/},
year = {2002}
}
@article{Liang2009,
abstract = {The (batch) EM algorithm plays an important role in unsupervised induction, but it sometimes suffers from slow convergence. In this paper, we show that online variants (1) provide significant speedups and (2) can even find better solutions than those found by batch EM. We support these findings on four unsupervised tasks: part-of-speech tagging, document classification, word segmentation, and word alignment.},
author = {Liang, Percy and Klein, Dan},
doi = {10.3115/1620754.1620843},
isbn = {9781932432411},
issn = {978-1-932432-41-1},
journal = {Proceedings of Human Language Technologies The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on NAACL 09},
keywords = {Bayesian Network,em,learning,online em,parameter learning},
number = {June},
pages = {611},
title = {{Online EM for unsupervised models}},
url = {http://portal.acm.org/citation.cfm?doid=1620754.1620843},
year = {2009}
}
@article{Staudte2011,
abstract = {Referential gaze during situated language production and comprehension is tightly coupled with the unfolding speech stream (Griffin, 2001; Meyer, Sleiderink, {\&} Levelt, 1998; Tanenhaus, Spivey-Knowlton, Eberhard, {\&} Sedivy, 1995). In a shared environment, utterance comprehension may further be facilitated when the listener can exploit the speaker's focus of (visual) attention to anticipate, ground, and disambiguate spoken references. To investigate the dynamics of such gaze-following and its influence on utterance comprehension in a controlled manner, we use a human-robot interaction setting. Specifically, we hypothesize that referential gaze is interpreted as a cue to the speaker's referential intentions which facilitates or disrupts reference resolution. Moreover, the use of a dynamic and yet extremely controlled gaze cue enables us to shed light on the simultaneous and incremental integration of the unfolding speech and gaze movement.We report evidence from two eye-tracking experiments in which participants saw videos of a robot looking at and describing objects in a scene. The results reveal a quantified benefit-disruption spectrum of gaze on utterance comprehension and, further, show that gaze is used, even during the initial movement phase, to restrict the spatial domain of potential referents. These findings more broadly suggest that people treat artificial agents similar to human agents and, thus, validate such a setting for further explorations of joint attention mechanisms. {\textcopyright} 2011 Elsevier B.V.},
author = {Staudte, Maria and Crocker, Matthew W.},
doi = {10.1016/j.cognition.2011.05.005},
isbn = {0010-0277},
issn = {00100277},
journal = {Cognition},
keywords = {Gaze,Human-robot interaction,Joint attention,Reference resolution,Referential gaze,Referential intention,Situated language processing,Utterance comprehension,attention,gaze,interaction},
month = {8},
number = {2},
pages = {268--291},
publisher = {Elsevier B.V.},
title = {{Investigating joint attention mechanisms through spoken human–robot interaction}},
url = {http://dx.doi.org/10.1016/j.cognition.2011.05.005 http://linkinghub.elsevier.com/retrieve/pii/S0010027711001302},
volume = {120},
year = {2011}
}
@incollection{Paper2013,
abstract = {We are building a digital desktop system designed to support the tasks that are usually performed around the traditional desktop. Tabletop platforms are not new environments, especially as a research topic, but most of the existent systems try to adapt the computer work style or only serve as platform for experimenting with new features. In contrast our targets are to support the traditional work flow around desktops, not forcing the users to modify theirs methods and to build the system as a complete tool for everyday tasks We want to provide a usable environment with computer-support features for raising productivity and enhancing the user experience. For doing this we realized a field study about the traditional desktop activities and with this knowledge we designed new tools and features that fit the user real needs and environment. {\textcopyright} 2013 Springer-Verlag.},
address = {Berlin, Heidelberg},
author = {Novick, David and Gris, Iv{\'{a}}n},
doi = {10.1007/978-3-642-39330-3_11},
editor = {Kurosu, Masaaki},
isbn = {978-3-642-39329-7},
issn = {03029743},
keywords = {multi person,multi-modal,turn-taking},
number = {July},
pages = {97--106},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Grounding and Turn-Taking in Multimodal Multiparty Conversation}},
url = {http://link.springer.com/10.1007/978-3-642-39330-3 http://link.springer.com/10.1007/978-3-642-39330-3{\_}11},
volume = {8007},
year = {2013}
}
@techreport{Tomasi1991,
author = {Tomasi, Carlo and Kanade, Takeo},
institution = {International Journal of Computer Vision},
keywords = {computer vision,imagery,motion,shape,time-varying},
number = {7597},
title = {{Detection and Tracking of Point Features}},
year = {1991}
}
@inproceedings{Lier2012,
address = {New York, New York, USA},
author = {Lier, Florian and Schulz, Simon and Wachsmuth, Sven},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1145/2559636.2559787},
isbn = {9781450326582},
pages = {331--331},
publisher = {ACM Press},
title = {{Reality check! - A Physical Robot Versus its Simulation}},
url = {http://dl.acm.org/citation.cfm?doid=2559636.2559787},
volume = {7628},
year = {2014}
}
@phdthesis{Turhan2013,
author = {Turhan, Anni-Yasmin},
keywords = {description logic,ontology,reasoning,situation},
number = {November},
school = {Technische Universit{\"{a}}t Dresden},
title = {{Reasoning Services for the Maintenance and Flexible Access to Description Logic Ontologies}},
year = {2013}
}
@article{Luo2016,
abstract = {In social interactions, it is common for individ- uals to possess different amounts of knowledge about a specific transaction, and those who are more knowledge- able might perform opportunistic behavior to others in their interest, which promotes their value but demotes others' value. Such a typical social behavior is called opportunistic behavior (opportunism). In this paper, we propose a formal account of opportunism based on the situation calculus. We first propose a model of opportunism that only considers a single action between two agents, and then extend it to multiple actions and incorporate social context in the model. A simple example of selling a broken cup is used to illustrate our models. Through our models, we can have a thorough understanding of opportunism.},
author = {Luo, Jieting and Meyer, John-Jules},
doi = {10.1007/s00146-016-0665-4},
issn = {0951-5666},
journal = {AI {\&} SOCIETY},
keywords = {Formalization,Opportunism,Situation calculus,Value,opportunism,opportunism {\'{a}} value {\'{a}},situation calculus,situation calculus {\'{a}}},
month = {6},
publisher = {Springer London},
title = {{A formal account of opportunism based on the situation calculus}},
url = {http://link.springer.com/10.1007/s00146-016-0665-4},
year = {2016}
}
@article{Brdiczka2009,
abstract = {This paper addresses the problem of learning situation models for providing context-aware services. Context for modeling human behavior in a smart environment is represented by a situation model describing environment, users, and their activities. A framework for acquiring and evolving different layers of a situation model in a smart environment is proposed. Different learning methods are presented as part of this framework: role detection per entity, unsupervised extraction of situations from multimodal data, supervised learning of situation representations, and evolution of a predefined situation model with feedback. The situation model serves as frame and support for the different methods, permitting to stay in an intuitive declarative framework. The proposed methods have been integrated into a whole system for smart home environment. The implementation is detailed, and two evaluations are conducted in the smart home environment. The obtained results validate the proposed approach.},
author = {Brdiczka, Oliver and Crowley, James L and Reignier, Patrick},
doi = {10.1109/TSMCB.2008.923526},
issn = {1941-0492},
journal = {IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society},
keywords = {Algorithms,Artificial Intelligence,Behavior,Behavior: physiology,Computer Simulation,Environment,Humans,Markov Chains,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Role,Sociometric Techniques,Video Recording},
month = {2},
number = {1},
pages = {56--63},
title = {{Learning situation models in a smart home.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19068433},
volume = {39},
year = {2009}
}
@article{Stiefelhagen2002,
abstract = {A user's focus of attention plays an important role in human-computer interaction applications, such as a ubiquitous computing environment and intelligent space, where the user's goal and intent have to be continuously monitored. We are interested in modeling people's focus of attention in a meeting situation. We propose to model participants' focus of attention from multiple cues. We have developed a system to estimate participants' focus of attention from gaze directions and sound sources. We employ an omnidirectional camera to simultaneously track participants' faces around a meeting table and use neural networks to estimate their head poses. In addition, we use microphones to detect who is speaking. The system predicts participants' focus of attention from acoustic and visual information separately. The system then combines the output of the audio- and video-based focus of attention predictors. We have evaluated the system using the data from three recorded meetings. The acoustic information has provided 8{\%} relative error reduction on average compared to only using one modality. The focus of attention model can be used as an index for a multimedia meeting record. It can also be used for analyzing a meeting.},
author = {Stiefelhagen, Rainer and {Jie Yang} and Waibel, Alex},
doi = {10.1109/TNN.2002.1021893},
isbn = {1581131518},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Focus of attention,Head pose estimation,Human-computer interaction,Meeting indexing,Multimedia meeting record,Multimodality,focus of attention,visual focus of attention},
month = {7},
number = {4},
pages = {928--938},
title = {{Modeling focus of attention for meeting indexing based on multiple cues}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1021893},
volume = {13},
year = {2002}
}
@inproceedings{Bilac2017,
abstract = {{\textcopyright} 2017 IEEE. Let the human speak! Interactive robots and voice interfaces such as Pepper, Amazon Alexa, and OK Google are becoming more and more popular, allowing for more natural interaction compared to screens or keyboards. One issue with voice interfaces is that they tend to require a 'robotic' flow of human speech. Humans must be careful to not produce disfluencies, such as hesitations or extended pauses between words. If they do, the agent may assume that the human has finished their speech turn, and interrupts them mid-Thought. Interactive robots often rely on the same limited dialogue technology built for speech interfaces. Yet humanoid robots have the potential to also use their vision systems to determine when the human has finished their speaking turn. In this paper, we introduce HOMAGE (Human-rObot Multimodal Audio and Gaze End-of-Turn), a multimodal turntaking system for conversational humanoid robots. We created a dataset of humans spontaneously hesitating when responding to a robot's open-ended questions such as, 'What was your favorite moment this year?'. Our analyses found that users produced both auditory filled pauses such as 'uhhh', as well as gaze away from the robot to keep their speaking turn. We then trained a machine learning system to detect the auditory filled pauses and integrated it along with gaze into the Pepper humanoid robot's real-Time dialog system. Experiments with 28 naive users revealed that adding auditory filled pause detection and gaze tracking significantly reduced robot interruptions. Furthermore, user turns were 2.1 times longer (without repetitions), suggesting that this strategy allows humans to express themselves more, toward less time pressure and better robot listeners.},
author = {Bilac, Miriam and Chamoux, Marine and Lim, Angelica},
booktitle = {International Conference on Humanoid Robots (Humanoids)},
doi = {10.1109/HUMANOIDS.2017.8246889},
isbn = {978-1-5386-4678-6},
keywords = {Multimodal perception,Social interaction and acceptability,System integration},
month = {11},
pages = {297--304},
publisher = {IEEE-RAS},
title = {{Gaze and Filled Pause Detection for Smooth Human-Robot Conversations}},
url = {http://ieeexplore.ieee.org/document/8246889/},
year = {2017}
}
@book{Petrou2010,
author = {Petrou, M and Petrou, C},
isbn = {9780470745861},
publisher = {John Wiley {\&} Sons},
title = {{Image Processing: The Fundamentals}},
url = {http://books.google.de/books?id=w3BpSIxN9ZYC},
year = {2010}
}
@inproceedings{Meyer-Delius2009a,
abstract = {In this paper, we present an approach for learning generalized models for traffic situations. We formulate the problem using a dynamic Bayesian network (DBN) from which we learn the characteristic dynamics of a situation from labeled trajectories using kernel regression. For a new and unlabeled trajectory, we can then infer the corresponding situation by evaluating the data likelihood for the individual situation models. In experiments carried out on laser range data gathered on a car in real traffic and in simulation, we show that we can robustly recognize different traffic situations even from trajectories corresponding to partial situation instances.},
author = {Meyer-Delius, Daniel and Sturm, Jurgen and Burgard, Wolfram},
booktitle = {2009 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2009.5354209},
isbn = {978-1-4244-3803-7},
month = {10},
pages = {1711--1716},
publisher = {IEEE},
title = {{Regression-based online situation recognition for vehicular traffic scenarios}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5354209},
year = {2009}
}
@article{Levinson2015,
abstract = {The core niche for language use is in verbal interaction, involving the rapid exchange of turns at talking. This paper reviews the extensive literature about this system, adding new statistical analyses of behavioral data where they have been missing, demonstrating that turn-taking has the systematic properties originally noted by Sacks et al. (1974; hereafter SSJ). This system poses some significant puzzles for current theories of language processing: the gaps between turns are short (of the order of 200 ms), but the latencies involved in language production are much longer (over 600 ms). This seems to imply that participants in conversation must predict (or 'project' as SSJ have it) the end of the current speaker's turn in order to prepare their response in advance. This in turn implies some overlap between production and comprehension despite their use of common processing resources. Collecting together what is known behaviorally and experimentally about the system, the space for systematic explanations of language processing for conversation can be significantly narrowed, and we sketch some first model of the mental processes involved for the participant preparing to speak next.},
author = {Levinson, Stephen C. and Torreira, Francisco},
doi = {10.3389/fpsyg.2015.00731},
issn = {1664-1078},
journal = {Frontiers in Psychology},
keywords = {Conversation,Language comprehension,Language processing,Language production,Turn-taking,conversational,speech,speech processing,timing,turn-taking},
month = {6},
number = {JUN},
pages = {1--17},
title = {{Timing in turn-taking and its implications for processing models of language}},
url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.00731/abstract},
volume = {6},
year = {2015}
}
@article{Art2009,
abstract = {Special purpose service robots have already entered the market and their users' homes. Also the idea of the general purpose service robot or personal robot companion is increasingly discussed and investigated. To probe human–robot interaction with a mobile robot in arbitrary domestic settings, we conducted a study in eight different homes. Based on previous results from laboratory studies we identified particular interaction situations which should be studied thoroughly in real home settings. Based upon the collected sensory data from the robot we found that the different environments influenced the spatial management observable during our subjects' interaction with the robot. We also validated empirically that the concept of spatial prompting can aid spatial management and communication, and assume this concept to be helpful for Human–Robot Interaction (HRI) design. In this article we report on our exploratory field study and our findings regarding, in particular, the spatial management observed during show episodes and movement through narrow passages . Keywords: COGNIRON, Domestic Service Robotics, Robot Field Trial, Human Augmented Mapping (HAM), Human–Robot Interaction (HRI), Spatial Management, Spatial Prompting},
author = {Art, T H E and Yates, Frances A},
doi = {10.1075/is.10.3.02hut},
issn = {1572-0373},
journal = {Interaction Studies},
keywords = {COGNIRON,Domestic Service Robotics,Human Augmented Mapping (HAM),Human–Robot Interaction (HRI),Robot Field Trial,Spatial Management,Spatial Prompting,cogniron,domestic service robotics,ham,hri,human,human augmented mapping,robot field trial,robot interaction,spatial management,spatial prompting},
month = {12},
number = {3},
pages = {274--297},
title = {{The Art of Gate-Crashing: Bringing HRI into users' homes}},
url = {https://benjamins.com/catalog/is.10.3.02hut},
volume = {10},
year = {2009}
}
@techreport{Vodyaho,
abstract = {The paper describes a system of ontologies developed for the appli- cations oriented on solving problems of situations recognition and assessment based on results of data processing and analyses. Main attention is focused on the problems of processing measurements of various objects parameters repre- sented in a form of time series. The considered applications process data using knowledge extracted from historical data with the help of Data Mining tech- niques. Such applications are highly knowledge centric and their core element is knowledge base that is represented as a system of ontologies. The proposed system of ontologies is a set of upper level ontologies for which techniques of adaptation for solving applied tasks for one or several related subject domains are developed.},
author = {Vodyaho, Alexander and Zhukova, Nataly},
keywords = {data analyses,data fusion,data-mining,knowledge representation,measure-,ments processing,ontology,situation recognition and assessment},
pages = {102--116},
title = {{System of Ontologies for Data Processing Applications Based on Implementation of Data Mining Techniques}},
year = {2014}
}
@article{Ekvall2005,
abstract = {We use the color cooccurrence histogram (CH) for recognizing objects in images. The color CH keeps track of the number of pairs of certain colored pixels that occur at certain separation distances in image space. The color CH adds geometric information to the normal color histogram, which abstracts away all geometry. We compute model CHs based on images of known objects taken from different points of view. These model CHs are then matched to subregions in test images to find the object. By adjusting the number of colors and the number of distances used in the CH, we can adjust the tolerance of the algorithm to changes in lighting, viewpoint, and the flexibility of the object. We develop a mathematical model of the algorithm's false alarm probability and use this as a principled way of picking most of the algorithm's adjustable parameters. We demonstrate our algorithm on different objects, showing that it recognize objects in spite of confusing background clutter, partial occlusions, and flexing of the object.},
author = {Ekvall, Staffan and Kragic, Danica and Hoffmann, Frank},
journal = {Image Vision Comput.},
keywords = {Color cooccurrence histograms,Model based tracking,Object recognition,Pose estimation},
pages = {943----955},
title = {{Object Recognition with Color Cooccurrence Histograms}},
volume = {23},
year = {2005}
}
@inproceedings{NazariShirehjini2005,
abstract = {The present paper introduces 3DSim, a tool for rapid prototyping Ambient Intelligence, applications. A major feature of this work is the use of a 3D-based virtual environment to represent an intelligent meeting space allowing for prototyping a number of components such as lights, blinds, SMART Boards™ large display walls, projectors, aware chairs, and humans as well as human activity animation. As a result of using standardized interfaces, any UPnP control point may be used to invoke actions on virtual, UPnP devices. Such devices are dynamically inserted to the environment and can be removed at run-time. Data delivered by sensors is interpreted by an environment monitoring component which routes higher-level atomic context information to virtual objects. As a result of invoked device actions and gathered context information, resulting state changes (e.g., switching light states) are visualized within the scene.},
address = {New York, New York, USA},
author = {{Nazari Shirehjini}, Ali Sghar and Klar, Felix},
booktitle = {Proceedings of the 2005 joint conference on Smart objects and ambient intelligence: innovative context-aware services: usages and technologies},
doi = {10.1145/1107548.1107621},
isbn = {1595933042},
keywords = {3D,Ambient Intelligence,Infrastructure Support,Pervasive Computing,Rapid Prototyping,Ubiquitous Computing,Virtual Reality,ambient intelligence,simulation},
number = {october},
pages = {303--307},
publisher = {ACM Press},
title = {{3DSim : Rapid Prototyping Ambient Intelligence}},
url = {http://www.soc-eusai2005.net/proceedings/articles{\_}pagines/77.pdf},
volume = {SoC-EUSAI},
year = {2005}
}
@incollection{Neal1998,
abstract = {The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible.},
author = {Neal, Radford and Hinton, Geoffrey E.},
booktitle = {Learning in Graphical Models},
doi = {10.1.1.114.4996},
isbn = {0262600323},
issn = {0883-4237},
keywords = {Bayesian Network,em,learning,missing data},
pages = {355--368},
publisher = {Kluwer Academic Publishers},
title = {{A View Of The Em Algorithm That Justifies Incremental, Sparse, And Other Variants}},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20{\&}path=ASIN/0262600323},
year = {1998}
}
@article{Scerri,
abstract = {Although a number of initiatives provide personalised context-aware guidance for niche use-cases, a standard framework for context awareness remains lacking. This article explains how semantic technology has been exploited to generate a centralised repository of personal activity context. This data drives advanced features such as, i) personal situation recognition and ii) customisable rules for the context-sensitive management of personal devices and data sharing. As a proof-of-concept, we demonstrate how an innovative context-aware},
author = {Scerri, Simon and Debattista, Jeremy and Attard, Judie and Rivera, Ismael},
title = {{A Semantic Infrastructure for Personalisable Context-aware Environments}},
year = {2014}
}
@inproceedings{Buford2006a,
abstract = {We extend the BDI (Belief, Desire, Intention) agent model by enabling agent beliefs to be based on real-time situations that are generated by a situation management (SM) system. This has several advantages for multi-agent systems using BDI agents. First, because of the use of event correlation and data fusion techniques in situation management, agent platforms can support highly reactive distributed applications. Second, the situation manager provides a semantically rich representation of the world and can dynamically adapt its representation for situations over time. From the system architecture perspective, we discuss several alternatives for how existing BDI-capable agent platforms can incorporate this extension. These alternatives range from complete SM functionality in each agent to having shared SMfunctionality among multiple agents. We also consider environments where different agent platforms use our Situation-Based BDI (SBBDI) Agent method and must interoperate. We include an example of an SBBDI agent system for homeland security threat assessment.},
author = {Buford, J and Jakobson, G. and Lewis, L.},
booktitle = {2006 9th International Conference on Information Fusion},
doi = {10.1109/ICIF.2006.301781},
isbn = {1-4244-0953-5},
keywords = {bdi,case-based,event correlation,multi-agent systems,sbbdi agent,situation management},
month = {7},
pages = {1--7},
publisher = {IEEE},
title = {{Extending BDI Multi-Agent Systems with Situation Management}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4086067},
year = {2006}
}
@article{Hepler2008,
abstract = {Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of interrelated objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language has clear declarative semantics: an OOBN can be interpreted as a stochastic functional program, so that it uniquely specifies a probabilistic model. We provide an inference algorithm for OOBNs, and show that much of the structural information encoded by an OOBN-particularly the encapsulation of variables within an object and the reuse of model fragments in different contexts--can also be used to speed up the inference process.},
archivePrefix = {arXiv},
author = {Hepler, Amanda B. and Weir, Bruce S.},
doi = {10.1016/j.fsigen.2007.12.003},
eprint = {1302.1554},
isbn = {1-55860-485-5},
issn = {18724973},
journal = {Forensic Science International: Genetics},
keywords = {bayesian networks,inference,object oriented},
month = {6},
number = {3},
pages = {166--175},
title = {{Object-oriented Bayesian networks for paternity cases with allelic dependencies}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1872497307004036},
volume = {2},
year = {2008}
}
@article{Ibrahim2015,
abstract = {In group activity recognition, the temporal dynamics of the whole activity can be inferred based on the dynamics of the individual people representing the activity. We build a deep model to capture these dynamics based on LSTM (long-short term memory) models. To make use of these ob- servations, we present a 2-stage deep temporal model for the group activity recognition problem. In our model, a LSTM model is designed to represent action dynamics of in- dividual people in a sequence and another LSTM model is designed to aggregate human-level information for whole activity understanding. We evaluate our model over two datasets: the collective activity dataset and a new volley- ball dataset. Experimental results demonstrate that our proposed model improves group activity recognition perfor- mance with compared to baseline methods.},
archivePrefix = {arXiv},
author = {Ibrahim, Moustafa and Muralidharan, Srikanth and Deng, Zhiwei and Vahdat, Arash and Mori, Greg},
eprint = {1511.06040},
keywords = {activity recognition,deep learning,group},
month = {11},
title = {{A Hierarchical Deep Temporal Model for Group Activity Recognition}},
url = {http://arxiv.org/abs/1511.06040},
year = {2015}
}
@article{Cohen1960,
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
month = {4},
number = {1},
pages = {37--46},
title = {{A Coefficient of Agreement for Nominal Scales}},
url = {http://journals.sagepub.com/doi/10.1177/001316446002000104},
volume = {20},
year = {1960}
}
@article{Stone2012,
author = {Stone, Erik and Skubic, Marjorie},
doi = {10.4108/icst.pervasivehealth.2012.248731},
isbn = {978-1-936968-43-5},
journal = {Proceedings of the 6th International Conference on Pervasive Computing Technologies for Healthcare},
keywords = {- gait,fall risk,kinect,passive monitoring,person tracking},
pages = {183--186},
publisher = {Ieee},
title = {{Passive, In-Home Gait Measurement Using an Inexpensive Depth Camera: Initial Results}},
url = {http://eudl.eu/doi/10.4108/icst.pervasivehealth.2012.248731},
year = {2012}
}
@article{Bates1994,
abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references. 4 Brooks, R. A robust layered control syst:em},
author = {Bates, Joseph},
doi = {10.1145/176789.176803},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {animation,art,arti cial intelligence,believable agents,believable characters,emotion},
month = {7},
number = {7},
pages = {122--125},
title = {{The role of emotion in believable agents}},
url = {http://portal.acm.org/citation.cfm?doid=176789.176803},
volume = {37},
year = {1994}
}
@inproceedings{Prankl2010,
abstract = {The vision and robotics communities have de- veloped a large number of increasingly successful methods for tracking, recognising and on-line learning of objects, all of which have their particular strengths and weaknesses. A researcher aiming to provide a robot with the ability to handle objects will typically have to pick amongst these and engineer a system that works for her particular setting. The work presented in this paper aims to provide a toolbox to simplify this task and to allow handling of diverse scenarios, though of course we have our own particular limitations: The toolbox is aimed at robotics research and as such we have in mind objects typically of interest for robotic manipulation scenarios, e.g. mugs, boxes and packaging of various sorts. We are not aiming to cover articulated objects (such as walking humans), highly irregular objects (such as potted plants) or deformable objects (such as cables). The system does not require specialised hardware and simply uses a single camera allowing usage on about any robot. The toolbox integrates state-of-the art methods for detection and learning of novel objects, and recognition and tracking of learned models. Integration is currently done via our own modular robotics framework, but of course the libraries making up the modules can also be separately integrated into own projects.},
author = {Prankl, J and Richtsfeld, A and Zillich, M and Vincze, M},
booktitle = {Best Practice in 3D Perception and Modeling for Mobile Manipulation (in conjunction with {\{}ICRA{\}} 2010)},
title = {{BLORT - The Blocks World Robotic Vision Toolbox}},
url = {http://users.acin.tuwien.ac.at/mzillich/files/moerwald2010blort.pdf},
year = {2010}
}
@article{McKeown2012,
abstract = {SEMAINE has created a large audiovisual database as part of an iterative approach to building Sensitive Artificial Listener (SAL) agents that can engage a person in a sustained, emotionally coloured conversation. Data used to build the agents came from interactions between users and an 'operator' simulating a SAL agent, in different configurations: Solid SAL (designed so that operators displayed appropriate non-verbal behaviour) and Semi-automatic SAL (designed so that users' experience approximated interacting with a machine). We then recorded user interactions with the developed system, Automatic SAL, comparing the most communicatively competent version to versions with reduced nonverbal skills. High quality recording was provided by 5 high-resolution, high framerate cameras, and 4 microphones, recorded synchronously. Recordings total 150 participants, for a total of 959 conversations with individual SAL characters, lasting approximately 5 minutes each. Solid SAL recordings are transcribed and extensively annotated: 6-8 raters per clip traced five affective dimensions and 27 associated categories. Other scenarios are labelled on the same pattern, but less fully. Additional information includes FACS annotation on selected extracts, identification of laughs, nods and shakes, and measures of user engagement with the automatic system. The material is available through a web-accessible database.},
author = {McKeown, Gary and Valstar, Michel and Cowie, Roddy and Pantic, Maja and Schroder, M.},
doi = {10.1109/T-AFFC.2011.20},
isbn = {1949-3045},
issn = {1949-3045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Emotional corpora,affective annotation,affective computing,social signal processing},
month = {1},
number = {1},
pages = {5--17},
title = {{The SEMAINE Database: Annotated Multimodal Records of Emotionally Colored Conversations between a Person and a Limited Agent}},
url = {http://ieeexplore.ieee.org/document/5959155/},
volume = {3},
year = {2012}
}
@article{Roshtkhari2013,
abstract = {We present a novel approach for video parsing and si- multaneous online learning of dominant and anomalous behaviors in surveillance videos. Dominant behaviors are those occurring frequently in videos and hence, usually do not attract much attention. They can be characterized by different complexities in space and time, ranging from a scene background to human activities. In contrast, an anomalous behavior is defined as having a low likelihood of occurrence. We do not employ any models of the entities in the scene in order to detect these two kinds of behaviors. In this paper, video events are learnt at each pixel with- out supervision using densely constructed spatio-temporal video volumes. Furthermore, the volumes are organized into large contextual graphs. These compositions are em- ployed to construct a hierarchical codebook model for the dominant behaviors. By decomposing spatio-temporal con- textual information into unique spatial and temporal con- texts, the proposed framework learns the models of the dom- inant spatial and temporal events. Thus, it is ultimately capable of simultaneously modeling high-level behaviors as well as low-level spatial, temporal and spatio-temporal pixel level changes.},
author = {Roshtkhari, Mehrsan Javan and Levine, Martin D.},
doi = {10.1109/CVPR.2013.337},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {activity recognition},
month = {6},
pages = {2611--2618},
publisher = {Ieee},
title = {{Online Dominant and Anomalous Behavior Detection in Videos}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619181},
year = {2013}
}
@article{MartinGarcia2013,
abstract = {We present an attention-based approach for the detection of unknown objects in a 3Denvironment. The abil- ity to address individual objects in the environment without having previous knowledge about their properties or their identity is one important requirement of the Situated Vision theory. Based on saliency maps, our attention system deter- mines the regions where objects are likely to be found; these are the proto-objects whose extent is refined by a 2D seg- mentation step. At the same time a 3D scene model is built from measurements of a depth camera. The detected objects are projected into the 3D scene, resulting in 3D object mod- els which are incrementally updated. We show the validity of our approach in an RGB-D sequence recorded in an office environment.},
annote = {"In the artificial intelligence and robotics community, a situated agent is an agent that is embedded in its environ- ment and aware of the situation that it is acting in."},
author = {{Mart{\'{i}}n Garc{\'{i}}a}, Germ{\'{a}}n and Frintrop, Simone and Cremers, Armin B.},
doi = {10.1007/s13218-013-0256-1},
issn = {0933-1875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
keywords = {attention,computational visual,object detection,situated vision,vision},
month = {7},
number = {3},
pages = {267--272},
title = {{Attention-Based Detection of Unknown Objects in a Situated Vision Framework}},
url = {http://link.springer.com/10.1007/s13218-013-0256-1},
volume = {27},
year = {2013}
}
@inproceedings{Nass1994,
abstract = {In this article Nass et al describe their approach to conducting research in the field ofhuman computer interaction: they take theories from other social sciences (like psychology) and try to replace the human-human interaction with human-computer interaction. Then they set up experimental tests to see if the same outcomes would be generated when these theories are applied in such a way. In this article they describe the general set-up of their research inpoliteness,stereotypesregarding the use ofvoices(male vs. female),genderetc. All of these studies lead to the conclusion that: "-Primitive cuesare powerful. - Social responses areautomaticandunconscious. - Findings in social psychology are relevant to human responses to computers - Human-computer interaction is social-psychological. - Experimental paradigms are interchangeable." [p. 77]},
address = {New York, New York, USA},
archivePrefix = {arXiv},
author = {Nass, Clifford and Steuer, Jonathan and Tauber, Ellen R.},
booktitle = {Companion of Human Actors in Computing Systems (CHI)},
doi = {10.1145/259963.260288},
eprint = {1607.05174},
isbn = {0897916514},
number = {June 2014},
pages = {204},
publisher = {ACM Press},
title = {{Computers are Social Actors}},
url = {http://portal.acm.org/citation.cfm?doid=259963.260288},
year = {1994}
}
@inproceedings{Joosse2014,
abstract = {In our daily life everything and everyone occupies an amount of space, simply by " being there " . Edward Hall coined the term proxemics for the studies of man's use of this space. This paper presents a study on proxemics in Human-Robot Interaction and particularly on robot's approaching groups of people. As social psychology research found proxemics to be culturally dependent, we focus on the question of the appropriateness of the robot's approach behavior in different cultures. We present an online survey (N=181) that was distributed in three countries; China, the U.S. and Argentina. Our results show that participants prefer a robot that stays out of people's intimate space zone just like a human would be expected to do. With respect to cultural differences, Chinese participants showed high-contact responses and believed closer approaches were appropriate compared to their U.S. counterparts. Argentinian participants more closely resembled the ratings of the U.S. participants.},
address = {New York, New York, USA},
author = {Joosse, Michiel P. and Poppe, Ronald W. and Lohse, Manja and Evers, Vanessa},
booktitle = {International Conference on Collaboration Across Boundaries: Culture, Distance {\&} Technology (CABS)},
doi = {10.1145/2631488.2631499},
isbn = {9781450325578},
pages = {121--130},
publisher = {ACM Press},
title = {{Cultural Differences in How an Engagement-Seeking Robot Should Approach a Group of People}},
url = {http://dl.acm.org/citation.cfm?doid=2631488.2631499},
year = {2014}
}
@inproceedings{cao2017realtime,
author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields}},
year = {2017}
}
@article{Tsai2015,
abstract = {The goal of addressee detection is to answer the question , “Are you talking to me?” When a dialogue system interacts with multiple users, it is crucial to detect when a user is speaking to the system as opposed to another person. We study this problem in a multimodal scenario, using lexical, acoustic, visual, dialogue state, and beamforming information. Using data from a multiparty dialogue system, we quantify the benefits of using multiple modalities over using a single modality. We also assess the relative importance of the various modalities, as well as of key individual features, in estimating the addressee. We find that energy-based acoustic features are by far the most important, that information from speech recognition and system state is useful as well, and that visual and beamforming features provide little additional benefit. While we find that head pose is affected by whom the speaker is addressing, it yields little nonredundant information due to the system acting as a situational attractor. Our findings would be relevant to multiparty, open-world dialogue systems in which the agent plays an active, conversational role, such as an interactive assistant deployed in a public, open space. For these scenarios , our study suggests that acoustic, lexical, and system-state information is an effective and practical combination of modalities to use for addressee detection. We also consider how our analyses might be affected by the ongoing development of more realistic, natural dialogue systems.},
author = {Tsai, T. J. and Stolcke, Andreas and Slaney, Malcolm},
doi = {10.1109/TMM.2015.2454332},
issn = {1520-9210},
journal = {IEEE Transactions on Multimedia},
keywords = {Addressee detection,beamforming,dialogue system,head pose,human-human-computer,multimodal,multiparty,prosody,speech recognition},
month = {9},
number = {9},
pages = {1550--1561},
title = {{A Study of Multimodal Addressee Detection in Human-Human-Computer Interaction}},
url = {http://ieeexplore.ieee.org/document/7153545/},
volume = {17},
year = {2015}
}
@article{Park2008,
abstract = {The independence of people who need help with daily activi- ties will become of vital importance to all societies in the future. This paper addresses the problem of controlling the assistive home environment and emphasizes human-friendly human-machine interactions in an approach designed to achieve independence. To provide residents with an accessible, convenient, and cost-effective environment for independent living, we in- troduce a new service robot, categorized as a steward robot, as an interme- diate agent between residents and their complex smart house environment. The learning capability and emotional interaction of the robot can make it more human-friendly in various tasks. A learning system enables the robot to provide customized services by accumulating knowledge of the user's be- havioral patterns in daily activities. An emotional interaction system gen- erates facial expressions to communicate with the user in a human-friendly manner.We have developed two types of a steward robot: a software type, which can be used everywhere via personal computing devices such as a PDA and a cellular phone, and a hardware type, which provides tangible services with physical interaction via two robotic arms and amobile base.},
author = {Park, Kwang-hyun and Lee, Hyong-euk and Kim, Youngmin and Bien, Z. Zenn},
doi = {10.1109/TASE.2007.911674},
issn = {1545-5955},
journal = {IEEE Transactions on Automation Science and Engineering},
keywords = {hri,smart home},
month = {1},
number = {1},
pages = {21--25},
title = {{A Steward Robot for Human-Friendly Human-Machine Interaction in a Smart House Environment}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4385549 http://ieeexplore.ieee.org/document/4385549/},
volume = {5},
year = {2008}
}
@inproceedings{Jeong2009,
abstract = {In this paper, we target the problem of the situation- aware application (task) recommendation on mobile de- vices. To tackle this problem, we develop both supervised and unsupervised approaches. We use Na¨ ıve Bayesian as a supervised approach, and co-clustering and vector quantization (VQ) as unsupervised approaches. We eval- uate the performance of the proposed approaches with both synthetic and actual user log data that we have collected for six months. Our initial experiment shows that the co-clustering-based approach results in comparable purity performance with much less computation time than VQ. Therefore, the co-clustering approach can be practical for high dimensional data. Furthermore, we characterize the recommendation performance of the proposed approaches in terms of the receiver-operating-characteristics (ROC). One interesting observation is that the unsupervised ap- proaches performwell with a single identical threshold over all applications, while the supervised approach does better with a different threshold for each application.},
author = {Jeong, Sangoh and Kalasapur, Swaroop and Cheng, Doreen and Song, Henry and Cho, Hyuk},
booktitle = {2009 International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2009.75},
isbn = {978-0-7695-3926-3},
month = {12},
pages = {353--358},
publisher = {IEEE},
title = {{Clustering and Na{\"{i}}ve Bayesian Approaches for Situation-Aware Recommendation on Mobile Devices}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5381517},
year = {2009}
}
@article{Premack1978,
abstract = {An individual has a theory of mind if he imputes mental states to himself and others. A system of inferences of this kind is properly viewed as a theory because such states are not directly observable, and the system can be used to make predictions about the behavior of others. As to the mental states the chimpanzee may infer, consider those inferred by our own species, for example, purpose or intention, as well as knowledge, belief, thinking, doubt, guessing, pretending, liking, and so forth. To determine whether or not the chimpanzee infers states of this kind, we showed an adult chimpanzee a series of videotaped scenes of a human actor struggling with a variety of problems. Some problems were simple, involving inaccessible food - bananas vertically or horizontally out of reach, behind a box, and so forth - as in the original Kohler problems; others were more complex, involving an actor unable to extricate himself from a locked cage, shivering because of a malfunctioning heater, or unable to play a phonograph because it was unplugged. With each videotape the chimpanzee was given several photographs, one a solution to the problem, such as a stick for the inaccessible bananas, a key for the locked up actor, a lit wick for the malfunctioning heater. The chimpanzee's consistent choice of the correct photographs can be understood by assuming that the animal recognized the videotape as representing a problem, understood the actor's purpose, and chose alternatives compatible with that purpose.},
author = {Premack, David and Woodruff, Guy},
doi = {10.1017/S0140525X00076512},
issn = {0140-525X},
journal = {Behavioral and Brain Sciences},
keywords = {1925,and simple tool use,carried out his now,chimpanzee communication,cognition,consciousness,fifty years ago kohler,he confronted his chimpanzees,in,intentionality,language,mind,primate intelligence,studies demonstrating problem solving,the chimpanzee,widely known,with food that},
month = {12},
number = {04},
pages = {515},
title = {{Does the chimpanzee have a theory of mind?}},
url = {http://www.journals.cambridge.org/abstract{\_}S0140525X00076512},
volume = {1},
year = {1978}
}
@inproceedings{Cech2013,
abstract = {In this paper we present a method for detecting and localizing an active speaker, i.e., a speaker that emits a sound, through the fusion between visual reconstruction with a stereoscopic camera pair and sound-source localization with several microphones. Both the cameras and the microphones are embedded into the head of a humanoid robot. The proposed statistical fusion model associates 3D faces of potential speakers with 2D sound directions. The paper has two contributions: (i) a method that discretizes the two-dimensional space of all possible sound directions and that accumulates evidence for each direction by estimating the time difference of arrival (TDOA) over all the microphone pairs, such that all the microphones are used simultaneously and symmetrically and (ii) an audio-visual alignment method that maps 3D visual features onto 2D sound directions and onto TDOAs between microphone pairs. This allows to implicitly represent both sensing modalities into a common audiovisual coordinate frame. Using simulated as well as real data, we quantitatively assess the robustness of the method against noise and reverberations, and we compare it with several other methods. Finally, we describe a real-time implementation using the proposed technique and with a humanoid head embedding four microphones and two cameras: this enables natural human-robot interactive behavior.},
annote = {Speaker detection by correlating sound source localization with faced fund.},
author = {Cech, Jan and Mittal, Ravi and Deleforge, Antoine and Sanchez-Riera, Jordi and Alameda-Pineda, Xavier and Horaud, Radu},
booktitle = {2013 13th IEEE-RAS International Conference on Humanoid Robots (Humanoids)},
doi = {10.1109/HUMANOIDS.2013.7029977},
isbn = {978-1-4799-2617-6},
issn = {21640580},
keywords = {humavips,nao,speaker detection},
month = {10},
number = {February},
pages = {203--210},
publisher = {IEEE},
title = {{Active-speaker detection and localization with microphones and cameras embedded into a robotic head}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84937845770{\&}partnerID=tZOtx3y1 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7029977},
volume = {2015-Febru},
year = {2013}
}
@inproceedings{Thomson2006,
abstract = {Automatically determining the situation of an ad-hoc group of people and devices within a smart environment is a significant challenge in pervasive computing systems. Current approaches often rely on an environment expert to correlate the situations that occur with the available sen- sor data, while other machine learning based approaches require long training periods before the system can be used. This paper presents a novel approach to situation determi- nation that attempts to overcome these issues by providing a reusable library of general situation specifications that can be easily extended to create new specific situations, and im- mediately deployed without the need of an environment ex- pert. A proposed architecture of an accompanying situation determination middleware is provided, as well as an analy- sis of a prototype implementation.},
author = {Thomson, G. and Terzis, S. and Nixon, P.},
booktitle = {Fourth Annual IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOMW'06)},
doi = {10.1109/PERCOMW.2006.126},
isbn = {0-7695-2520-2},
pages = {620--623},
publisher = {IEEE},
title = {{Situation Determination with Reusable Situation Specifications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1599061},
year = {2006}
}
@inproceedings{OpdenAkker2009,
abstract = {Rieks op den Akker and David Traum, A comparison of addressee detection methods for multiparty conversations, in proceedings of Diaholmia 2009: the 13th workshop on the semantics and pragmatics of dialogue. Stockholm, Sweden, June 2009.},
address = {Stockholm, Sweden},
author = {op den Akker, Rieks and Traum, David},
booktitle = {DiaHolmia Workshop on the Semantics and Pragmatics of Dialogue},
pages = {99--106},
title = {{A Comparison of Addressee Detection Methods for Multiparty Conversations}},
year = {2009}
}
@inproceedings{Fischer2012,
abstract = {In today's surveillance systems, there is a need for enhancing the situation awareness of an operator. Supporting the situation assessment process can be done by extending the system with a module for automatic interpretation of the observed environment. In this article we introduce a consistent terminology for the domain of intelligent surveillance systems. We clarify the separation of the real world and the world model, which is used for the internal representation in the system. For the definition of an automatic situation assessment module, we make use of an existing conceptual framework. We will further introduce a concept for an internal representation of situations of interest and show how the existence of such situations can be inferred from sensor observations. Based on these considerations, an automatic situation assessment module for a maritime surveillance system was developed. The module was evaluated with a small user group and the results show that such an automatic support reduces the workload of the user and is highly accepted.},
annote = {Situation: statement about constellation of entities in the environment


Modeled: binary random variable


BayesianNetwork: modeled by experts


Related: [11] Markov random fields are used to model contextual relationships and maximum a posteriori labeling is used to infer intentions of observed elements


Situation characterization and template situations,Network of situations is a PGM
* E: directly inferable situations (evidence variables)
* S: indirectly observable situations (state variables)


Situation abstraction/representation
* go from evidence to situations through machine learning or threshold


Situation recognition
* matching representation to template (belief for each template)},
author = {Fischer, Yvonne and Beyerer, Jurgen},
booktitle = {2012 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support},
doi = {10.1109/CogSIMA.2012.6188404},
isbn = {978-1-4673-0345-3},
keywords = {bayesian,situation,situation awareness,situation recognition,surveillance},
month = {3},
pages = {324--331},
publisher = {IEEE},
title = {{Acceptance of automatic situation assessment in surveillance systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6188404},
year = {2012}
}
@inproceedings{Ju2008,
abstract = {In this paper, we developed the intelligent interface between the user and the wheelchair. To facilitate a wide variety of user abilities, the proposed system uses face-inclination and mouth-shape information as user's intention, where the di- rection of an IW is determined by the inclination of the user's face, while proceeding and stopping are determined by the shape of the user's mouth. This mechanism requires minimal motion, thereby making the system more comfortable and adaptable for the severely disabled. Furthermore, to fully guarantee user's safety, the 10 range-sensors are used to detect obstacles in environment and avoid them. To assess the ef- fectiveness of the proposed IW, it was tested with 34 users and the results show that it can provide a user unable to drive a standard joystick with friendly and convenient system.},
address = {New York, New York, USA},
author = {Ju, Jin Sun and Shin, Yunhee and Kim, Eun Yi},
booktitle = {Proceedingsc of the 13th international conference on Intelligent user interfaces - IUI '09},
doi = {10.1145/1502650.1502693},
isbn = {9781605581682},
keywords = {adaboost,facial feature recognition,intelligent interface,intelligent wheelchair,sion based interface,vi-},
pages = {307},
publisher = {ACM Press},
title = {{Intelligent wheelchair (IW) interface using face and mouth recognition}},
url = {http://portal.acm.org/citation.cfm?doid=1502650.1502693},
year = {2008}
}
@inproceedings{Nicolai06exploringsocial,
abstract = {The Wireless Rope is a framework to study the notion of social context and the detection of social situations by Bluetooth proximity detection with consumer devices and its effects on group dynamics. Users can interact through a GUI with members of an existing group or form a new group. Connection information is collected by stationary tracking devices and a connection map of all participants can be obtained via the web. Besides interaction with familiar persons, the Wireless Rope also includes strange persons to provide a rich representation of the surrounding social situation. This paper seeks to substantiate the notion of social context by an exploratory analysis of interpersonal proximity data collected during a computer conference. Two feature functions are presented that indicate typical situations in this setting.},
author = {Nicolai, Tom and Behrens, Nils and Kenn, Holger},
booktitle = {In Proc. Workshop MONET: LNCS 4277},
keywords = {social robotics},
title = {{Exploring Social Context with the Wireless Rope}},
year = {2006}
}
@phdthesis{Dahlbom2011,
abstract = {Situation recognition is a process with the goal of identifying a priori defined situations in a flow of data and information. The purpose is to aid decision makers with focusing on relevant information by filtering out situations of interest. This is an increasingly important and non trivial problem to solve since the amount of information in various decision making situations constantly grow. Situation recognition thus addresses the information gap, i.e. the problem of finding the correct information at the correct time. Interesting situations may also evolve over time and they may consist of multiple participating objects and their actions. This makes the problem even more complex to solve. This thesis explores situation recognition and provides a conceptualization and a definition of the problem, which allow for situations of partial temporal definition to be described. The thesis then focuses on investigating how Petri nets can be used for recognising situations. Existing Petri net based approaches for recognition have some limitations when it comes to fulfilling requirements that can be put on solutions to the situation recognition problem. An extended Petri net based technique that addresses these limitations is therefore introduced. It is shown that this technique can be as efficient as a rule based techniques using the Rete algorithm with extensions for explicitly representing temporal constraints. Such techniques are known to be efficient; hence, the Petri net based technique is efficient too. The thesis also looks at the problem of learning Petri net situation templates using genetic algorithms. Results points towards complex dynamic genome representations as being more suited for learning complex concepts, since these allow for promising solutions to be found more quickly compared with classical bit string based representations. In conclusion, the extended Petri net based technique is argued to offer a viable approach for situation recognition since it: (1) can achieve good recognition performance, (2) is efficient with respect to time, (3) allows for manually constructed situation templates to be improved and (4) can be used with real world data to find real world situations.},
annote = {"Situation recognition is a process with the goal of identifying a priori defined situations in a flow of data and information. The purpose is to aid decision makers with focusing on relevant information by filtering out situations of in- terest."

"Simply put, the situation analysis process can be broken down into many subprocesses, at many levels of abstraction, which together maintain the situation model."

"In conclusion, a suitable set of symbolic relations are needed, by which an
abstraction of the domain of interest can be captured. Steinberg (2009) identi- fies five categories of relations: logical/semantic, physical, functional, conven- tional and cognitive. Similarly, Lambert (2003c) identifies five levels of rela- tions: social, intentional, functional, physical and metaphysical. For more information about types of relations, see Lambert (2003c); Steinberg (2009)."

Problem gap: find correct information at correct time

OODA loop: 
* Observe. Data and information regarding the situation is gathered from many different sources in the observe phase.
* Orient. In order to establish some form of awareness, the data and infor- mation is analysed in the orient phase. Past information, culture, heritage and experience play key roles here.
* Decide. In the decide phase, some decision making paradigm is used to decide on what to do, in light of the established awareness.
* Act. Finally, the decision is carried out in the act phase, having an impact on the world, which again can be observed (although usually after some delay, since most actions are not carried out instantaneously)

Situation awareness as result of Observe{\&}Orient (Roy,2001):
* situation model continuously developed in OO phases

lambda-JDL
* Object fusion is the process of utilising one or more data sources over time to assemble a representation of objects of interest in an environment, and an object assessment is a stored representation of objects.
* Situation fusion is the process of utilising one or more data sources over time to assemble a representation of relations between objects of interest in an environment, and a situation assessment is a stored representation of relations between objects
* Impact fusion is the process of utilising one or more data sources over time to assemble a representation of effects of situations in an environment, rel- ative to our intentions, and an impact assessment is a stored representation of effects of situations.

Chronicle recognition: recognition of situations/events in a timed eventstream},
author = {Dahlbom, Anders},
isbn = {9789176687796},
keywords = {Bayesian Network Fragments,Multi Entity BN (MEBN),OODA loop,Petri nets,Rete algorithm,Situation recognition,genetic algorithms,information fusion,petri nets,rule based,situation assessment,situation recognition},
pages = {250},
title = {{Petri nets for Situation Recognition}},
year = {2011}
}
@book{Barry2010,
author = {Barry, Paul},
edition = {1},
keywords = {head-first,programming,python},
pages = {494},
title = {{Head First: Python}},
year = {2010}
}
@inproceedings{Cabrera-Quiros2016,
address = {New York, New York, USA},
author = {Cabrera-Quiros, Laura},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing Adjunct - UbiComp '16},
doi = {10.1145/2968219.2971359},
isbn = {9781450344623},
keywords = {acceleration,cameras,human behavior analysis,movement,proximity,video,wearable sensors},
pages = {399--404},
publisher = {ACM Press},
title = {{Towards multimodal analysis of human behavior in crowded mingling scenarios using movement cues from wearable sensors and cameras}},
url = {http://doi.acm.org/10.1145/2968219.2971359 http://dl.acm.org/citation.cfm?doid=2968219.2971359},
year = {2016}
}
@article{Kuhnel2011,
abstract = {Mobile phones seem to present the perfect user interface for interacting with smart environments, e.g. smart-home systems, as they are nowadays ubiquitous and equipped with an increasing amount of sensors and interface components, such as multi-touch screens. After giving an overview on related work this paper presents the adapted design methodology proposed by Wobbrock et al. (2009) for the development of a gesture-based user interface to a smart-home system. The findings for the new domain, device and gesture space are presented and compared to findings by Wobbrock et al. (2009). Three additional steps are described: A small pre-test survey, a mapping and a memory test and a performance test of the implemented system. This paper shows the adaptability of the approach described by Wobbrock et al. (2009) for three-dimensional gestures in the smart-home domain. Elicited gestures are described and a first implementation of a user interface based on these gestures is presented. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {K{\"{u}}hnel, Christine and Westermann, Tilo and Hemmert, Fabian and Kratz, Sven and M{\"{u}}ller, Alexander and M{\"{o}}ller, Sebastian},
doi = {10.1016/j.ijhcs.2011.04.005},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {Gesture-based interaction,Mobile device,Smart-home,User-centered design},
month = {10},
number = {11},
pages = {693--704},
title = {{I'm Home: Defining and Evaluating a Gesture Set for Smart-Home Control}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1071581911000668 https://linkinghub.elsevier.com/retrieve/pii/S1071581911000668},
volume = {69},
year = {2011}
}
@inproceedings{Xu2013,
abstract = {Recognizing users' engagement state and intentions is a pressing task for computational agents to facilitate fluid conversations in situated interactions. We investigate how to quantitatively evaluate high-level user engagement and intentions based on low-level visual cues, and how to design engagement-aware behaviors for the conversational agents to behave in a sociable manner. Drawing on machine learning techniques, we propose two computational models to quantify users' attention saliency and engagement intentions. Their performances are validated by a close match between the predicted values and the ground truth annotation data. Next, we design a novel engagement-aware behavior model for the agent to adjust its direction of attention and manage the conversational floor based on the estimated users' engagement. In a user study, we evaluated the agent's behaviors in a multiparty dialog scenario. The results show that the agent's engagement-aware behaviors significantly improved the effectiveness of communication and positively affected users' experience.},
address = {New York, New York, USA},
author = {Xu, Qianli and Li, Liyuan and Wang, Gang},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2481308},
isbn = {9781450318990},
keywords = {engagement,gaze,human-robot interaction,intention,interaction,multiparty,multiparty conversation,visual perception.},
pages = {2233},
publisher = {ACM Press},
title = {{Designing engagement-aware agents for multiparty conversations}},
url = {http://dl.acm.org/citation.cfm?id=2470654.2481308 http://dl.acm.org/citation.cfm?doid=2470654.2481308},
year = {2013}
}
@inproceedings{Katz-ISER2010,
annote = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/iser2010{\_}Katz{\_}Orthey{\_}Brock.pdf},
author = {{Dov Katz Andreas Orthey} and Brock, Oliver},
booktitle = {International Symposium of Experimental Robotics},
month = {12},
pages = {1--15},
title = {{Interactive Perception of Articulated Objects}},
url = {http://www.robotics.tu-berlin.de/fileadmin/fg170/Publikationen{\_}pdf/iser2010{\_}Katz{\_}Orthey{\_}Brock.pdf},
year = {2010}
}
@inproceedings{Pauwels2013,
abstract = {We propose a novel model-based method for estimat- ing and tracking the six-degrees-of-freedom (6DOF) pose of rigid objects of arbitrary shapes in real-time. By com- bining dense motion and stereo cues with sparse keypoint correspondences, and by feeding back information from the model to the cue extraction level, the method is both highly accurate and robust to noise and occlusions. A tight in- tegration of the graphical and computational capability of Graphics Processing Units (GPUs) results in pose updates at framerates exceeding 60 Hz. Since a benchmark dataset that enables the evaluation of stereo-vision-based pose esti- mators in complex scenarios is currently missing in the lit- erature, we have introduced a novel synthetic benchmark dataset with varying objects, background motion, noise and occlusions. Using this dataset and a novel evaluation methodology, we show that the proposed method greatly outperforms state-of-the-art methods. Finally, we demon- strate excellent performance on challenging real-world se- quences involving object manipulation.},
address = {Portland},
author = {Pauwels, Karl and Rubio, Leo and {Diaz Alonso}, Javier and Ros, Eduardo},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
doi = {10.1109/CVPR.2013.304},
title = {{Real-time Model-based Rigid Object Pose Estimation and Tracking Combining Dense and Sparse Visual Cues}},
year = {2013}
}
@inproceedings{Shirehjini2006,
abstract = {A systematic activity-based domain study is presented, analysing the context of multimedia presentation scenarios. The analysis was motivated by the aim of situation recognition and the realisation of proactive environment control systems. Altogether information is collected for presentation scenarios in an activity- centred approach. During a first preliminary study seven major situations have been identified for the presentation process, including Setup, Welcome, Presentation, Explanation, Question{\&}Answer, Discussion, and Conclusion. Additionally, following parameters have been defined: Activity, Location, Interaction, Media, Device, Environment and Audience states (e.g., light and noise setting, user movement, number of person, level of attention, interruptions). These parameters provide distinguishing characteristics for mentioned situation at a high-level and do not depend on a specific technology or sensor (e.g., A/V indexing). In a second step, interview and observation techniques have been deployed to collect data characterizing those seven situations with previously defined parameters. By doing so, the user study quantifies the situations of a presentation scenario; it provides a statistical situation model.},
annote = {Situationserkennung zur proaktiven Umgebungssteuerung bei Pr{\"{a}}sentationen
---
* "Situation modelling is a central building block of pro- active systems."* "The accuracy of situation recognition and the quality of implicated pro-active support depends directly on the quality of the situation model, whuich underlie the design of a system."
---* Studie: Aufnahmen von Pr{\"{a}}sentationen mit Frageb{\"{o}}gen
* Situationen
** Aufbau
** Empfang
** Pr{\"{a}}sentation
** Erkl{\"{a}}rung
** Fragen {\&} Antworten
** Diskussion
** Schluss


* Parameter (Unterscheidungscharakteristiken. Diskret. Status von:)
** Aktivit{\"{a}}t 
** Ort (Wo steht der vortragende?)
** Interaktion
** Meden
** Ger{\"{a}}t
** Umgebung
** Audienz


* Statistisches Situationsmodell quantifiziert:
** Parameter differenzieren Situationen gut -{\textgreater} distanzen zwischen Vektoren signifikant
* 'Sensor-independent situation modelling' (siehe Parameter)},
author = {Shirehjini, All A Nazari},
booktitle = {Intelligent Environments, 2006. IE 06. 2nd IET International Conference on},
keywords = {situation},
pages = {193--199},
title = {{Situation modelling: A domain user study}},
year = {2006}
}
@inproceedings{Cristani2011a,
abstract = {This paper proposes a study corroborated by preliminary experiments on the inference of social relations based on the analysis of interpersonal distances, measured with on obtrusive computer vision techniques. The experiments have been performed over 13 individuals involved in casual standing conversations and the results show that people tend to get closer when their relation is more intimate. In other words, social and physical distances tend to match one another. In this respect, the results match the findings of proxemics, the discipline studying the social and affective meaning of space use and organization in social gatherings. The match between results and expectations of proxemics is observed also when changing one of the most important contextual factors in this type of scenarios, namely the amount of space available to the interactants.},
author = {Cristani, Marco and Paggetti, Giulia and Vinciarelli, Alessandro and Bazzani, Loris and Menegaz, Gloria and Murino, Vittorio},
booktitle = {International Conference on Privacy, Security, Risk and Trust / International Conference on Social Computing},
doi = {10.1109/PASSAT/SocialCom.2011.32},
isbn = {978-1-4577-1931-8},
month = {10},
pages = {290--297},
publisher = {IEEE},
title = {{Towards Computational Proxemics: Inferring Social Relations from Interpersonal Distances}},
url = {http://ieeexplore.ieee.org/document/6113127/},
year = {2011}
}
@article{Shi2015,
abstract = {Consider a situation where a robot initiates a conversation with a person. What is the appropriate timing for such an action? Where is a good position from which to make the initial greeting? In this study, we analyze human interactions and establish a model for a natural way of initiating conversation. Our model mainly involves the participation state and spatial formation. When a person prepares to participate in a conversation and a particular spatial formation occurs, he/she feels that he/she is participating in the conversation; once he/she perceives his/her participation, he/she maintains particular spatial formations. Theories have addressed human communication related to these concepts, but they have only covered situations after people start to talk. In this research, we created a participation state model for measuring communication participation and provided a clear set of guidelines for how to structure a robot's behavior to start and maintain a conversation based on the model. Our model precisely describes the constraints and expected behaviors for the phase of initiating conversation. We implemented our proposed model in a humanoid robot and conducted both a system evaluation and a user evaluation in a shop scenario experiment. It was shown that good recognition accuracy of interaction state in a conversation was achieved with our proposed model, and the robot implemented with our proposed model was evaluated as best in terms of appropriateness of behaviors and interaction efficiency compared with other two alternative conditions.},
annote = {look into 22, 3},
author = {Shi, Chao and Shiomi, Masahiro and Kanda, Takayuki and Ishiguro, Hiroshi and Hagita, Norihiro},
doi = {10.1007/s12369-015-0285-z},
issn = {1875-4791},
journal = {International Journal of Social Robotics},
keywords = {Behavior modeling,Initiation of interaction,Natural-HRI},
month = {11},
number = {5},
pages = {889--910},
title = {{Measuring Communication Participation to Initiate Conversation in Human–Robot Interaction}},
url = {http://dx.doi.org/10.1007/s12369-015-0285-z http://link.springer.com/10.1007/s12369-015-0285-z},
volume = {7},
year = {2015}
}
@inproceedings{Jayagopi2013a,
abstract = {We introduce a new conversational Human-Robot-Interaction (HRI) dataset with a real-behaving robot inducing interactive behavior with and between humans. Our scenario involves a humanoid robot NAO1 explaining paintings in a room and then quizzing the participants, who are naive users. As perceiving nonverbal cues, apart from the spoken words, plays a major role in social interactions and socially-interactive robots, we have extensively annotated the dataset. It has been recorded and annotated to benchmark many relevant perceptual tasks, towards enabling a robot to converse with multiple humans, such as speaker localization and speech segmentation; tracking, pose estimation, nodding, visual focus of attention estimation in visual domain; and an audio-visual task such as addressee detection. NAO system states are also available. As compared to recordings done with a static camera, this corpus involves the head-movement of a humanoid robot (due to gaze change, nodding), posing challenges to visual processing. Also, the significant background noise present in a real HRI setting makes auditory tasks challenging.},
author = {Jayagopi, Dinesh Babu and Sheiki, Samira and Klotz, David and Wienke, Johannes and Odobez, Jean-Marc and Wrede, Sebastien and Khalidov, Vasil and Nyugen, Laurent and Wrede, Britta and Gatica-Perez, Daniel},
booktitle = {International Conference on Human-Robot Interaction (HRI)},
doi = {10.1109/HRI.2013.6483545},
isbn = {978-1-4673-3101-2},
keywords = {HRI corpus,Multimodal dataset,Social-robotics,Vernissage},
month = {3},
pages = {149--150},
publisher = {ACM/IEEE},
title = {{The Vernissage Corpus: A Conversational Human-Robot-Interaction Dataset}},
url = {http://ieeexplore.ieee.org/document/6483545/},
year = {2013}
}
@misc{Richter2013,
author = {Richter, Viktor},
number = {November},
title = {{Framework zum Lernen und Verfolgen von Objekten mithilfe eines Tiefensensors}},
year = {2013}
}
@article{Goffman1979,
address = {Abingdon, UK},
author = {Goffman, Erving},
chapter = {Footing},
doi = {10.1515/semi.1979.25.1-2.1},
isbn = {978-0-203-00234-6},
issn = {0037-1998},
journal = {Semiotica},
keywords = {code switching,conversational roles,footing,interaction,social situation},
number = {1-2},
pages = {1--30},
publisher = {Taylor {\&} Francis},
title = {{Footing}},
url = {http://www.tandfebooks.com/action/showBook?doi=10.4324/9780203002346 https://www.degruyter.com/view/j/semi.1979.25.issue-1-2/semi.1979.25.1-2.1/semi.1979.25.1-2.1.xml},
volume = {25},
year = {1979}
}
@article{Collet2009,
abstract = {Robust perception is a vital capability for robotic manipulation in unstructured scenes. In this context, full pose estimation of relevant objects in a scene is a critical step towards the introduction of robots into household environments. In this paper, we present an approach for building metric 3D models of objects using local descriptors from several images. Each model is optimized to fit a set of calibrated training images, thus obtaining the best possible alignment between the 3D model and the real object. Given a new test image, we match the local de- scriptors to our stored models online, using a novel combination of the RANSAC and Mean Shift algorithms to register multiple instances of each object. A robust initialization step allows for arbitrary rotation, translation and scaling of objects in the test images. The resulting system provides markerless 6-DOF pose estimation for complex objects in cluttered scenes. We provide experimental results demonstrating orientation and translation accuracy, as well a physical implementation of the pose output being used by an autonomous robot to perform grasping in highly cluttered scenes.},
author = {Collet, Alvaro and Berenson, Dmitry and Srinivasa, Siddhartha S. and Ferguson, Dave},
doi = {10.1109/ROBOT.2009.5152739},
isbn = {978-1-4244-2788-8},
journal = {2009 IEEE International Conference on Robotics and Automation},
month = {5},
pages = {48--55},
publisher = {Ieee},
title = {{Object recognition and full pose registration from a single image for robotic manipulation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152739},
year = {2009}
}
@inproceedings{Pejsa,
author = {Pejsa, Tomislav and Gleicher, Michael and Mutlu, Bilge},
booktitle = {International Conference on Intelligent Virtual Agents},
doi = {10.1007/978-3-319-67401-8_45},
keywords = {embodied conversational agents,gaze,orientation,virtual reality},
pages = {347--359},
title = {{Who, Me? How Virtual Agents Can Shape Conversational Footing in Virtual Reality}},
url = {http://link.springer.com/10.1007/978-3-319-67401-8{\_}45},
year = {2017}
}
@book{Simon2011,
author = {Simon, Jonathan},
isbn = {9781449393304},
keywords = {android,head-first,programming},
pages = {532},
title = {{Head First: Android Development}},
year = {2011}
}
@inproceedings{Inamura2006,
abstract = {Memorization, abstraction, and generation of a time-series of sensors and motion patterns are some of the most important functions for intelligent robots, because these memories are useful for situation recognition and behavior decision making. In conventional research, recurrent neural networks are often used for such memory functions. However, they cannot memorize a lot of patterns and its learning algorithm is unreliable. In this paper, we propose a method for the induction of behavior and situational estimation based on Hidden Markov Models, which is currently one of the most useful stochastic models. With the proposed method, we show the feasibility of: (1) Both recognition and association are executed at the same time, and (2) A multiple degrees of freedom and multiple sensorimotor patterns are acceptable.},
author = {Inamura, Tetsunari and Kojo, Naoki and Inaba, Masayuki},
booktitle = {2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2006.282609},
isbn = {1-4244-0258-1},
keywords = {situation},
month = {10},
pages = {5147--5152},
publisher = {IEEE},
title = {{Situation Recognition and Behavior Induction based on Geometric Symbol Representation of Multimodal Sensorimotor Patterns}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4059240},
year = {2006}
}
@book{Bringhurst2008,
author = {Bringhurst, Robert},
edition = {3.2},
isbn = {0881792063},
publisher = {Hartley {\&} Marks},
title = {{The Elements of Typographic Style}},
year = {1992}
}
@article{Welke2010,
abstract = {The autonomous acquisition of object represen- tations which allow recognition, localization and grasping of objects in the environment is a challenging task, which has shown to be difficult. In this paper, we present a systems for autonomous acquisition of visual object representations, which endows a humanoid robot with the ability to enrich its internal object representation and allows the realization of complex visual tasks. More precisely, we present techniques for segmentation and modeling of objects held in the five-fingered robot hand. Multiple object views are generated by rotating the held objects in the robot's field of view. The acquired object representations are evaluated in the context of visual search and object recognition tasks in cluttered environments. Experimental results show successful implementation of the complete cycle from object exploration to object recognition on a humanoid robot.},
author = {Welke, Kai and Issac, Jan and Schiebener, David and Asfour, Tamim and Dillmann, Ruediger},
doi = {10.1109/ROBOT.2010.5509328},
isbn = {978-1-4244-5038-1},
journal = {2010 IEEE International Conference on Robotics and Automation},
month = {5},
pages = {2012--2019},
publisher = {Ieee},
title = {{Autonomous acquisition of visual multi-view object representations for object recognition on a humanoid robot}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5509328},
year = {2010}
}
@inproceedings{Oertel2014,
abstract = {In the last couple of years more and more multimodal corpora have been created. Recently many of these corpora have also included RGB-D sensors' data. However, there is to our knowledge no publicly available corpus, which combines accurate gaze-tracking, and high- quality audio recording for group discussions of varying dynamics. With a corpus that would fulfill these needs, it would be possible to investigate higher level constructs such as group involvement, individual engagement or rapport, which all require multi-modal feature extraction. In the following paper we describe the design and recording of such a corpus and we provide some illustrative examples of how such a corpus might be exploited in the study of group dynamics.},
address = {New York, New York, USA},
author = {Oertel, Catharine and {Funes Mora}, Kenneth Alberto and Sheikhi, Samira and Odobez, Jean-Marc and Gustafson, Joakim},
booktitle = {Proceedings of the 2014 workshop on Understanding and Modeling Multiparty, Multimodal Interactions - UM3I '14},
doi = {10.1145/2666242.2666251},
isbn = {9781450306522},
keywords = {corpus,dialog,eye-gaze,group dynamics,involvement,turn-taking},
pages = {27--32},
publisher = {ACM Press},
title = {{Who Will Get the Grant?}},
url = {http://dl.acm.org/citation.cfm?doid=2666242.2666251},
year = {2014}
}
@inproceedings{Trafton2008,
abstract = {We describe a computational cognitive architecture for robots which we call ACT-R/E (ACT-R/Embodied). ACT-R/E is based on ACT-R [1, 2] but uses different visual, auditory, and movement modules. We describe a model that uses ACT-R/E to integrate visual and auditory information to perform conversation tracking in a dynamic environment. We also performed an empirical evaluation study which shows that people see our conversational tracking system as extremely natural.},
address = {New York, New York, USA},
author = {Trafton, J. Gregory and Bugajska, Magda D. and Fransen, Benjamin R. and Ratwani, Raj M.},
booktitle = {Proceedings of the 3rd international conference on Human robot interaction - HRI '08},
doi = {10.1145/1349822.1349849},
isbn = {9781605580173},
issn = {2167-2121},
keywords = {act-r,cognitive modeling,con-,human-robot interaction},
pages = {201},
publisher = {ACM Press},
title = {{Integrating vision and audition within a cognitive architecture to track conversations}},
url = {http://portal.acm.org/citation.cfm?doid=1349822.1349849},
year = {2008}
}
@incollection{Levinson1992,
author = {Levinson, Stephen C.},
booktitle = {Talk at work: Interaction in Institutional Settings},
pages = {66--100},
publisher = {Cambridge University Press},
title = {{Activity Types and Language}},
year = {1992}
}
@inproceedings{Mozer1992,
abstract = {Learning structure in temporally-extended sequences is a diicult com-putational problem because only a fraction of the relevant information is available at any instant. Although variants of back propagation can in principle be used to structure in sequences, in practice they are not suuciently powerful to discover arbitrary contingencies, especially those spanning long temporal intervals or involving high order statistics. For example, in designing a connectionist network for music composition, we have encountered the problem that the net is able to learn musical struc-ture that occurs locally in time|e.g., relations among notes within a mu-sical phrase|but not structure that occurs over longer time periods|e.g., relations among phrases. To address this problem, we require a means of constructing a reduced description of the sequence that makes global aspects more explicit or more readily detectable. I propose to achieve this using hidden units that operate with diierent time constants. Simulation experiments indicate that slower time-scale hidden units are able to pick up global structure, structure that simply can not be learned by standard back propagation. Many patterns in the world are intrinsically temporal, e.g., speech, music, the un-folding of events. Recurrent neural net architectures have been devised to accom-modate time-varying sequences. For example, the architecture shown in Figure 1 can map a sequence of inputs to a sequence of outputs. Learning structure in temporally-extended sequences is a diicult computational problem because the in-put pattern may not contain all the task-relevant information at any instant. Thus,},
author = {Mozer, M and Lippmann, R and Moody, J and Touretsky, D},
booktitle = {Advances in Neural Information Processing Systems 4},
pages = {275--282},
title = {{Induction of multiscale temporal structure}},
year = {1992}
}
@book{Freeman2004,
author = {Freeman, Eric and Freeman, Elisabeth and Sierra, Kathy and Bates, Bert},
isbn = {0596007124},
issn = {1547-3317},
keywords = {design patterns,head-first,programming},
number = {5},
pages = {619},
title = {{Head First: Design Patterns}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25260185},
volume = {48},
year = {2004}
}
@article{gco1,
author = {Boykov, Yuri and Veksler, Olga and Zabih, Ramin},
doi = {10.1109/34.969114},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Energy minimization,Markov Random Fields,Potts model,early vision,graph algorithms,image restoration,maximum flow,minimum cut,motion,multiway cut.,stereo},
month = {11},
number = {11},
pages = {1222--1239},
title = {{Fast Approximate Energy Minimization via Graph Cuts}},
url = {https://doi.org/10.1109/34.969114},
volume = {23},
year = {2001}
}
@inproceedings{Ruesch2008,
abstract = {This work presents a multimodal bottom-up at- tention system for the humanoid robot iCub where the robot's decisions to move eyes and neck are based on visual and acoustic saliency maps.We introduce a modular and distributed software architecture which is capable of fusing visual and acoustic saliency maps into one egocentric frame of reference. This system endows the iCub with an emergent exploratory behavior reacting to combined visual and auditory saliency. The developed software modules provide a flexible foundation for the open iCub platform and for further experiments and developments, including higher levels of attention and representation of the peripersonal space.},
annote = {{\textgreater} Motivation: Neurowissenschaft

Aufmerksamkeitssteuerung beim iCub.

* Salienz im Kamerabild
* Sound lokalisierung
* projektiona auf ego-sph{\"{a}}re
* inhibition

{\textgreater} Evaluation: Schaut alles glecihm{\"{a}}{\ss}ig an},
author = {Ruesch, Jonas and Lopes, Manuel and Bernardino, Alexandre and Hornstein, Jonas and Santos-Victor, Jose and Pfeifer, Rolf},
booktitle = {2008 IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2008.4543329},
isbn = {978-1-4244-1646-2},
keywords = {attention},
month = {5},
pages = {962--967},
publisher = {IEEE},
title = {{Multimodal saliency-based bottom-up attention a framework for the humanoid robot iCub}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.141.5032 http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4543329},
year = {2008}
}
@inproceedings{Ballendat2010,
abstract = {In the everyday world, much of what we do is dictated by how we interpret spatial relationships, or proxemics. What is surprising is how little proxemics are used to mediate people's interactions with surrounding digital devices. We imagine proxemic interaction as devices with fine-grained knowledge of nearby people and other devices -- their position, identity, movement, and orientation -- and how such knowledge can be exploited to design interaction techniques. In particular, we show how proxemics can: regulate implicit and explicit interaction; trigger such interactions by continuous movement or by movement of people and devices in and out of discrete proxemic regions; mediate simultaneous interaction of multiple people; and interpret and exploit people's directed attention to other people and objects. We illustrate these concepts through an interactive media player running on a vertical surface that reacts to the approach, identity, movement and orientation of people and their personal devices.},
address = {New York, New York, USA},
author = {Ballendat, Till and Marquardt, Nicolai and Greenberg, Saul},
booktitle = {ACM International Conference on Interactive Tabletops and Surfaces - ITS '10},
doi = {10.1145/1936652.1936676},
isbn = {9781450303996},
issn = {10725520},
keywords = {large digital surfaces,location and orientation,proxemics,proximity,smart environments},
pages = {121},
publisher = {ACM Press},
title = {{Proxemic Interaction: Designing for a Proximity and Orientation-Aware Environment}},
url = {http://dl.acm.org/citation.cfm?id=1936652.1936676 http://portal.acm.org/citation.cfm?doid=1936652.1936676},
year = {2010}
}
@techreport{Hirmer2015a,
author = {Hirmer, Pascal and Mitschang, Bernhard},
keywords = {data flow,data mashups,domain-specific,patterns,recognition,situation},
title = {{ICWE2015 Rapid Mashup Challenge : Extended Techniques for Flexible Modeling and Execution of Data Mashups}},
year = {2015}
}
@article{Saarbrucken2010,
abstract = {In this paper? w e set up a unifying perspectiv e of the individual con? trol la ers of the arc y hitecture InteRRaP for autonomous in teracting agen ts? InteRRaP is a pragmatic approac h to designing complex dynamic agen t societies? e?g? for robotics ?M? uller ? Pisc hel ??a? and cooperativ e sc heduling applications ?Fisc her et al? ???? It is based on three general functions describ? ing ho w the actions an agen t commits to are deriv ed from its perception and from its men tal model? b elief r evision and abstr action? situation r c gnition e o and go al activation? and planning and sche duling? It is argued that eac h InteRRaP con trol la er?the beha y viour?based la y? er? the local planning la er? and the cooperativ y e planning la er ? can be de? y scribed b y a com bination of di?eren t instan tiations of these con trol functions? The basic structure of a con trol la er is de?ned? The individual functions and y their implemen tation in the di?eren tla ers are outlined? y W e demonstrate v arious options for the design of in teracting agen ts with? in this framew ork b y means of an in teracting robots application? The per? formance of di?eren t agen t t ypes in a m ultiagen t en vironmen t is empirically ev aluated b y a series of experimen ts?},
author = {Saarbrucken, D- and Pischel, Markus},
journal = {Technical memo / Deutsches Forschungszentrum f{\"{u}}r K{\"{u}}nstliche Intelligenz [ISSN 0946-0071]},
title = {{UNIFYING CONTROL IN A LAYERED AGENT}},
year = {2010}
}
